\chapter{Introduction}

\section{The rise of large-scale software mining}

\emph{Software mining} is a field of empirical software engineering which aims
to study datasets of extant software to uncover patterns and knowledge that can
help improve future software development. By organizing the knowledge obtained
from software mining studies, researchers can build models using statistics and
machine learning techniques that can then be queried to get design insights and
analytics, discover bugs, or even obtain a high-level architectural view of
how software components interact together.

Software mining particularly shines in the context of \emph{software
evolution}, which studies the dynamic behavior of software as it is maintained
and enhanced over its lifetime. The software engineering industry has a
profound awareness of how pervasive long-lived software is in the world's
digital infrastructure, and there is a clear need for research to be focusing
not just on the software source code itself, but also on its dynamic evolution
over time. Methodologically sound empirical studies have a particularly
important role to play as the basis for improving software maintenance tools,
methods and processes.

Historically, performing software evolution research was challenging and
focused on small amounts of data, one software project at a time. In one
seminal empirical work~\cite{belady1976model} in the '70s, Belady and Lehman
studied 20 releases of the OS/360 operating system and drew some observations
on the complexity growth of a large software project. This scale of study was
increasing slowly up until the late '90s, with Basili et
al.~\cite{basili1996understanding} studying 25 releases of around 100 different
software projects at NASA Goddard.  In a 1999
paper~\cite{kemerer1999empirical}, Kemerer and Slaughter highlighted the
inherent challenges of collecting empirical data to study software evolution:
researchers had to collect data at a minimum of two different points in time,
which required sustainable research efforts over a long period, or
collaborating with organizations that retained useful software measurement data
and development history.

These constraints drastically shifted in the past decades with the rise in
popularity of Free/Open Source Software (FOSS) and collaborative development
platforms~\cite{kalliamvakou2014promises}. Developers have started making
publicly available a large wealth of \emph{software artifacts}: the source code
files and directories of the software projects, as well as their complete
development history over time and all its associated metadata, which have in
turn benefited empirical software engineering research fields such as software
mining and software evolution. This was made possible especially thanks to the
emergence of \gls{VCS}, collaborative software development systems which track
the history of development by retaining the successive states of the source
code over time. They have been frequently analyzed~\cite{kagdi2007msrsurvey}
due to the rich view they provide on software evolution, and their ease of
exploitation since the advent of \gls{DVCS}. The peer-to-peer approach to
version control used by \gls{DVCS} makes it so that each user can retrieve the
full development history locally, which allows complex development patterns
like ``branches'' and ``forks'' to be directly embedded inside the change
history.

\section{Universal software mining}

\gls{DVCS} hosting platforms have allowed researchers to easily retrieve and
process software repositories for mining purposes and made the literature on
software evolution abundant~\cite{herraiz2013evolution}. Yet, most present-day
evolution studies still focus on the evolutive patterns of \emph{individual}
software projects, which is particularly interesting to ascertain which factors
contribute to maximize software health~\cite{DBLP:conf/icse/2018soheal}.

Larger-scale studies can sometimes analyze hundreds or thousands of
repositories at the same time, but their approach has generally remained
limited in scope: researchers will tend to focus on some specific sampling
rule, e.g., retrieving the top ten thousands repositories in a specific
programming language from a particular hosting platform. This can introduce
various sources of selection bias, as the most popular repositories in the most
popular platforms could be unrepresentative of development practices at large.

At an even larger scale, some studies have been performed on up to the entirety
of a specific software ecosystem like app stores or package
managers~\cite{gonzalez2009macro,debsources-ese-2016}, but those were limited
to the granularity of software releases and did not study finer-grained history
data like commits.

Arguably, one of the next frontiers in the field would be to attain
\emph{universal software mining}, the methodological analysis of software at
the largest scale possible down to a fine granularity of software artifacts.
At present, the largest scale possible is that of the \emph{software commons},
i.e., the body of all software which is available at little or no cost and
which can be reused with few
restrictions~\cite{1999-beagle-in-commons,kranich2008information}.

Unlocking this capability would have profound implications on the field of
software mining. First, it would considerably \textbf{reduce barriers to entry}
for empirical studies, by removing the need for researchers to manually
retrieve thousands of repositories from various sources to perform their
analyses: having access to the entire body of the software commons in a single
centralized location would allow them to simply request a subset of the data
they are interested in to perform their analysis, and obtain it in a format
that is convenient for data processing. It would also \textbf{enhance study
replicability} by providing an entry point to a static collection of already
available data that can be used to rerun studies at will.
In addition, it would significantly \textbf{reduce potential sources of
selection bias} by offering a representative view of the software found in
all different kinds of formats, on diverse more or less popular hosting
platforms, tracked by a variety of \gls{VCS}.  Finally, the exhaustiveness of
the data would allow us to get a \textbf{detailed high-level view of the social
processes and interactions} that govern software development, by looking at the
global network of the programming community in its entirety rather than
focusing on a particular subset.

Systematic initiatives~\cite{flossmole2006,gao2007archive,mockus2009}
have been established to gather as much public \gls{VCS} data as possible in a
single centralized place. Two recent initiatives, World of
Code~\cite{mockus2019woc} and Software Heritage~\cite{swhipres2017,
swhcacm2018} go one step further and aim to build a sustainable and accessible
archive of \emph{all} the source code artifacts of the software commons, down
to the level of the source code files. Even though full coverage w.r.t.\ the
software commons is by nature a moving target for any archive, the far-reaching
comprehensiveness and diversity of Software Heritage makes it the current best
approximation for an exhaustive corpus of public software development.

In this thesis, I explore the ways the body of software commons stored in the
Software Heritage archive can be organized and made accessible to researchers
for the purpose of \strong{universal software mining}.

\section{Availability vs.\ accessibility}

To understand the motivations and design decisions behind the work presented in
this thesis, it is important to establish the difference between making data
\emph{available} and \emph{accessible}.  The data collected in the Software
Heritage archive is already \emph{available} insofar as it is at everyone's
disposal by sole virtue of being public. Publishing periodical hard drive dumps
of the storage would theoretically be another way of making the data available,
as anyone would be able to retrieve it and exploit it.

What we aim to achieve by building a research platform for universal software
mining is to make the data not only available but also \emph{accessible}, which
means that the data should be presented in a way that is already suitable for
analysis purposes. Using the data should be convenient and frictionless,
as the platform only has value to researchers to the extent that its data can
be easily leveraged for many kinds of experiments. Merely providing raw data
dumps do not solve the accessibility problem, and what we need instead is a
comprehensive understanding of the needs of researchers in order to develop
reusable tools and infrastructures that can be built upon, instead of having
those be constantly rebuilt for one-off purposes. Simply put, the goal is to
build an \emph{abstraction} that allows researchers to absolve themselves from
the low-level technicalities of software mining to focus on their own
experiments.
