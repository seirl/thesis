\chapter{Version Control Systems}%
\label{chp:version-control-systems}

The work described in this thesis is about organizing the \emph{software
artifacts} in the Software Heritage graph to make them accessible to
researchers. These artifacts are abstract building blocks that represent the
source code trees, development history and hosting data stored in the archive.
In the next chapter, we will look at how the Software Heritage data model is a
graph built on the relationships between software artifacts. However, to better
grasp this model, it is good to first get a better understanding of how
these objects are captured in the data model of traditional \glspl{VCS}.

This chapter describes a generic model for how data is stored in most
\glspl{VCS}; it is very close to Git, the most popular of these systems, while
being abstract enough to be independent of any specific implementation.

\section{A simple repository model}

\subsection{Files and directories}

At a fundamental level, the most basic elements in a source code repository are
source code files. Developers tend to organize their code in different logical
units that are then hierarchized in several levels of directories. At a low
granularity, programmers separate their program in small logical blocks (like
\emph{functions} or \emph{classes}) that perform a specific task or
computation. A \emph{source code file} is generally a collection of
conceptually related functions, put together as a single logical unit. Source
files that define the behavior of a logically distinct component of a program
are also often logically grouped in the same directory, sometimes called a
\emph{module} or \emph{package}.

\begin{figure}
    \centering
    \begin{subfigure}[b]{.40\textwidth}
        \dirtree{%
            .1 /.
                .2 src.
                    .3 evalexpr.c.
                    .3 parser.
                        .4 ast.c.
                        .4 parser.c.
                        .4 lexer.c.
                .2 tests.
                    .3 eval.c.
                    .3 operands.c.
        }
        \caption{Directory listing.}%
        \label{fig:vcs-dir-flat}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.58\textwidth}
        \centering
        \input{tikz/figures/dir-tree.tikz}
        \caption{Represented as a tree structure.}%
        \label{fig:vcs-dir-tree}
    \end{subfigure}
    \caption{Example of a directory hierarchy for a code repository.}
    % \label{fig:vcs-dir-project}
\end{figure}

The example in \cref{fig:vcs-dir-flat} illustrates a file hierarchy for a
project meant to evaluate simple mathematical expressions. There is always one
root directory containing the entire project, that we represent with a slash.
At the root level, there is a directory containing C source code files, with
some of them organized in a ``parser'' module, and a test directory containing
tests also written in C.

The file hierarchy in this example project can naturally be visualized as a
tree data structure, by representing the directories and the file contents as
the vertices, with their respective names on the edges. The resulting tree is
shown in \cref{fig:vcs-dir-tree}.

\subsection{Revisions}

The concept of \acrlong{VCS} arose from a need to keep track of the
changes happening in the code. This ability is critically important for
software systems, as it allows potentially erroneous changes to be reverted
without having to recall or reconstruct the previous behavior. Furthermore, the
ability to retain institutional knowledge on a codebase by having the
possibility to look up past source code changes is very valuable.

Traditionally, this ``versioning'' could be done by simply copying the software
source tree to a new directory for each new version and keeping around the old
versions as archives of the past state of the code. Generally, creating an
entire new source tree is convoluted, making this manual process tedious and
discouraging small incremental changes. In addition, it hinders collaboration
between developers, who have to exchange work in progress source files and can
easily lose track of which version they are developing on.

However, this basic concept of keeping track of ``snapshots'' of the state of the
source tree at different points in time was a key design insight in the
development of most modern \glspl{VCS}. They introduce the notion of
\emph{revisions}, which can be conceptualized as a chain of frozen past states
of a source tree that were recorded at various points in time during
development. In some systems, these successive states are known as
\emph{commits}, or sometimes \emph{changesets}, but they all refer to the same
abstract concept.

\begin{figure}
    \centering
    \input{tikz/figures/rev-chain-example.tikz}
    \caption{Three revisions in the example repository, forming a chain of
    past states of the source tree.}%
    \label{fig:vcs-rev-chain-example}
\end{figure}

\Cref{fig:vcs-rev-chain-example} shows three successive states of the source
tree from our example repository, recorded as three different revisions. The
revisions all point to a full copy of the source tree, exactly as it was when its
state was recorded. Each revision also contains a reference to the revision
immediately preceding itself, as a way to keep track of the order in which the
changes were made.\footnote{Note that, somewhat confusingly, this implies that
the arrows between commit nodes are in the opposite direction of time: each
revision holds a reference to its \emph{previous} revision, and thus points
towards the \emph{past}.} By iteratively walking the chain of parents from a
given revision, it is thus possible to visit all the previous states of the
repository on which this revision was based.

Along with its source tree and a reference to its parent, a revision also
typically contains additional metadata: the \emph{date and time} at which it
was created, its \emph{author}, and a \emph{message} describing the changes it
introduced since the last state of the code. Retaining this information
directly in the \gls{VCS} allows one to precisely track down when a given
change was made, who authored the change and the rationale given for it. Most
systems expose this information in two ways: as a \emph{log}, where one can
look at an ordered list of all the changes that were made to the source tree
(or even a specific file or sub-directory), and through an \emph{annotate} (or
\emph{blame}) command that shows the revision in which each line of a given
file was modified for the last time.

\subsection{Branching and merging}

A crucial challenge in collaborative software development is that the changes
do not necessarily happen in a linear fashion. To efficiently share tasks amongst
multiple people, development teams need the ability to work on the codebase
simultaneously, which requires having several versions of the repository in
parallel. In a \gls{VCS}, this is typically achieved through \emph{branching}:
from a single revision, developers can create multiple states of the repository
that can be modified separately and in parallel, which splits the revision
chain.  Generally, developers working on different branches will then attempt
to integrate their respective changes to the main codebase by \emph{merging}
them.  This merging operation combines the split chains back into a single
state, called a \emph{merge revision}.

\begin{figure}
    \centering
    \input{tikz/figures/rev-branching-merging.tikz}
    \caption{Branching and merging.}%
    \label{fig:vcs-rev-branching-merging}
\end{figure}

\Cref{fig:vcs-rev-branching-merging} shows an example of branching and
merging in a revision history.  As can be seen on the merge revision, this
concept extends the simple model introduced above by allowing revisions to
refer to multiple parent revisions as a way to support merging.

When working with multiple revision chains in parallel, it is useful to give
them a name to keep track of their purpose. For that, we can use
\emph{branch} objects, which are simple references to the tip of a revision
chain and which get automatically updated when new revisions are added to it.

In most simple development workflows, developers tend to always keep a main
branch that is long-lived and reflects the current state of the project, and
various kinds of additional ``topic'' branches which contain features that are
currently being worked on in parallel. \Cref{fig:vcs-rev-branches} shows an
example of three branches pointing to different revision chains. Note that
since branches are just pointers, there is no obvious notion of a revision
\emph{belonging} to a specific branch, but rather revisions are
\emph{reachable} from one or even several branches.

\begin{figure}[b]
    \centering
    \input{tikz/figures/rev-branches.tikz}
    \caption{Feature branches keeping track of parallel revision chains.}%
    \label{fig:vcs-rev-branches}
\end{figure}

\subsection{Releases}

Some revisions in the development history are particularly important because
they represent specific milestones in the history of a software. This is often
the case for \emph{software releases}, where the software is distributed to
its users as part of its release cycle, usually being given a numbered version
like ``\texttt{v2.4.3}''.

To be able to refer to these special revisions using mnemonic names like the
software versions themselves, \glspl{VCS} allow developers to add named markers,
or \emph{tags}, to some revisions.
While branches already provide a way to refer to a specific revision by a
mnemonic name, they are by nature dynamic and ever-changing, as adding more
revisions to a branch will change the tip of the branch and thus the revision
to which the branch points. In contrast, \emph{releases} are static and always
refer to one specific revision.
\Cref{fig:vcs-rel-example} shows an example of two releases as first-class
objects in the development history that point to two specific revisions.

\begin{figure}
    \centering
    \input{tikz/figures/rel-example.tikz}
    \caption{Example of a development history with two revisions tagged as
    \emph{releases}, each corresponding to a new version of the software
    distributed to end-users.}%
    \label{fig:vcs-rel-example}
\end{figure}

As with revisions, but unlike branch names which are just shallow named
references, releases can also contain additional metadata: the \emph{date} at
which they were created, the \emph{author} of the release, and a \emph{message}
describing the release.

% While releases and branches are typically meant to point to revisions, the
% data model of some \glspl{VCS} also support the possibility of making them
% reference other objects, such as files and directories. While this is a
% generally rare occurrence, there are some examples of this happening in
% real-world repositories, and our generic \gls{VCS} data model should
% accommodate for that possibility.

\section{Artifact deduplication}

The simple repository model we described is general enough to represent the
data stored in the majority of \acrlongpl{VCS}, both at the level of the files
and directories that constitute the source tree, and at the level of the
development history by capturing the successive states of this source tree over
time.

One immediately apparent issue that arises when trying to implement this model
in a naive way is the sheer amount of \emph{duplication} of identical data
across revisions: creating a new revision for a single line change duplicates
the entire source tree and all the files it contains. This drawback goes
opposite to one of the goals of modern \glspl{VCS}, which is to encourage a
high granularity of development history to isolate every logical change in a
separate revision.

However, it is possible to integrate \emph{deduplication} in the model as a way
to reduce its storage footprint. This section details how we can leverage
cryptographic hash functions to achieve deduplication at the level of single
objects and entire subtrees.

\subsection{Cryptographic hash functions}

The basic primitive used for deduplication in many \glspl{VCS} are
\emph{cryptographic hash functions}: mathematical algorithms that can map
arbitrary data into a single fixed-length unique identifier. By computing the
identifier, or \emph{hash}, of each object, it is possible to check when two
objects are identical very quickly by simply comparing their hashes. Because
they have a fixed length, comparing two hashes is an operation with a time
complexity of $O(1)$.

\begin{figure}
    \centering
    \input{tikz/figures/cryptographic-hash-function.tikz}
    \caption{A cryptographic hash function deterministically converts data of
    arbitrary size to a fixed-length and virtually unique identifier.}%
\end{figure}

Due to the pigeonhole principle, the identifiers cannot be truly unique. If the
resulting hash is 256 bits long, there must be at least one \emph{collision}
in any set of $2^{256}+1$~elements, that is, at least two elements must have
the same hash. For sufficiently large hash sizes however, assuming the hash
function is resistant to cryptanalytic attacks, it is in practice impossible to
generate a collision as it would require inordinate amounts of time and
computing power, and we can thus be confident that no such collision exists.
Finding a collision in a 256-bit collision resistant cryptographic hash
function would take more than \num{2600}~times the lifespan of the universe,
rendering it impossible for all intents and purposes.

In some cases, the hash functions can have weaknesses that are exploited by
cryptanalytic attacks. This is the case of SHA-1~\cite{standard1995fips}, the
hash function used by default as a deduplication method for Git, where a
collision was found in 2017~\cite{stevens2017first}.  Various methods can be
deployed to reduce susceptibility to these collision attacks, including
detecting and rejecting payloads designed to produce collisions, or migrating
to more secure hash functions like SHA-256~\cite{fips2012180} or
BLAKE2~\cite{aumasson2013blake2}.  In this thesis, we always assume that
cryptographic hash functions map to a single unique identifier and discard the
possibility of collisions. These attacks do not have a big impact for our use
case, as objects with colliding hashes are artificially generated for this
purpose, reducing the interest of archiving and analyzing them.

\subsection{Deduplicating single files}

Armed with cryptographic hashes, it becomes relatively easy to deduplicate the
identical files that get copied between each revision. We systematically
compute the hash of each file and store them only once. If a new directory
references a file with a hash that has already been stored, the directory
simply references the file that is already in the storage rather than storing
the file again.

\begin{figure}
    \centering
    \input{tikz/figures/deduplicate-contents.tikz}
    \caption{Deduplication of files with identical hashes. The transparent
    files are detected as duplicate objects, removed and replaced by a pointer
    to the already existing objects (red arrows).}%
    \label{fig:deduplicate-contents}
\end{figure}

\Cref{fig:deduplicate-contents} shows an example of deduplication at the file
level, based on the example repository shown in
\cref{fig:vcs-rev-chain-example}. The \gls{VCS} computes the hashes of all
the files, which are represented as single letters for the purpose of clarity.
As more revisions are added to the repository, the multiple states of the
source tree will tend to reference files that have already been stored in the
system.  Because the hash of these files will match the hash of the previous
files, they will get deduplicated and only stored once. Here, the file with
hash A is present in three different directories and the files with hashes B, C
and D are present in two different directories, but these directories all
reference the same unique objects.

\subsection{Deduplicating subtrees}%
\label{sec:deduplicating-subtrees}

While deduplicating the files that remain identical between successive
revisions significantly reduces the storage impact of the model, each version
still requires recreating and storing an entire file hierarchy. In a repository
with $n$ directories, modifying a single file creates $O(n)$ new directories.
This is quite inefficient, considering that the majority of directories are
unchanged between the two revisions.

Instead of solely deduplicating files, which are only the leaves of the source
trees, a better approach could be to deduplicate \emph{shared subtrees}: if a
node and all its descendants are not modified between two revisions, they
should not be copied and instead be stored as a single tree.

The challenge of doing so arises from the difficulty of checking whether two
subtrees are identical, which is typically an operation with a time complexity
of $O(n)$, as it requires recursively checking that all the descendants of the
root of the subtree are identical. To improve this, we can instead
\emph{recursively compute a cryptographic hash} for each directory, that will
be dependent on the hashes of all its children. Put simply, if a single element
anywhere in the subtree changes, its hash will also change, which will then
change the hash of its parent and recursively do so up to the top of the
subtree.

Using this hashing scheme, if two subtrees have the same hash, it will mean
that they are entirely identical, from their root down to their leaves. This
again makes checking the equality of two subtrees a simple hash comparison,
which has a time complexity of $O(1)$. Because these cryptographic hashes
can perfectly identify the entire content of a subtree, we call them
\emph{intrinsic hashes}: in some sense, these hashes are an algorithmic way to
capture the essential nature of a directory and all its contents.

\begin{figure}
    \centering
    \begin{subfigure}{.53\textwidth}
        \centering
        \input{tikz/figures/deduplicate-subtrees.tikz}
    \end{subfigure}\hfill{\Huge $\rightarrow$}\hfill%
    \begin{subfigure}{.35\textwidth}
        \centering
        \input{tikz/figures/deduplicated-repo.tikz}
    \end{subfigure}
    \caption{Deduplication of an entire subtree in the example repository. An
    intrinsic hash is recursively computed for each directory, allowing the
storage to deduplicate the unmodified ``\texttt{src}'' directory between the
second and third revisions.}%
    \label{fig:deduplicate-subtree}
\end{figure}

\Cref{fig:deduplicate-subtree} shows this subtree deduplication in action. In
the same vein as for file deduplication, we now recursively compute the
intrinsic hash of each object in the source trees. Whenever a new directory has
to be added, its hash is compared to the hashes of the directories already
present in the main storage. If a matching hash is found, the new directory is
deduplicated and replaced by a reference to the already existing directory.

\subsection{Merkle trees and directed acyclic graphs}

The hashing scheme we described is generalized and applied to all the objects
in the graph of development history, like revisions and tags. All the software
artifacts are attributed an intrinsic hash recursively computed from the
objects it refers to (e.g., in the case of revisions, its parents and thus,
recursively, its descendants). These intrinsic hashes will be the primary
identifiers of the software artifacts, and guarantee their unicity in the graph
of the \gls{VCS}.

This technique of building trees identified by a cryptographic hash that is
recursively dependent on its entire chain of descendants has first been
described by \textcite{Merkle}. We call \emph{Merkle tree} or \emph{hash
tree} a tree in which every node is labelled with the cryptographic hash of a
data manifest containing its key and the hashes of its child nodes. An example
of such a manifest for a directory object is shown in
\cref{fig:directory-manifest}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \matrix [draw=black, matrix of nodes, nodes={anchor=west}] (dirmanifest)
        [label=above:{\emph{Directory manifest}}] {%
            \texttt{100644} & |[rounded corners, contentfill]| \texttt{blob}
                            & \texttt{82fab97d8d…} & \texttt{README}\\
            \texttt{100644} & |[rounded corners, contentfill]| \texttt{blob}
                            & \texttt{1b25a2041c…} & \texttt{evalexpr.c}\\
            \texttt{100755} & |[rounded corners, contentfill]| \texttt{blob}
                            & \texttt{1b25a2041c…} & \texttt{configure.sh}\\
            \texttt{040000} & |[rounded corners, directoryfill]| \texttt{tree}
                            & \texttt{b62e96afcf…} & \texttt{parser}\\
            \texttt{160000} & |[rounded corners, revisionfill]| \texttt{commit}
                            & \texttt{e1a77d8342…} & \texttt{libcuda}\\
        };

        \node [draw, right=2cm of dirmanifest.east,
            label=above:{\emph{Directory hash}}] (treehash)
            {\texttt{a009a5c4fd63bc5262…}};

        \draw[style=arrow] (dirmanifest.east) -- (treehash.west)
            node[midway, above] {\footnotesize SHA-1};

        \node [below=1.3cm of dirmanifest-5-1.south] (labelentryperms)
            {Permissions};
        \node [below=0.7cm of dirmanifest-5-2.south] (labelentrytype)
            {Target type};
        \node [below=1.3cm of dirmanifest-5-3.south] (labelentryhash)
            {Target hash};
        \node [below=0.7cm of dirmanifest-5-4.south] (labelentryname)
            {Entry name};
        \draw[style=arrow] (labelentryperms.north) -- (dirmanifest-5-1.south);
        \draw[style=arrow] (labelentrytype.north) -- (dirmanifest-5-2.south);
        \draw[style=arrow] (labelentryhash.north) -- (dirmanifest-5-3.south);
        \draw[style=arrow] (labelentryname.north) -- (dirmanifest-5-4.south);

    \end{tikzpicture}
    \caption{Data manifest of an example directory. Inner nodes in a Merkle DAG
        are identified by a cryptographic hash of a \emph{data manifest} that
        contains the hash of all their children, which recursively makes their
        hash dependent of all their descendants.}%
    \label{fig:directory-manifest}
\end{figure}

In our case, the structure that contains the development history is technically
not a tree, but a \emph{\gls{DAG}}: a tree can only have one parent node, which
is not the case if a software artifact is referenced by two other artifacts.
This can happen when merging (it creates a revision with two parents) or when
sharing objects (a deduplicated directory or a blob is referenced by
multiple objects). We thus call this structure a \textbf{Merkle DAG}. Note that
it, by definition, cannot contain any cycles: because adding a node requires
the hash of all its children to be already known to compute its own hash, a
cycle would require computing two mutually dependent cryptographic hashes,
which is at least as hard as finding a collision.

Like all Merkle structures, this data model exhibits interesting and useful
algorithmic properties. In particular, it is worth noting that as long as two
Merkle DAGs are \emph{complete} (i.e., no node is missing), one can efficiently
determine if they are identical or not by simply comparing the identifiers of
all their root nodes, which in our case are snapshot nodes. Also, in case they
differ, one can efficiently identify the topmost differences by performing a
parallel visit on the two graphs.

Furthermore, Merkle structures natively deduplicate all artifacts stored in it,
as detailed in \cref{sec:deduplicating-subtrees}.  As object identifiers are
not assigned, but rather computed intrinsically on the content of the objects
and their descendants, adding a node to a Merkle structure is an idempotent
operation. Trying to add to our data model the same source code blob or tree
multiple times (e.g., because it is found in multiple commits in the same
repository) will result in adding it only once; the same applies to all types
of objects, so a commit which occurs in multiple branches or repositories
will be stored only once.

In addition to how they allow us to identify and deduplicate software
artifacts, intrinsic hashes also have the side benefit of enabling data
integrity checks at any time when using a \gls{VCS}. If any piece of data is
corrupted in the development history, rehashing the nodes in the graph would
yield different hashes. This can be used to precisely track down the pieces of
corrupted data and reject them, which is particularly useful in the case of
\glspl{DVCS} where artifacts can be broadcast through unreliable data links and
from potentially untrusted sources.

\subsection{Purely functional persistence}%
\label{sec:purely-functional}

Because the goal of \glspl{VCS} is to preserve the history of the software
artifacts they track, the Merkle DAG on which they base their data model is
designed to be \emph{persistent}: updating a file or a directory in a source
tree does not destroy the existing version of the subtree, but rather creates a
new version that coexists with the old one. As we have seen in the previous
sections, persistence is the basis upon which \glspl{VCS} store successive
states of the development history, and is achieved by \emph{copying} the
subgraphs that need to be modified, and \emph{sharing} all the nodes unaffected
by the update.

These kinds of data structures which are immutable, persistent and which use
data sharing to minimize memory usage were systematically categorized in a
seminal work by \textcite{okasaki1999purely}, where he describes them as
\emph{purely functional data structures}.

Purely functional \glspl{DAG} like the ones used in our model to store source
code trees have interesting properties, notably in terms of memory complexity.
As shown in \cref{fig:okasaki-complexity}, creating a new revision with a
single node changed only requires to recreate the nodes constituting the chain
of parents from the changed node to the root of the tree. A revision changing a
node in a source tree of size $n$ and height $h$ will therefore have a memory
complexity of $O(h)$. If the tree is balanced, as well-hierarchized source code
trees generally tend to be, this memory complexity will approach $O(\log(n))$.

The immutability of purely functional \glspl{DAG} is also an interesting
property for archival and empirical research purposes, as it allows us to
preserve rewritten \gls{VCS} history. When developers use history rewriting
commands like \texttt{git rebase} or \texttt{git commit --amend}, instead of
modifying the nodes themselves, the \gls{VCS} will create a new node and make
the branch point to that modified node. This model makes it easy to store
past states of rewritten history, simply by preserving all the immutable nodes
that were added to the graph.

All in all, Merkle \glspl{DAG} are an overarching keystone of the development
of modern \glspl{VCS} thanks to their ability to \emph{deduplicate} common
artifacts, to \emph{ensure integrity} of the data they contain, and to their
memory efficiency through \emph{purely functional data sharing}.

\begin{figure}
    \centering
    \input{tikz/figures/okasaki-complexity.tikz}
    \caption{Modifying a single file between two revisions in a source tree of
    height $h$ only requires $O(h)$ new nodes, thanks to data sharing in purely
    functional trees.}%
    \label{fig:okasaki-complexity}
\end{figure}
