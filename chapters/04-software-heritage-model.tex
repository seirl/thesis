\chapter{The Software Heritage Graph}%
\label{chp:swh-model}

% This chapter describes the abstract data model of the Software Heritage
% archive.

\section{Canonical Software Artifacts}%
\label{sec:swh-artifacts}

The Software Heritage archive is continuously ingesting software artifacts from
a wide array of sources, including different VCS and package managers that
each have their own internal data model. The main purpose of the Software
Heritage data model is to provide a generic structure in which the individual
object types specific to each system can be mapped to abstract concepts. For
example, Git ``commits'', SVN ``revisions'' and Mercurial ``changesets'' all
correspond to the same idea of a frozen state of the source tree, and thus they
are all \emph{canonicalized} as a single type of artifact that we call
``revisions''.

By stripping the implementation peculiarities of the individual data sources,
the artifacts are boiled down to a purely abstract form. This unifies the
representation of all the artifacts stored in the archive, which is
particularly interesting for research: since all the artifacts are already
stored in canonical form, we can provide a uniform interface for researchers
to study artifacts coming from a variety of different sources. This isolates
the complexity of handling artifacts sourced from different \glspl{VCS} behind
an abstraction layer; researchers can then run analyses on the abstract
artifacts instead of having to deal with each specific system.

The following kinds of canonical software artifacts are supported in the data
model:

\begin{wrapfigure}{l}{0.07\textwidth}\centering
\begin{tikzpicture}\node[style=content,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Blobs}} (or ``file contents'') represent the raw content of
source code files, as recorded in modern \glspl{VCS}. A blob contains only the
data stored in a file as a raw sequence of bytes. File names and other
properties normally associated to the more abstract notion of ``file'' are not
associated to blob nodes. Other types of nodes, and most notably directories,
attach such directory-dependent information to blobs.

Blobs are identified by a cryptographic hash computed from the full binary data
they contain.

% \TODO{Add examples of manifests}


\begin{wrapfigure}{l}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=directory,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Directories}} represent source code trees. Each directory is
a list of \emph{named} directory entries, each entry pointing to either blob
objects (``file entries''), directory objects (``directory entries''), or
revision objects (``revision entries''). Each entry from a directory is
associated to a local name (i.e., a \emph{relative} path without any path
separator) and permission metadata (i.e., a Unix permission mode such as
\texttt{0o755} for an executable file). While file and directory entries are
the most common to form nested source code trees, \emph{revision entries} also
exist and are used to represent sub-directories that reference specific
revisions from external repositories, as it is permitted by VCS like Git (to
reference so called ``git submodules'') and SVN (with ``subversion
externals''). Permission metadata is also used to recognize symbolic links from
regular files.

A directory node is identified by a cryptographic hash of a canonical textual
representation of all its entries, that include the identifier of target
objects.



\begin{wrapfigure}{l}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=revision,scale=1.3] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Revisions}} (or ``commits'') are point-in-time captures of
the state of the entire source code tree of a project. Each revision points to
the ``root'' directory of the project source tree at the time the commit is
recorded. The following properties are associated to commit objects:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \emph{commit message}: a descriptive, human-targeted message
        explaining the reasons for the changes made since the previous
        revision.
    \item \emph{author}: the name and e-mail of the person who authored the
        revision.
    \item \emph{date}: the date at which the revision was authored, including
        timezone information.
    \item \emph{committer}/\emph{committer date}: two properties analogous to
        author/date, but capturing the person who actually committed the change
        (who is not always the person who \emph{authored} it, in particular in
        development workflows that rely on code reviews) and when the commit
        happened.
\end{itemize}

Finally, each revision points to an ordered list of all its parent revisions,
referenced using their own intrinsic identifier: zero parent revisions for the
first revision in a given development history (e.g., first revision in a VCS
repository), one parent for non-merge revisions, two or more parents for
revisions that merge together several development branches.

The order of the parents does not have a strict semantic meaning, it is mostly
used as a guide by tools that show the difference between two revisions. The
first parent is generally considered to be issued from the ``main'' branch that
the revision is merged onto, and thus diffing tools will show the impact of
this revision on the main branch by default.

Revisions are identified by an intrinsic hash of a canonical textual
manifest containing all their metadata, their parent identifiers, and the
identifier of the directory node denoting the root of the source tree at the
time of the revision.


\begin{wrapfigure}{l}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=release,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Releases}} (or ``tags'') denote marker objects that label
specific revisions as project milestones. These usually denote the revisions
where the software is distributed to its user base, as well as the various
steps of its release cycle (``alpha'', ``beta'', ``rc1'', etc.). These releases
are marked with a specific and usually mnemonic short name (e.g., a version
number such as \texttt{v2.0}). Aside from this name and a reference to their
target revision, releases additionally contain some metadata:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \emph{message}: analogous to revision messages, an annotation
        describing the release, generally by including its full changelog.
    \item \emph{author}: the name and e-mail of the person who authored the
        release.
    \item \emph{date}: the date at which the release was created, including
        timezone information.
\end{itemize}

The data model also supports revisions pointing to other kinds of artifacts
(contents and directories), which is supported by some \glspl{VCS} and can be
found in real-world repositories.

Releases are identified by a cryptographic hash taken on a canonical text
manifest containing release name, release properties, and the identifier of the
revision node they reference.


\begin{wrapfigure}{l}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=snapshot,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Snapshots}} are point-in-time captures of the \emph{full
state} of a project development repository. Unlike revisions, which capture the
state of a single development branch, snapshots capture the state of \emph{all}
the branches and releases in a repository. These artifacts are not typically
part of \glspl{VCS}, because they generally have no need to retain the
information of the past states of the full repository, as the important
historical information for developers is already tracked by revisions. However,
in the case of a software archive where development history can be rewritten or
even erased, we need to track the successive states of the repositories for
each crawling \emph{visit}, which includes retaining old information about
which branches and tags were formerly present in the repository, and what they
pointed to.

More specifically, each snapshot contains a list of references to other
artifacts, which have an associated binary name (e.g., ``refs/heads/main'' or
``refs/tags/v0.1.2'') and the intrinsic hash of the artifact they reference.
Most references are either to revisions (in the case of branches) or release
objects, but on rare occasions they can also point to directories and
contents, which is supported by some \glspl{VCS} and also sometimes present in
real-world repositories.

The data model also supports \emph{branch aliasing}: some branches stored in
snapshots do not point to a specific artifact, but rather reference another
branch name in the same snapshot (e.g.,
\texttt{HEAD}$\to$\texttt{refs/heads/main}). These are to be treated as
symbolic links to the target of the branch they reference.

Snapshots are also deduplicated and identified by an intrinsic hash, computed
from a textual manifest which associate each branch name and the intrinsic
identifier of its target. Therefore, they also benefit from the advantageous
space-complexity of purely functional persistent structures as described in
\cref{sec:purely-functional}: if two consecutive visits of the same repository
show that nothing changed in the interval, the visits can both share the same
deduplicated snapshot in the data model, instead of having to copy and store
the set of branches and tags twice.


\begin{wrapfigure}{l}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=origin,scale=1.9] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Origins}} are objects referencing the specific places from
which source code artifacts have been retrieved to be ingested into the
archive.  They are represented by a URL (e.g., the address at which
one can \texttt{git clone} a repository or \texttt{wget} a source code
tarball.)

Origins only represent specific locations which host code, and are distinct
from the more abstract notion of \emph{projects}: a project is an entity that
can relate together different development resources, including websites, issue
trackers, mailing lists, and software origins. A software project can migrate
its development from one origin to another for various reasons, and even
migrate to a different \gls{VCS}. Projects are ontologically complex notions
that are not directly present in the archive; the data model is mostly
concerned about directly addressable and concrete artifacts, and \emph{origins}
are better suited for this purpose. In the rest of the thesis, we sometimes use
the term ``project'' to refer to origins when the meaning is not ambiguous, as
the latter are often are our best approximation for the former.

\section{Consolidating software artifacts in a unified archive}%
\label{sec:consolidation}

So far, we have covered the different abstract artifacts stored in \glspl{VCS}
in a way that allows us to represent an entire repository using these building
blocks, notably by deduplicating them using their intrinsic identifiers.
In fact, it is possible to go even further, and deduplicate these artifacts
across the \emph{entire archive}.

A key point of consideration is that software products are generally built by
reusing components from other projects, rather than being mostly isolated and
independent pieces of work. This organic code reuse happens through different
means, either by simply copying source code files or modules between different
projects, or by ``forking'' an existing project (i.e., starting an independent
development on an existing project by building upon its development history).
Modern \glspl{VCS} also allow referencing external software projects to be
fetched as dependencies (e.g., Git submodules or Mercurial subrepositories),
which further entangles the relationships between different software projects.

\glspl{VCS} and the abstract software artifacts used by Software Heritage
already allow us to canonicalize and deduplicate objects inside a single
software project. This principle can be generalized to deduplicate the
artifacts found in several projects \emph{across the entire archive}.
The process of systematically gathering and storing all publicly available
software artifacts in a deduplicated and canonical fashion is inherently a way
to materialize an immense highly interconnected graph, linking together all
derivative works and shared codebases.

This process is illustrated in \cref{fig:consolidating-archive}. The
intrinsic cryptographic hashes of the software artifacts are used to enforce
their full deduplication, consolidating them in a unified Merkle \gls{DAG} with
all the artifacts in the archive.

\begin{figure}
    \centering
    \input{tikz/figures/consolidating-archive.tikz}
    \caption{Consolidation of two different repositories in the archive by
    deduplicating and sharing all their common artifacts.}%
    \label{fig:consolidating-archive}
\end{figure}

In short, this means that whenever a directory, file or any artifact is
referenced by multiple projects, it will get deduplicated as a single node in
the graph, and will be directly referenced by all projects and artifacts
that contain it. As an example, there is only one artifact for the empty
content (a blob of length zero) in the entire archive, and millions of
directories reference it.

Merging all archived repositories into a single large collection of software
artifacts leverages all the benefits of Merkle \gls{DAG} models for single
repositories to the entire archive: simple identification of unique artifacts,
built-in data integrity checks, very high rate of deduplication and reduced
storage costs.

In addition, storing canonical artifacts only once in the archive is also
semantically useful for research. Essentially, it becomes easy to visualize
and analyze code reuse: walking back the \gls{DAG} can generate a list of all
projects that contain a specific piece of code.
% By looking at the commit chains in the graph, it is possible to discover when
% any given file was copied, and all changes that were subsequently made to it.
We can easily draw research applications for software evolution from this
capability: by having direct access to the history of changes of a specific
fragment of code in the entire corpus of public software commons, it could be
possible to make automatic linting suggestions based on likelihood that a given
piece of code will be modified in the future. Overall, materializing these
relationships between canonical software artifacts has the potential to widen
the frontier of software mining by providing access to a unique corpus of
provenance data and evolution history of the artifacts.


\section{The Software Heritage Merkle DAG}

\subsection{Overview}

After having walked through the key principles that underpin the design of the
Software Heritage archive, this section describes its data model in more
detail.

The archive is stored as a single large Merkle \gls{DAG}, as depicted
in \cref{fig:swh-model}. In this graph, all software artifacts we have
described correspond to the nodes, which can be of six different types:
contents, directories, revisions, releases, snapshots and
origins\footnote{Origins are technically not part of the Merkle DAG but rather
only \emph{point} to it, as their intrinsic identifiers are not computed from
their list of descendants, which changes over time.}.
All the nodes are deduplicated and identified by a single intrinsic
identifier, recursively computed from the identifiers of its descendants, which
serves as the primary key in the Merkle \gls{DAG}.

The edges between the nodes of the graph are derived from the relationships
between the artifacts: directory entries point to other directories or file
contents; revisions point to directories and previous revisions; releases point
to revisions; snapshots point to revisions and releases; origins point to the
snapshots that they contained in past visits.
The cardinality diagram shown in \cref{fig:swh-cardinality} formalizes the
different sorts of edges in the graph and their numerical relationships: edges
noted $\prescript{\ast}{}\rightarrow^1$ represent \emph{many-to-one}
relationships (e.g., a revision only points to a single directory, but a
directory can be pointed by multiple revisions) and edges noted
$\prescript{\ast}{}\rightarrow^\ast$ are \emph{many-to-many} relationships
(e.g., a directory can point to multiple contents, and a single content can be
pointed by multiple directories).

A few observations can be drawn from this diagram:

\begin{itemize}
    \item Most relationships are many-to-many. Only release$\to$commit
        and\linebreak commit$\to$directory relationships are many-to-one, as
        they can only point respectively to a single commit and a single
        directory.

    \item Commit and directory nodes are parts of recursive relationships.
        Commits point to their parent commits, while directories point to their
        children (sub)directories and files. These two node types are thus what
        gives the DAG an arbitrary depth, as all the other layers have a fixed
        maximum height. Note however that this cycle in the graph cardinality
        diagram does \emph{not} induce cycles in the graph itself, because
        graph nodes are created bottom-up and need to know in advance the
        cryptographic identifiers of target nodes (which thus need to
        exist \emph{before} their parents) in order to be identifiable.

    \item Commits and directories are also mutually recursive, as commits point
        to directories and directories can occasionally point to commits (for
        submodules/externals). This is the only case in which a node can point
        to a node from an upper layer in the cardinality diagram.
\end{itemize}

Note: in the wild one can encounter VCS repositories that contain relationships
between source code artifacts that are not depicted in
\cref{fig:swh-cardinality}.
For instance, Git allows you to tag blobs and directories as releases, or to
use blobs as branch destinations.
These are anomalies that in most cases result in non usable VCS data.  We
measured these occurrences in our corpus and verified that they are rare and
unconventional (less than 0.0004\,\% of the total number of edges from releases
and snapshots) and thus, even if they \emph{can} be represented in the \SWH{}
data model, we generally exclude them from our discussion for the sake of
simplicity.

\begin{figure}
    \centering
    \input{tikz/figures/swh-model.tikz}
    \caption{The Software Heritage Archive: a unified Merkle \gls{DAG} of all
    the software artifacts in public development.}%
    \label{fig:swh-model}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{img/swh-cardinality}
    \caption{Cardinality diagram of the possible relationships between the node
    types in the archive, highlighting the difference between
    \emph{many-to-one} and \emph{many-to-many} edge types.}%
    \label{fig:swh-cardinality}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/swh-merkle-dag}
    \caption{Detailed view of the Merkle \gls{DAG}, with object contents,
    metadata fields and intrinsic hashes.}%
\end{figure}

\subsection{Layers}%
\label{sec:layers}

While each relationship can be analyzed independently, is useful to regroup
the node and edges of the Merkle \gls{DAG} by type into logical layers that are
conceptually meaningful.  To that end we define the following subgraphs of the
Software Heritage graph:
\begin{itemize}

\item \emph{Full corpus}: the entire graph of public software development

\item \emph{Filesystem layer}: subset of the full corpus consisting of blob and
  directory nodes only, and edges between them

\item \emph{History layer}: subset of the full corpus consisting of revision and
  release nodes only, and edges between them

\item \emph{Commit layer}: subset of the history layer consisting of revision
  nodes only, and edges between them

\item \emph{Hosting layer}: subset of the full corpus consisting of origins and
  snapshot nodes only, and edges between them

\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth,trim=1cm 1cm 1cm 1cm]{img/swh-layers}
    \caption{Logical layers used to refer to specific subsets of the graph.}%
    \label{fig:layers}
\end{figure}
\Cref{fig:layers} shows the various layers, which types of nodes belong to
each of them, as well as how edges connect them. In later chapters, we will
study properties of both the full graph of public software development and of
the specific subgraphs named above.


\subsection{Persistent identifiers}%
\label{sec:swhid}

The intrinsic hashes used to identify the nodes can be used to provide a
standardized and portable way of referring to the artifacts in the
archive~\cite{swhipres2018}. This is achieved by \glspl{SWHID},
% (IPA: /swiːd/)
short strings that identify a single object and which are guaranteed to remain
stable over time.  These identifiers were introduced in~\cite{cise-2020-doi}
and are documented in the Software Heritage development
documentation\footnote{\url{https://docs.softwareheritage.org/devel/swh-model/persistent-identifiers.html}}.
\glspl{SWHID} are represented in the following basic syntax:

\texttt{swh:<scheme version>:<object type>:<object id>}

The current version of the identifier scheme, stored in the second field, is
\texttt{1}. The third field contains a three-letter code for the object type
(\texttt{snp} for snapshots, \texttt{rel} for releases, \texttt{rev} for
revisions, \texttt{dir} for directories and \texttt{cnt} for contents). The
last field contains a hexadecimal-encoded intrinsic hash computed with the
SHA-1 algorithm on the content and metadata of the object.

Here are a few examples of \glspl{SWHID} and the objects they point to:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \texttt{swh:1:cnt:94a9ed024d3859793618152ea559a168bbcbb5e2} points to
        the content of a file containing the full text of the GPL-3 license.
    \item \texttt{swh:1:rev:309cf2674ee7a0749978cf8265ab91a60aea0f7d} points to
        a revision in the development history of the Darktable photography
        application.
    \item \texttt{swh:1:snp:c7c108084bc0bf3d81436bf980b46e98bd338453} points to
        a snapshot of the entire Git repository of Darktable as of May 4, 2017.
\end{itemize}

\glspl{SWHID} have interesting uses for research replicability: studies
citing software by URL can be subject to \emph{link rot}, which could make it
harder to run the software again as a way to independently verify the study
results. Because \glspl{SWHID} are persistent, using these identifiers to cite
scientific software is a way to ensure that the citation remains unambiguous
and resilient to URL changes. \glspl{SWHID} can be computed independently and
do not require a centralized authority for allocation, as they are solely
computed from the intrinsic properties of the object they refer to.

\section{Implementation}%
\label{sec:swh-infrastructure}

Although a full description of the physical implementation of the Software
Heritage archive is out of scope for this thesis, a cursory understanding of
some of its core systems is needed for later chapters, as we present frameworks
and tools built upon these foundations.

At a physical level, the archive is stored using a few different technologies,
due to the differences in technical requirements for storing the different
layers of the graph~\cite{swhipres2017}.
Most notably, the source contents require orders of magnitude more space
($\approx 850$ TiB as of May 2021) than the rest of the archive and are thus
stored on dedicated key-value object storage systems, keyed by their intrinsic
hash.  In the infrastructure hosted in-house by the Software Heritage
initiative, the object hashes allow for efficient horizontal sharding, by
assigning hash prefixes to specific servers. This technique is used to
trivially implement a reasonably performant and redundant storage.
Key-value object storage systems are an almost universal primitive in cloud
offerings, which allows Software Heritage to store an entire copy of the
content layers in two different clouds: the Microsoft Azure Blob Storage and
Amazon AWS S3.

On the other hand, the upper layers of the archive require significantly less
storage space ($\approx 10$ TiB) but have different functional requirements, as
they have more associated metadata (author names, revision messages, children
nodes, …) that need to be made searchable and joinable. This part of the graph
is stored in a \gls{RDBMS}, where nodes and edges have a few associated
relational tables that describe it. These systems can be leveraged to build
indexes, e.g., on the hashes stored in the different fields, which can then be
used to randomly access specific nodes and edges, and to efficiently
\emph{join} tables together to combine data from multiple node types by
following their links.

This approach can have scalability issues, as inserting data in traditional
\glspl{RDBMS} gets exponentially slower as the tables grow.  Other storage
systems for the graph are under study to circumvent this problem, notably the
use of key-value document storage systems like Cassandra or ScyllaDB\@.

One last relevant component is the \emph{journal}, which serves as a
synchronization pipeline across the infrastructure. In order to keep the
variety of storage backends (object storage, database, document stores…)
consistent together, the journal acts as a persistent log of the archive, where
all the ingested objects are pushed in order. All the backends and their
replicas can then subscribe to the journal and read its messages to keep
themselves up-to-date with the current state of the archive.
