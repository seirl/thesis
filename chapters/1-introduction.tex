\chapter{Introduction}

\TODO{Write a ``thesis guide'', describing in which chapter are we going to
find everything?}

\section{The rise of large-scale software mining}

\emph{Software mining} is a field of empirical software engineering which aims
to study datasets of extant software to uncover patterns and knowledge that can
help improve future software development~\cite{2006-zeller-msr}. By organizing
the knowledge obtained from software mining studies, researchers can build
models using statistics and machine learning techniques that can then be
queried to get design insights and analytics~\cite{hassan2006mining}, discover
bugs~\cite{williams2005automatic,BugRepair2017}, or even obtain a high-level
architectural view of how software components interact
together~\cite{hassan2008road}.

Software mining particularly shines in the context of \emph{software
evolution}~\cite{mens2008swevolintro,kagdi2007msrsurvey}, which studies the
dynamic behavior of software as it is maintained and enhanced over its
lifetime. The software engineering industry has a profound awareness of how
pervasive long-lived software is in the world's digital infrastructure, and
there is a clear need for research to be focusing not just on the software
source code itself, but also on its dynamic evolution over time.
Methodologically sound empirical studies have a particularly important role to
play as the basis for improving software maintenance tools, methods and
processes.

Historically, performing software evolution research was challenging and
focused on small amounts of data, one software project at a time. In one
seminal empirical work~\cite{belady1976model} in the '70s, Belady and Lehman
studied 20 releases of the OS/360 operating system and drew some observations
on the complexity growth of a large software project. This scale of study was
increasing slowly up until the late '90s, with Basili et
al.~\cite{basili1996understanding} studying 25 releases of around 100 different
software projects at NASA Goddard.  In a 1999
paper~\cite{kemerer1999empirical}, Kemerer and Slaughter highlighted the
inherent challenges of collecting empirical data to study software evolution:
researchers had to collect data at a minimum of two different points in time,
which required sustainable research efforts over a long period, or
collaborating with organizations that retained useful software measurement data
and development history.

These constraints drastically shifted in the past decades with the rise in
popularity of Free/Open Source Software
(FOSS)~\cite{syeed-2013-oss-evol-review} and collaborative development
platforms~\cite{kalliamvakou2014promises}. Developers have started making
publicly available a large wealth of \emph{software artifacts}: the source code
files and directories of the software projects, as well as their complete
development history over time and all its associated metadata, which have in
turn benefited empirical software engineering research fields such as software
mining and software evolution. This was made possible especially thanks to the
emergence of \glspl{VCS}~\cite{spinellis2005vcs}, collaborative software
development systems which track the history of development by retaining the
successive states of the source code over time. They have been frequently
analyzed~\cite{kagdi2007msrsurvey} due to the rich view they provide on
software evolution, and their ease of exploitation since the advent of
\glspl{DVCS}. The peer-to-peer approach to version control used by \glspl{DVCS}
makes it so that each user can retrieve the full development history locally,
which allows complex development
patterns~\cite{gousios2014pullrequests,gousios2015work} like ``branches'' and
``forks'' to be directly embedded inside the change history.

\section{Universal software mining}

\glspl{DVCS} hosting platforms have allowed researchers to easily retrieve and
process software repositories for mining purposes and made the literature on
software evolution abundant~\cite{herraiz2013evolution}. Yet, most present-day
evolution studies still focus on the evolutive patterns of \emph{individual}
software projects, which is particularly interesting to ascertain which factors
contribute to maximize software health~\cite{DBLP:conf/icse/2018soheal}.

Larger-scale studies can sometimes analyze hundreds or thousands of
repositories at the same time, but their approach has generally remained
limited in scope: researchers will tend to focus on some specific sampling
rule, e.g., retrieving the top ten thousands repositories in a specific
programming language from a particular hosting platform. This can be a
methodological issue for empirical research, as the most popular repositories
in the most popular platforms could be unrepresentative of development
practices at large.

At an even larger scale, some studies have been performed on up to the entirety
of a specific software ecosystem like app stores or package
managers~\cite{gonzalez2009macro,debsources-ese-2016}, but those were limited
to the granularity of software releases and did not study finer-grained history
data like commits.

Arguably, one of the next frontiers in the field would be to attain
\textbf{universal software mining}: the methodological analysis of software at
the largest scale possible down to a fine granularity of software artifacts.
At present, the largest scale possible is that of the \emph{software commons},
i.e., the body of all software which is available at little or no cost and
which can be reused with few
restrictions~\cite{1999-beagle-in-commons,kranich2008information}.

Unlocking this capability would have profound implications on the field of
software mining. First, it would considerably \textbf{reduce barriers to entry}
for empirical studies, by removing the need for researchers to manually
retrieve thousands of repositories from various sources to perform their
analyses: having access to the entire body of the software commons in a single
centralized location would allow them to simply request a subset of the data
they are interested in to perform their analysis, and obtain it in a format
that is convenient for data processing. It would also \textbf{enhance study
replicability}~\cite{Collberg2016,the-real-software-crisis,ferro2018sigir} by
providing an entry point to a static collection of already available data that
can be used to rerun studies at will.  In addition, it would significantly
\textbf{reduce potential sources of selection bias} by offering a
representative view of the software found in all different kinds of formats, on
diverse more or less popular hosting platforms, tracked by a variety of
\glspl{VCS}.  Finally, the exhaustiveness of the data would allow us to get a
\textbf{detailed high-level view of the social processes and interactions} that
govern software development, by looking at the global network of the
programming community in its entirety rather than focusing on a particular
subset.

Systematic initiatives~\cite{flossmole2006,gao2007archive,mockus2009}
have been established to gather as much public \gls{VCS} data as possible in a
single centralized place. Two recent initiatives, World of
Code~\cite{mockus2019woc} and Software Heritage~\cite{swhipres2017,
swhcacm2018} go one step further and aim to build a sustainable and accessible
archive of \emph{all} the source code artifacts of the software commons, down
to the level of the source code files. Even though full coverage w.r.t.\ the
software commons is by nature a moving target for any archive, the far-reaching
comprehensiveness and diversity of Software Heritage makes it a good
approximation for an exhaustive corpus of public software development; notably,
it covers more types of \glspl{VCS} and a wider array of data sources than
comparable corpuses~\cite{ma2021world}.

In this thesis, I explore the ways the body of software commons stored in the
Software Heritage archive can be organized and made accessible to researchers
for the purpose of universal software mining.

\section{Availability and accessibility}

To understand the motivations and design decisions behind the work presented in
this thesis, it is important to establish the difference between making data
\emph{available} and \emph{accessible}.  The Software Heritage initiative aims
to contribute to both of these aspects in different fashions.

Some of the data collected in the Software Heritage archive is already
\emph{available} inasmuch as it is at everyone's disposal by sole virtue of
being public. However, this availability is often temporary, as software data
sometimes get rewritten or deleted from their hosting places. Relevant
knowledge is routinely lost in code hosting platforms because their main
purpose is not to archive code, but rather host it. Because of this, other
history rewriting patterns like \texttt{git rebase} or ``force-pushes'' are not
observable on these platforms, but \emph{could} be made observable by an
archive which retains the past states of the repositories.

While making the data stored in the Software Heritage archive \emph{available}
is relevant to software mining research, it could be achieved simply by
publishing periodical hard drive dumps of the storage, as anyone would be able
to retrieve and exploit it.
What we aim to achieve by building a research platform for universal software
mining is to make the data not only available but also \emph{accessible}, which
means that the data should be presented in a way that is already suitable for
analysis purposes. Using the data should be convenient and frictionless,
as the platform only has value to researchers to the extent that its data can
be easily leveraged for many kinds of experiments. Merely providing raw data
dumps does not solve the accessibility problem, and what we need instead is a
comprehensive understanding of the needs of researchers in order to develop
reusable tools and infrastructures that can be built upon, instead of having
those be constantly rebuilt for one-off purposes. Simply put, the goal is to
build an \emph{abstraction} that allows researchers to absolve themselves from
the low-level technicalities of software mining to focus on their own
experiments.
