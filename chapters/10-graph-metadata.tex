\chapter{Property data in the compressed graph}%
\label{chp:graph-metadata}

\section{Introduction}

In \cref{chp:graph-dataset} we introduced the \SWHGD{}, a corpus made available
in relational formats suitable for large-scale analysis through scale-out
processing on large Big Data clusters.
In \cref{chp:graph-compression}, we proposed a method to compress the graph to
fit a few hundred gigabytes to efficiently run shared-memory algorithms on a
single machine.
These two approaches are designed to be complementary: scale-out processing is
particularly suited for embarrassingly parallel workflows and can process
terabytes of data in a few minutes with the appropriate scale factor; in-memory
graph compression can run more complex algorithms exploiting the recursive
structure of the graph (traversals, connected components, etc.) in a relatively
cheap way, albeit with longer runtimes of a few hours to a few days.

However, this latter approach is more limited in the kinds of data it can
exploit. The compressed graph only contains the topological structure (nodes
and arcs) of the graph, whereas the graph dataset in relational format can be
used to study the entire \emph{property graph} of development history, i.e.,
not only the graph structure of the dataset but also the properties associated
to each nodes and edges (e.g., commit messages, timestamps, file names, etc.).
Because of this, recursive graph queries cannot use graph properties during
their traversals. This can be worked around by performing graph queries
without using the properties themselves, then \emph{joining} the results with
data from the relational property graph, but this can be significantly more
expensive and time-consuming.
As an example, let us consider the following query:

\textbf{Query 1}: \emph{Given a blob object, find the earliest revision in
which this blob can be found anywhere in its source code tree.}

If the commit timestamp property can be accessed ``on the fly'' during graph
traversal, this query can be answered rather straightforwardly: for each
revision in the transitive closure of the blob in the transposed graph, find
the one with the lowest timestamp. However, if these properties are only
available externally, the process is more involved: it requires returning the
entire transitive closure from the blob, then joining it with the timestamps,
which can require transferring very large amounts of nodes to join with the
timestamps (up to significant fractions of the entire graph, e.g., for the
empty file).

The added complexity gets even worse when the traversed edges are conditional
on the properties themselves. Consider the following query:

\textbf{Query 2}: \emph{Given a snapshot, find all the unique blobs which
were present at the path \texttt{src/test/README.txt} at any point in its
development history.}

With direct access to the properties of the graph, the algorithm is again
straightforward: (1) traverse the revision history (2) for each revision,
follow the given path in its source tree (3) add the destination blob to the
resulting set if not present. This only requires following a single path of
fixed depth for each source code tree, for a time complexity of $O(|R|)$ where
$R$ is the set of revisions in the transitive closure of the snapshot. In
contrast, when the file and directory names of the graph edges are not directly
accessible, one needs to return the \emph{entire transitive closure} ($O(|N|)$)
of the snapshot to later join this data with the graph properties.

Whether the compressed graph is used only for selecting artifacts for the
purpose of performing subsequent scale-out analyses on the extracted data, or
directly used for the analyses themselves, having direct access to the graph
properties from the compressed format would significantly improve the
efficiency of more complex graph queries.
In this chapter, we look at ways to bridge this potency gap between the two
formats by enabling access to node and edge properties from the compressed
graph.

\section{Design and memory considerations}

% Move after ?

We export graph properties as a set of files which live next to the compressed
graph, one file per property. The specific formats used for these property
files will be detailed in later sections. In this section we discuss time and
memory considerations surrounding access to these properties from the
compressed graph.

As one might expect, in the graph of software development the vast majority of
storage space is occupied by the properties themselves; the structure of nodes
and arcs is just a small portion of the entire data usage. This highlights an
important speed/memory trade-off: it is highly unlikely that the entire
property graph can be realistically stored in memory in the same way as the
graph structure itself. Therefore, it makes sense for the properties to be
stored offline by default (presumably on-disk) and then loaded on demand
when the analysis requires it.

More specifically % TODO


\section{SWHID mappings}

The input of the graph compression is a list of nodes and edges referenced by
their \glspl{SWHID} in a format described in \cref{sec:edges-format}. These
\glspl{SWHID} are the unique identifiers of the objects in the graph, and are
of prime importance to be able to relate the nodes being manipulated in the
compressed representation with the archived artifacts themselves.

As detailed in \cref{sec:compression-pipeline}, the graph compression pipeline
works by mapping each unique \gls{SWHID} to a set of $\{0,\ldots,N-1\}$
consecutive integers, where $N$ is the number of nodes in the graph by
computing a \emph{minimal perfect hash function}~\cite{GOVFSCF}.
After this hashing step, the \gls{SWHID} information is lost, and all the nodes
are subsequently referred to as their integer identifier from the contiguous
set. To restore the ability to associate the graph nodes with the objects they
refer to, we need to construct mapping functions that allow to translate
between integer node IDs and \glspl{SWHID} in both directions.

\subsection{SWHID $\to$ Node ID}%
\label{sec:swhid2node}

The natural way to map \glspl{SWHID} to the compressed node IDs is to reuse the
minimal perfect hash function which was used to map the nodes in the first
place.
This hash function is stored as a \texttt{graph.mph} file (which weighs
around 5\,GiB for the 2020-12-15 version of the graph) and can translate the
input \glspl{SWHID} to node IDs in this set. However, during the compression
pipeline the graph is \emph{reordered} to achieve better compression results,
as described in \cref{sec:compression-pipeline}. When compressing with the
\gls{LLP} algorithm, the graph is even reordered twice: once for the initial
BFS-based compression, and then once for the LLP-based compression. As a
consequence of these reorderings, the node IDs obtained from feeding
\glspl{SWHID} to the \gls{MPH} function no longer correspond to their matching
output nodes.

A byproduct of the compression pipeline are ``order files'' which define
the \emph{permutations} of each reordering step. Their on-disk format is a
binary array of integers in which the integer at position $x$ is $p(x)$ where
$p$ is the permutation of the reordering step. As such, the \gls{SWHID} $\to$
node ID mapping can be obtained by composing the MPH with the subsequent
reorderings. As a preliminary step, if the graph was permuted multiple times
(as is the case for \gls{LLP}), we compose the \texttt{.order} files to keep a
single permutation $p_G$, which corresponds to the resulting permutation of
applying successively all the compression permutations of the graph:

\begin{itemize}
    \item For BFS-based compression: $p_G(i) = p_{\mathrm{BFS}}(i)$
    \item For LLP-based compression:
        $p_G(i) = (p_{\mathrm{LLP}} \circ p_{\mathrm{BFS}})(i)$
\end{itemize}

This lowers memory usage by reducing the number of order files necessary for
the translation to node IDs. For reference, an order file representing a
permutation for 19.3 billion nodes (the size of the 2020-12-15 dataset) weighs
around 145\,GiB ($\approx 19.3 \times 10^9 \times 8 \times \frac{1}{2^{30}}$
for 64-bit node IDs).
This composed permutation (or ``graph permutation'', because it represents how
the graph is permuted from the original order of the nodes given by the hashing
function) is stored as a \texttt{graph.order} file in the same format than the
input permutations.

Then, the mapping function for the \gls{SWHID} $\to$ node ID direction can be
defined from the \gls{MPH} and the composed permutation:

\[\mathrm{swhid2node}(s) = p_G(\mathrm{mph}(s))\]

In other words, getting the node ID associated with a given input SWHID can be
done through the following two-step process:

\begin{enumerate}
    \item Hash the SWHID $s$ using the \gls{MPH} function loaded from the
        \texttt{graph.mph} file to obtain $k_0$, the node ID from the initial
        graph order.
    \item Take the image of $k_0$ from the graph permutation stored in the
        \texttt{graph.order} file to obtain the node ID $k$.
\end{enumerate}

The storage space taken by the entire mapping is the on-disk size of the
permutation plus the on-disk size of the hash function, which is $\approx
150$\,GiB in the 2020-12-15 dataset. This is comparable to the size of the
graph itself (134\,GiB for the direct graph only). This size could be reduced
by compressing the \texttt{.order} file, which could be achieved by reducing
the number of bits used for each entry in the permutation to the bare minimum
necessary to represent an integer in the set of node IDs. This would come at
the cost of some inefficiency due to the overhead of correcting word
misalignment, which would potentially be negligible.
For the same example, the estimated size would then be:

\[
    n \times \left\lceil{\frac{\log_2(n)}{8}}\right\rceil\,\mathrm{bytes}
    = 19.3 \times 10^9 \times
        \left\lceil{\frac{\log_2(19.3 \times 10^9)}{8}}\right\rceil
        \times \frac{1}{2^{30}}
    \approx 90\,\mathrm{GiB}
\]

\subsection{Node ID $\to$ SWHID}%
\label{sec:node2swhid}

Now that we are able to easily convert \glspl{SWHID} to their corresponding
node IDs in the compressed graph, we can build a \emph{reverse mapping} which
allows us to retrieve the \gls{SWHID} preimage of any node ID found in the
compressed graph. This reverse mapping is the inverse function
$\mathrm{node2swhid}$ (= $\mathrm{swhid2node}^{-1}$) of the mapping
described in the previous section.

This function is relatively easy to represent on-disk for two reasons. First,
its input domain is a contiguous set of integers ${0,\ldots,N-1}$, which means
the function does not require any hashing scheme to associate images to its
inputs; they can simply be stored as a contiguous array. Second, because the
images are \glspl{SWHID} they all have a fixed size, which facilitates random
access in the array.

This reverse mapping is stored in a file called \texttt{graph.node2swhid.bin},
a binary file containing a contiguous binary sequence of records, each record
representing a \gls{SWHID}. The binary format used to represent a \gls{SWHID}
as a 22-byte long byte sequence is constituted of:

\begin{itemize}
    \item 1 byte for the namespace version, represented as a C \emph{unsigned
        char}
    \item 1 byte for the object type as the integer value of a software
        artifact type enum (0 = content, 1 = directory, 2 = origin, 3 =
        release, 4 = revision, 5 = snapshot), represented as a C \emph{unsigned
        char}
    \item 20 bytes for the SHA-1 digest represented as a byte sequence.
\end{itemize}

This mapping can be generated relatively straightforwardly by (1) allocating a
binary file of $22 \times n$ bytes, (2) iterating on the list of all the
\glspl{SWHID} used to compress the graph, (3) for each \gls{SWHID}, getting
$k$, its image node ID, using the mapping described in \cref{sec:swhid2node},
(4) writing the 22 bytes binary record representing the \glspl{SWHID} at the
offset $22 \times k$ in the binary file.

Once this mapping is written to disk, the \gls{SWHID} of a node $k$ can be
retrieved by simply reading the 22-bytes binary record at index $22 \times k$.
A \texttt{graph.node2swhid.bin} file mapping 19.3 billion node IDs (the size of
the 2020-12-15 dataset) to their corresponding \glspl{SWHID} weighs
around 395\,GiB ($\approx 19.3 \times 10^9 \times 22 \times \frac{1}{2^{30}}$).

\subsection{Domain checking}

One caveat of the approach described in \cref{sec:swhid2node} to map
\gls{SWHID} $\to$ node ID is that it does not do any kind of checking on its
input domain: an unknown/invalid \gls{SWHID} will be accepted by the hash
function, which will return an arbitrary integer in its image ${0,\ldots,N-1}$
instead of throwing an error. This is problematic for some use cases: if the
compressed graph is exposed as an API, it needs to be able to reject queries
using unknown \glspl{SWHID}, instead of silently computing a garbage result on
the wrong input node without any way for the user to notice.

While it is possible to externally check whether a \gls{SWHID} is present in
the Software Heritage archive by querying its API, this is not sufficient for
this use-case. The compressed graph is not built incrementally but is instead a
\emph{static} export of the state of the archive at a given point in time. This
means that all the nodes added to the archive during the time period between
the graph compression and the present are unknown to the compressed graph.

One option to add checks to the input domain would be to add \textbf{signing} to
the \gls{MPH} function. By associating a signature of $w$ bits to each input
key of the \gls{MPH} function, it is able to detect input strings which were
not in the original key set. This approach is probabilistic, and false
positives are possible with a probability of {\Large $\frac{1}{2^w}$}. While
this is useful to catch bugs, a probabilistic approach does not allow us to
systematically fallback when encountering an unknown \gls{SWHID} unless using
very large signatures. Those can also dramatically increase the size of the
hashing function: for the 2020-12-15 dataset, a 32-bit signatures increases its
size from 5\,GiB to 72\,GiB while still allowing a false positive every 4
billion \gls{SWHID} on average, which is less than a quarter of the number of
objects in the graph.

Instead, we consider another option based on \textbf{round-trips} on the
mapping from \cref{sec:swhid2node} and reverse mapping from
\cref{sec:node2swhid}.  Because both mappings are bijections which are the
inverses of each other, it follows that for every \gls{SWHID} $s$ in the input
key set of the \gls{MPH}, $\mathrm{node2swhid}(\mathrm{swhid2node}(s)) = s$.
In contrast, if $s$ \emph{is not} in the input domain, $\mathrm{swhid2node}(s)$
will return an arbitrary node, and
$\mathrm{node2swhid}(\mathrm{swhid2node}(s))$ will then return a random
\gls{SWHID} \emph{from the input domain}, which thus cannot be equal to $s$.

Given this, it is possible to create a function which maps \gls{SWHID} $\to$
node IDs \emph{with domain checking} by performing this ``round-trip'' to
verify that the node is in the input domain, and returning an error otherwise:

\[
    \mathrm{swhid2nodeCheck}(s) =
    \begin{cases}
        \mathrm{swhid2node}(s) &
        \text{\textbf{if}}~\mathrm{node2swhid}(\mathrm{swhid2node}(s)) = s,\\
        \text{-1}            & \text{\textbf{otherwise.}}
    \end{cases}
\]

\section{Node properties}

For the reasons outlined in \cref{sec:node2swhid}, mapping nodes to properties
in external storage is generally a relatively easy feat. Because the nodes IDs
in the compressed graph are consecutive integers in the range ${0,\ldots,N-1}$,
mapping each node to a given property is equivalent to creating a random-access
list of properties, in which the property at index $k$ is the property
associated with the node $k$.

The best way to achieve this depends on the type and characteristics of each
property, detailed in \cref{sec:relational-model}. We can use these to organize
the properties in different categories:

\paragraph{Node types}

In \cref{sec:compression-comp-scope} we already mentioned keeping \emph{node
types} in memory, i.e., whether a node is a blob, directory, revision, etc.
Node types have a special status because that they are necessary for almost all
kinds of analyses on the graph as they allow to determine at runtime which
kinds of objects are being traversed. They are also
already present as part of the textual identifiers of the nodes themselves,
being one of the fields in \gls{SWHID}. For that reason, we systematically
load this node type mapping in memory along with the graph, contrary to the
other mappings which have to be loaded on demand.
This mapping is implemented very efficiently as a bit array indexed by integer
node identifiers, with records of only 3 bits per node (as there are 6 node
types in total), which amounts to around 7\,GB of RAM for the 2020-12-15 graph.

\paragraph{Integer properties}

The following properties are simple integers, either 16, 32 or 64 bits:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \texttt{content.length}: length of a blob
    \item \texttt{revision.date}: timestamp of when a revision was authored
    \item \texttt{revision.date\_offset}: timezone offset of when a revision
        was authored
    \item \texttt{revision.committer\_date}: timestamp of when a revision was
        committed
    \item \texttt{revision.committer\_date\_offset}: timezone offset of when a
        revision was committed
    \item \texttt{release.date}: timestamp of when a release was authored
\end{itemize}

Storing these integer properties in property files is done in a similar way
to storing permutations in order files, which we described in
\cref{sec:swhid2node}. Because integers have a fixed width, these properties
can be stored relatively straightforwardly as binary array of integers in which
the $k$-th integer corresponds to the value of the property for node $k$.  The
resulting property files have a size of {\Large $n \times \frac{w}{8}$}, where
$n$ is the number of nodes in the graph, and $w$ the number of bits of the
integer field (16, 32 or 64). For the 2020-12-15 export, these bit sizes
correspond respectively to property files of size 35\,GiB, 71\,GiB and
145\,GiB.

\paragraph{Persons}

The following properties represent ``persons'', which are stored as records
containing a full name and an email address.\footnote{Like projects, persons
are ontologically complex objects and cannot be solely described by a name and
an email: people have multiple emails, change names, can misspell their own
name, etc. Past works have attempted to map authorship information to a
canonical concept of ``persons'' by merging duplicate information to single
identities~\cite{wiese2016mailing, zhu2019empirical}. For the purposes of this
thesis, we call ``person'' a unique pair of full name and email address, and
leave identity deduplication in the hands of the users of the platform.}

\begin{itemize}
    \setlength\itemsep{0em}
    \item \texttt{revision.author}: author of a revision
    \item \texttt{revision.committer}: committer of a revision
    \item \texttt{release.author}: author of a release
\end{itemize}

Those fields are natively represented as byte sequences in the relational
format.  However, two factors come in play to figure out the best way to store
these properties for the compressed graph. First, there are considerably fewer
unique persons than there are objects referring to persons: only 42 million
persons in the 2021-03-23 graph export, whereas there are 2 billion objects
containing person fields (revisions and releases), which highlights the
usefulness of having some deduplication of unique authors. Second, as discussed
in \cref{sec:relational-model}, the research use cases we want to support are
not those exploiting the names and e-mails of the authors themselves (which we
anonymize anyway), but rather those trying to relate identical authors, e.g.,
for the purpose of analyzing the structure of social coding networks.

In consequence, the only data we actually want to provide for this property is
a unique identifier of a given person, which will then be used in place of the
full name and email and can be compared across different objects.
Because the graph is static, persons can be mapped to integers in the range
$\{0,\ldots,N-1\}$ where $N$ is the total number of persons referenced in the
graph artifacts. This is once again achieved with the use of a \gls{MPH}: we
first extract the list of unique persons in the graph, then compute a minimal
perfect hash on the full names and e-mails to associate each person to a unique
integer. The integer array is then stored in binary format using the method
described above for regular integer fields. There is no need to build and
provide a reverse mapping for the anonymization reasons explained above.

\paragraph{Text}

Finally, the most complex type of node fields in the property graph is the
textual type, or rather, arbitrary sequences of bytes. The following properties
use this byte sequence type:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \texttt{revision.message}: descriptive message of a revision
    \item \texttt{release.message}: descriptive message of a release
\end{itemize}

Because these fields need to be properly readable (i.e., one should actually be
able to get the entire byte sequence from a node, contrary to persons), there
is not much optimization to be done for this case. We can use an arbitrary
on-disk string list which maps each node ID to its property value, as long as
it allows random access. For this purpose we use the
\texttt{FrontCodedStringBigList}
class\footnote{https://dsiutils.di.unimi.it/docs/it/unimi/dsi/big/util/FrontCodedStringBigList.html}
from the \texttt{dsiutils} library\footnote{https://dsiutils.di.unimi.it/} used
in the WebGraph framework. It contains an index table to efficiently jump to
the $k$-th bytestring in the array.
For the 2021-03-23 export, the total size of all the byte strings we need to
store is 159\,GiB, which is a manageable amount.  However,
\texttt{FrontCodedStringBigList} objects have to be loaded entirely in memory,
which can be a significant memory burden. Improving this by making it possible
to memory-map this data structure from disk is left open as a future
implementation direction.

\section{Edge properties}
