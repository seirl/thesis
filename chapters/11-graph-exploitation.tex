\chapter{Graph exploitation for software mining}%
\label{chp:graph-exploitation}

\TODO{Intro linking all the sections together.}

\section{Working with graph subsets}

So far, we have worked on compressed graph representations as a way to make
running analyses on the entire graph of development history manageable for
researchers using commodity hardware. While the compression ratio achieved is
impressive, the entire graph is often too computationally expensive to analyze
to be practical for many research use cases.  This is especially true for
prototyping, when the research is still at an exploratory stage and analysis
code is quickly iterated upon.

This problem can be addressed by providing less cumbersome \emph{graph
subsets}, i.e., smaller yet coherent collections of software artifacts which
can be used to perform analysis at a more reasonable scale. A tangentially
related problem of focusing analysis on a pre-narrowed logical subset of data,
i.e., only analyzing repositories matching some specific criteria, can also be
tackled by exporting ``subdatasets'' which only contain the relevant data to
process.

This section presents ways to provide subsets of the graph data in a way that
can be properly exploited for software mining, by leveraging the compressed
graph to select and export relevant software artifacts.

\subsection{Selecting artifacts of interest}

Evidently, to provide an exploitable subdataset of software development data,
it is not sufficient to uniformly extract random nodes from the graph. Doing so
would not preserve the logical structure linking software artifacts together
and make the graph extremely disconnected, which would render most of the
analysis results non-generalizable and of generally little value.
%
A better approach to generate logically coherent subdatasets is to always
export entire \emph{repositories} at once, i.e., the entire transitive closure
of a given set of origins. This minimizes the number of dangling links and
loose objects, and is generally an intuitive and expected way to construct
logical subsets of software mining data, as it closely matches the way the
entire dataset was built in the first place.

This reduces the artifact selection problem to selecting the list
\emph{origins} to be included in the subdataset.
Often, researchers will have a predetermined set of repositories of interest
(see \cref{sec:mining-selection-criteria}), which can be used to compute the
transitive closure of relevant artifacts. In other cases, the list of URLs
can be obtained externally using specific criteria, such as taking all the
repositories from a given hosting place, or all those with a minimum number of
stars. Finally, if the objective is to get a representative subset of origins
present in the archive, this can be achieved with uniform random sampling on
the list of origins in the archive.

To choose the appropriate number of origins to include in the subdataset, we
need a heuristic to estimate the size of the resulting graph given the number
of origins used to compute the transitive closure. Because of deduplication,
the size of the resulting graph does not scale linearly with the number of
origins: the more origins get included in the subgraph, the more likely it is
that the nodes in its transitive closure were already present from the closure
of another origin.

To attempt to measure this effect, we run an experiment on the entire
compressed graph: we (1) shuffle all the origins of the graph in a uniform
random order, (2) for each origin, lookup its transitive closure (3) once every
10k origins, tally the number of unique objects visited so far. The resulting
data is shown plotted in in \cref{fig:subdataset-size-function}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/graph-exploitation/subdataset_size_function}
    \caption{Proportion of nodes and edges obtained after sub-sampling the
        graph with a given number of origins. The x-axis represents the
        percentage of origins included in the subdataset; the y-axis represents
        the proportion of objects from the full dataset obtained in the
    resulting subdataset.}%
    \label{fig:subdataset-size-function}
\end{figure}

As expected, the proportion of objects present in the resulting subdataset is
not proportional to the number of origins due to sharing effects. Instead,
the number of included objects has a sharp initial growth rate when few origins
have been visited, and this growth tapers off as a larger share of the graph
has already been visited.

This data can be fitted in a logarithmic model of the form:

\[S(x) = a \log(1+bx^c)\]

A curve fitting algorithm minimizing the sum of squared residuals yields the
following coefficients for the subsampling heuristic, respectively for the
number of nodes and edges in the resulting subdataset:

\[
\begin{cases}
    S_V(x) = 500 \times \log(1 + 0.002 \times x ^{0.6}) \\
    S_E(x) = 250 \times \log(1 + 0.004 \times x ^{0.5}) \\
\end{cases}
\]

This model can be used to predict the size of the resulting dataset when
selecting a given sample size of origins, expressed as a proportion of the
total number of origins in the original dataset. As an example, if one wanted
to predict the number of nodes and edges in a subdataset containing a sample of
20\% of the total number of origins, replacing 20\% in the above model
yields the following values:

\[
\begin{cases}
    S_V(20\%) = 500 \times \log(1 + 0.002 \times 0.2 ^{0.6}) = 38\% \\
    S_E(20\%) = 250 \times \log(1 + 0.004 \times 0.2 ^{0.5}) = 44\% \\
\end{cases}
\]

Therefore, a subdataset generated from a random sample of 20\% of the origins
will contain 38\% of the nodes and 44\% of the edges of the entire graph.
In order to target a specific amount of data in the subdataset (e.g., limiting
the size of the subdataset to less than \num{100000} nodes), the inverse of the
model can be used:

\[
\begin{cases}
    S_V^{-1}(x) = {\left(\cfrac{1}{0.002}~e^{\frac{1}{500}x}-1\right)}^{\frac{1}{0.6}} \\
    S_E^{-1}(x) = {\left(\cfrac{1}{0.004}~e^{\frac{1}{250}x}-1\right)}^{\frac{1}{0.5}} \\
\end{cases}
\]

Replacing $x$ in the model above with the target number of nodes or edges will
return a sample size of origins which is predicted to generate a subdataset
containing the given number of nodes or edges.

\subsection{Exporting subdatasets}

After having selected a subset of origins of interest (either with random
sampling or manual selection using various criteria), the next logical step is
to materialize the subdataset in a way that is suitable for a subsequent
analysis.
For this, we first need to compute the transitive closure of the set of all the
selected origins using the compressed graph. Once the subset of nodes has been
narrowed down, it becomes possible export the subdataset in the various formats
presented in \cref{chp:graph-dataset}.

By reading node and edge properties as described in
\cref{chp:graph-metadata}, it is relatively straightforward to export the
subdataset in the input edges format (\cref{sec:edges-format}), then recompress
it as a new graph using the graph compression pipeline
(\cref{chp:graph-compression}). The newly exported subgraph is then usable for
small-scale experiments.

Afterwards, the list of \glspl{SWHID} obtained from the compressed graph
traversal can be used to export the subdatasets in columnar format. Scale-out
processing tools like Amazon Athena can perform large JOINs to compute the
intersection between the full dataset and the set of \glspl{SWHID} of the
subdataset, and write the result in columnar format compatible with the
original graph dataset.

Using these techniques, we exported and made available a few ``teaser''
subdatasets containing a small set of origins selected using different
criteria, which can be used for prototyping with minimal hardware requirements.
These subdatasets illustrate different ways the \SWHGD{} can be used to build
exploitable datasets focused on narrowed data that is particularly relevant for
some specific research.

\paragraph{popular-4k} a subset of 4000 popular repositories from
GitHub, GitLab, PyPI and Debian. The selection criteria to pick the software
origins was the following:

\begin{itemize}
    \setlength\itemsep{0em}
    \item The 1000 most popular GitHub projects (by number of stars)
    \item The 1000 most popular Gitlab projects (by number of stars)
    \item The 1000 most popular PyPI projects (by usage statistics, according
        to the Top PyPI Packages
        database\footnote{\url{https://hugovk.github.io/top-pypi-packages/}}),
    \item The 1000 most popular Debian packages (by “votes” according to the
        Debian Popularity Contest
        database\footnote{\url{https://popcon.debian.org/}})
\end{itemize}

The resulting dataset is made available in the Apache Parquet and CSV formats,
with respective sizes of 23\,GiB and 27\,GiB, as well as on the Amazon Athena
query engine.

\paragraph{popular-python-3k} a subset of 3052 popular repositories tagged as
being written in the Python language from GitHub, GitLab, PyPI and Debian. The
selection criteria to pick the software origins was the following, similar to
popular-4k:

\begin{itemize}
    \setlength\itemsep{0em}
    \item The 1000 most popular GitHub projects written in Python (by number of
        stars)
    \item The 131 Gitlab projects written in Python which have 2 stars or more
    \item The 1000 most popular PyPI projects (by usage statistics, according
        to the Top PyPI Packages database)
    \item The 1000 most popular Debian packages with the debtag
        \texttt{implemented-in::python} (by “votes” according to the Debian
        Popularity Contest database)
\end{itemize}

The resulting dataset is made available in the Apache Parquet and CSV formats,
with respective sizes of 4.7\,GiB and 5.3\,GiB, as well as on the Amazon Athena
query engine.

\paragraph{gitlab-100k} a subset of \num{100000} repositories from GitLab,
hosted at the main \texttt{gitlab.com} instance, sampled using a uniform random
distribution. This dataset is made available as a compressed graph, containing
304 million nodes and 9.5 billion edges, for a total size of 6.8\,GiB (3.6\,GiB
for the direct graph and 3.2\,GiB for the transposed graph).

\paragraph{gitlab-all} a subset of every repository from GitLab, hosted at the
main \texttt{gitlab.com} instance. This dataset is made available as a
compressed graph, containing 1.0 billion nodes and 27.9 billion edges, for a
total size of 20.6\,GiB (9.6\,GiB for the direct graph and 11\,GiB for the
transposed graph).

\subsection{Subgraph overlays}

While exporting entire subdatasets is extremely useful for a lot of research
use cases, as it reduces the size of the datasets so that they only contain the
relevant data for these needs, the process is quite slow. Running the full
compression pipeline, including all the graph metadata, for even 10\% of the
graph can take more than a week.

A lot of research use cases involve working on a subset of the data, but not
necessarily as a way to reduce the memory usage but simply to filter out
irrelevant nodes from the analysis. As an example, computing the connected
components of the subgraph containing only revision nodes, while doable by
recompressing a revision subgraph, can also be done by simply ``masking''
non-revision nodes to the algorithm. Providing \emph{views} on subsets of the
graph which can mask irrelevant nodes is an effective alternative to
reexporting subdatasets for some types of workflows.

We propose a few different ways to wrap the compressed graph with a subgraph
overlay which can be used to mask nodes not present in the subgraph. The
overlay always has the same API as the original graph and can be used in its
place in any graph analysis algorithm.

\paragraph{Subgraph}
This subgraph overlay is already present as an experimental library in the
WebGraph framework (\texttt{ImmutableSubgraph}). It requires to be provided a
set of node IDs to include in the subgraph. In order to keep its node IDs in a
continuous range, the class remaps all the node IDs between the original graph
and the subgraph, as seen in \cref{fig:immutablesubgraph}. Two methods can be
used to convert the node IDs back and forth: \texttt{toSupergraphNode} and
\texttt{fromSupergraphNode}. These methods are particularly useful to retrieve
node properties from the subgraph, as these properties are indexed with the IDs
of the supergraph.


\paragraph{LazySubgraph}
In some use cases, whether a node is included or not in a subgraph can be
expressed as a simple function (e.g., ``is the node a revision?'' to create a
subgraph only containing revision nodes). Instead of writing the set of nodes
included in the subgraph, the \texttt{LazySubgraph} class determines on the fly
during iteration whether a node is part of the subgraph or not. This class does
not remap any of the node IDs, and leaves holes for masked nodes, as seen in
\cref{fig:lazysubgraph}. Programs using this subgraph overlay must be aware
that node IDs are not contiguous; however, this simplifies access to graph
properties by preserving node IDs between the subgraph and the supergraph.

\paragraph{EliasFanoSubgraph}
Lastly, another option would be to use a succinct data structure~\cite{NavCDS}
such as an \emph{Elias-Fano}~\cite{EliESRCASF} integer list to store the
bijection between subgraph and supergraph nodes (see
\cref{sec:graph-compression-techniques}). This representation is particularly
suited for this purpose, as both sets of nodes are monotone lists of increasing
integers. This approach was not implemented yet and is left open as a future
research direction.

\tikzstyle{memcell}=[draw, minimum width=2em, minimum height=2em, outer sep=0pt]
\tikzstyle{memcells}=[
    matrix of math nodes,
    nodes={style=memcell, anchor=center},
    column sep=-\pgflinewidth]

\begin{figure}
    \centering
    \begin{tikzpicture}
        \matrix (Sub) [memcells, label=left:{Subgraph nodes}]
            {0 & 1 & 2 & 3 & 4\\};
        \matrix (Super) [memcells, label=left:{Supergraph nodes},
                        below=2cm of Sub.west, anchor=west]
            {0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\};
        \draw[style=arrow, out=-90, in=90] (Sub-1-1.south) to (Super-1-2.north);
        \draw[style=arrow, out=-90, in=90] (Sub-1-2.south) to (Super-1-4.north);
        \draw[style=arrow, out=-60, in=120] (Sub-1-3.south) to (Super-1-7.north);
        \draw[style=arrow, out=-30, in=130] (Sub-1-4.south) to (Super-1-8.north);
        \draw[style=arrow, out=-30, in=140] (Sub-1-5.south) to (Super-1-10.north);
    \end{tikzpicture}
    \caption{The ImmutableSubgraph overlay maps node IDs from the set of nodes
    in the subgraph to nodes in the supergraph.}%
    \label{fig:immutablesubgraph}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \matrix (Sub) [memcells, label=left:{Subgraph nodes}]
        {|[fill=lightgray]| & 1 &|[fill=lightgray]| & 3 &|[fill=lightgray]| &
         |[fill=lightgray]| & 6 & 7 &|[fill=lightgray]| & 9\\};
        \matrix (Super) [memcells, label=left:{Supergraph nodes},
                        below=2cm of Sub.west, anchor=west]
            {0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\};
        \draw[style=arrow] (Sub-1-2.south) to (Super-1-2.north);
        \draw[style=arrow] (Sub-1-4.south) to (Super-1-4.north);
        \draw[style=arrow] (Sub-1-7.south) to (Super-1-7.north);
        \draw[style=arrow] (Sub-1-8.south) to (Super-1-8.north);
        \draw[style=arrow] (Sub-1-10.south) to (Super-1-10.north);
    \end{tikzpicture}
    \caption{The LazySubgraph overlay conserves node IDs between the subgraph
    and the supergraph, leaving gaps in the node list.}%
    \label{fig:lazysubgraph}
\end{figure}


\section{Querying the graph}
