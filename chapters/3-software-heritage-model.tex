\chapter{The Software Heritage Graph}

% This chapter describes the abstract data model of the Software Heritage
% archive.

\section{Canonical Software Artifacts}

The Software Heritage archive is continuously ingesting software artifacts from
a wide array of sources, including different VCS and package managers that
each have their own internal data model. The main purpose of the Software
Heritage data model is to provide a generic structure in which the individual
object types specific to each system can be mapped to abstract concepts.  Git
``commits'', SVN ``revisions'' and Mercurial ``changesets'' all correspond to
the same idea of a frozen state of the source tree, and thus they are all
\emph{canonicalized} as a single type of artifact that we call ``revisions''.

By stripping the implementation peculiarities of the individual data sources,
the artifacts are boiled down to a purely abstract form. This unifies the
representation of all the artifacts stored in the archive, which is
particularly interesting for research: since all the artifacts are already
stored in canonical form, we can provide a uniform interface for researchers
to study artifacts coming from a variety of different sources. This isolates
the complexity of handling artifacts sourced from different \glspl{VCS} behind
an abstraction layer; researchers can then run analyses on the abstract
artifacts instead of having to deal with each specific system.

The following kinds of canonical software artifacts are supported in the data
model:

\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node[style=content,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Contents}} (or ``blobs'') are the raw content of the source
code files. They represent only the concrete binary data contained in the
files; file \emph{names} are external and directory-dependent, and are not
included in this type of artifact. Contents are identified by an intrinsic hash
of the full binary data they contain.

\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=directory,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Directories}} are the source code trees. They contain a
list of \emph{named} directory entries, each entry pointing to content objects
(``file entries''), other directories (``directory entries''), or revisions
(``revision entries''). Each entry is associated with a local binary name
(i.e., a relative path without any path separator) and permission metadata.
Revision entries are used to encode directories referencing a specific
revision, for instance Git submodules and Mercurial subrepositories.  The
permission integer can encode arbitrary permissions such as whether the file is
an executable, a symbolic link, who can read or write in it, etc.  They are
identified by an intrinsic hash of the textual manifest of all their entries
and their own identifiers.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=revision,scale=1.3] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Revisions}} (or ``commits'') are point-in-time captures of
the state of the entire source code tree of a project. Each revision points to
the ``root'' directory of the project source tree. In addition, revisions are
associated with the following commit metadata:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \emph{commit message}: a descriptive message generally detailing the
        changes made since the previous revision.
    \item \emph{author}: the name and e-mail of the person who authored the
        revision.
    \item \emph{committer}: the name and e-mail of the person who applied the
        revision to the project on behalf of the original author. Often this
        field contains the same information as the author field.
    \item \emph{date}: the date at which the revision was authored, including
        timezone information.
    \item \emph{committer date}: the date at which the revision was applied to
        the project, including timezone information. Often the same as the date
        field.
\end{itemize}

Finally, each revision points to an ordered list of all its parent revisions,
referenced using their own intrinsic identifier. The order does not have a
strict semantic meaning, it is mostly used as a guide by tools that show the
difference between two revisions. The first parent is generally considered to
be issued from the ``main'' branch that the revision is merged onto, and thus
diffing tools will show the impact of this revision on the main branch by
default.

Revisions are identified by an intrinsic hash of a textual manifest containing
all their metadata, the hash of their parents, and the hash of the root
directory of the source tree they reference.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=release,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Releases}} (or ``tags'') denote marker objects that label
specific revisions as project milestones. These usually denote the revisions
where the software is distributed to its user base, as well as the various
steps of its release cycle. These releases are marked with a specific and
usually mnemonic short name (e.g., a version number). Aside from this name and
a reference to their target revision, releases additionally contain some
metadata:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \emph{message}: an annotation generally describing the release, e.g.,
        by including its full changelog.
    \item \emph{author}: the name and e-mail of the person who authored the
        release.
    \item \emph{date}: the date at which the release was created, including
        timezone information.
\end{itemize}

The data model also supports revisions pointing to other kinds of artifacts
(contents and directories), which is supported by some \glspl{VCS} and can be
found in real-world repositories.

Releases are identified by an intrinsic hash of a manifest containing their
name, their metadata, and the hash of the target artifact they reference.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=snapshot,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Snapshots}} are point-in-time captures of the \emph{full
state} of a project development repository. Unlike revisions, which capture the
state of a single development branch, snapshots capture the state of \emph{all}
the branches and releases in a repository. These artifacts are not typically
part of \glspl{VCS}, because they generally have no need to retain the
information of the past states of the full repository, as the important
historical information for developers is already tracked by revisions. In the
case of a software archive however, we need to track the successive states of
the repositories for each crawling \emph{visit}, which includes retaining old
information about which branches and tags were formerly present in the
repository, and what they pointed to.

More specifically, each snapshot contains a list of references to other
artifacts, which have an associated binary name (e.g., ``refs/heads/main'' or
``refs/tags/v0.1.2'') and the intrinsic hash of the artifact they reference.
Most references are either to revisions (in the case of branches) or release
objects, but on rare occasions they they can also point to directories and
contents, which is supported by some \glspl{VCS} and also sometimes present in
real-world repositories.

The data model also supports \emph{branch aliasing}: some branches stored in
snapshots do not point to a specific artifact, but rather reference another
branch name in the same snapshot. These are to be treated as symbolic links to
the target of the branch they reference.

Snapshots are also deduplicated and identified by an intrinsic hash, computed
from a manifest of each branch name and the intrinsic identifier of their
target. Therefore they also benefit from the advantageous space-complexity of
purely functional persistent structures as described in
Section~\ref{sec:purely-functional}: if two consecutive visits of the same
repository show that nothing changed in the interval, the visits can both share
the same deduplicated snapshot in the data model, instead of having to copy and
store the set of branches and tags twice.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=origin,scale=1.9] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Origins}} are objects referencing the specific places from
which source code artifacts have been retrieved to be ingested into the
archive.  They are represented by a canonical URL (e.g., the address at which
one can \texttt{git clone} a repository or \texttt{wget} a source code
tarball.)

Origins only represent specific locations which host code, and are distinct
from the more abstract notion of \emph{projects}: a project is an entity that
can relate together different development resources, including websites, issue
trackers, mailing lists, and software origins. A software project can migrate
its development from one origin to another for various reasons, and even
migrate to a different \gls{VCS}. Projects are ontologically complex notions
that are not directly present in the archive; the data model is mostly
concerned about directly addressable and concrete artifacts, and \emph{origins}
are better suited for this purpose.

\section{Consolidating software artifacts in a unified archive}

So far, we have covered the different abstract artifacts stored in \glspl{VCS}
in a way that allows us to represent an entire repository using these building
blocks, notably by deduplicating them using their intrinsic identifiers.
In fact, it is possible to go even further, and deduplicate these artifacts
across the \emph{entire archive}.

A key point of consideration is that software products are generally built by
reusing components from other projects, rather than being mostly isolated and
independent pieces of work. This organic code reuse happens through different
means, either by simply copying source code files or modules between different
projects, or by ``forking'' an existing project (i.e., starting an independent
development on an existing project by building upon its development history).
Modern \glspl{VCS} also allow referencing external software projects to be
fetched as dependencies (e.g., Git submodules or Mercurial subrepositories),
which further entangles the relationships between different software projects.

\glspl{VCS} and the abstract software artifacts used by Software Heritage
already allow us to canonicalize and deduplicate objects inside a single
software project. This principle can be generalized to deduplicate the
artifacts found in several projects \emph{across the entire archive}.
This process of systematically gathering and storing all publicly available
software artifacts in a deduplicated and canonical fashion is inherently a way
to materialize an immense highly interconnected graph, linking together all
derivative works and shared codebases.

This process is illustrated in~\figref{fig:consolidating-archive}. The
intrinsic cryptographic hashes of the software artifacts are used to enforce
their full deduplication, consolidating them in a unified Merkle \gls{DAG} with
all the artifacts in the archive.

\begin{figure}
    \centering
    \input{../tikz/figures/consolidating-archive.tikz}
    \caption{Consolidation of two different repositories in the archive by
    deduplicating and sharing all their common artifacts}%
    \label{fig:consolidating-archive}
\end{figure}

In short, this means that whenever a directory, file or any artifact is
referenced by multiple projects, it will get deduplicated as a single node in
the graph, and will be directly referenced by all projects and artifacts
that contain it. As an example, there is only one artifact for the empty
content (a blob of length zero) in the entire archive, and millions of
directories reference it.

Merging all archived repositories into a single large collection of software
artifacts leverages all of the benefits of Merkle \gls{DAG} models for single
repositories to the entire archive: simple identification of unique artifacts,
built-in data integrity checks, very high rate of deduplication and reduced
storage costs.

In addition, storing canonical artifacts only once in the archive is also
semantically useful for research. Essentially, it becomes easy to visualize
and analyze code reuse: walking back the \gls{DAG} can generate a list of all
projects that contain a specific piece of code. By looking at the commit
chains in the graph, it is possible to discover when any given file was copied,
and all changes that were subsequently made to it.
We can easily draw research applications for software evolution from this
capability: by having direct access to the history of changes of a specific
fragment of code in the entire corpus of public software commons, it could be
possible to make automatic linting suggestions based on likelihood that a given
piece of code will be modified in the future. Overall, materializing these
relationships between canonical software artifacts has the potential to widen
the frontier of software mining by providing access to a unique corpus of
provenance data and evolution history of the artifacts.


\section{The Software Heritage Merkle DAG}

\subsection{Overview}

After having walked through the key principles that underpin the design of the
Software Heritage archive, this section describes its data model in more
detail.

The archive is stored as a single large Merkle \gls{DAG}, as depicted
in~\figref{fig:swh-model}. In this graph, all software artifacts we have
described correspond to the nodes, which can be of six different kinds:
contents, directories, revisions, releases, snapshots and origins.
All the nodes are deduplicated and identified by a single intrinsic
identifier, recursively computed from the identifiers of its descendants, which
serves as the primary key in the Merkle \gls{DAG}.

The edges between the nodes emerge naturally: directory entries point to other
directories or file contents; revisions point to directories and previous
revisions; releases point to revisions; snapshots point to revisions and
releases; origins point to the snapshots that they contained in past visits.
The cardinality diagram shown in~\figref{fig:swh-cardinality} formalizes the
different kinds of edges in the graph and their numerical relationships: edges
noted $\prescript{\ast}{}\rightarrow^1$ represent \emph{many-to-one}
relationships (e.g., a revision only points to a single directory, but a
directory can be pointed by multiple revisions) and edges noted
$\prescript{\ast}{}\rightarrow^\ast$ are \emph{many-to-many} relationships
(e.g., a directory can point to multiple contents, and a content can be pointed
by multiple directories).

\begin{figure}
    \centering
    \input{../tikz/figures/swh-model.tikz}
    \caption{The Software Heritage Archive: a unified Merkle \gls{DAG} of all
    the software artifacts in public development.}%
    \label{fig:swh-model}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{../img/swh-cardinality}
    \caption{Cardinality diagram of the possible relationships between the node
    types in the archive, highlighting the difference between
    \emph{many-to-one} and \emph{many-to-many} edge types.}%
    \label{fig:swh-cardinality}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../img/swh-merkle-dag}
    \caption{Detailed view of the Merkle \gls{DAG}, with object contents,
    metadata fields and intrinsic hashes.}%
    \label{fig:swh-cardinality}
\end{figure}

\subsection{Persistent identifiers}

The intrinsic hashes used to identify the nodes can be used to provide a
standardized and portable way of referring to the artifacts in the archive. This
is achieved by \glspl{SWHID}, short strings that identify a single object
and which are guaranteed to remain stable over time.
\glspl{SWHID} are represented following this basic syntax:

\texttt{swh:<scheme version>:<object type>:<object id>}

The current version of the identifier scheme, stored in the second field, is
\texttt{1}. The third field contains a three-letter code for the object type
(\texttt{snp} for snapshots, \texttt{rel} for releases, \texttt{rev} for
revisions, \texttt{dir} for directories and \texttt{cnt} for contents). The
last field contains an hexadecimal-encoded intrinsic hash computed with the
SHA-1 algorithm on the content and metadata of the object.

Here are a few examples of \glspl{SWHID} and the objects they point to:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \texttt{swh:1:cnt:94a9ed024d3859793618152ea559a168bbcbb5e2} points to
        the content of a file containing the full text of the GPL-3 license.
    \item \texttt{swh:1:rev:309cf2674ee7a0749978cf8265ab91a60aea0f7d} points to
        a revision in the development history of the Darktable photography
        application.
    \item \texttt{swh:1:snp:c7c108084bc0bf3d81436bf980b46e98bd338453} points to
        a snapshot of the entire Git repository of Darktable as of May 4, 2017.
\end{itemize}

\glspl{SWHID} have interesting uses for research replicability: studies
citing software by URL can be subject to \emph{link rot}, which could make it
harder to run the software again as a way to independently verify the study
results. Because \glspl{SWHID} are persistent, using these identifiers to cite
scientific software is a way to ensure that the citation remains unambiguous
and resilient to URL changes. \glspl{SWHID} can be computed independently and
do not require a centralized authority for assignation, as they are solely
computed from the intrinsic properties of the object they refer to.

\section{Infrastructure}

At a physical level, the archive is stored using a few different technologies,
due to the differences in technical requirements for storing the different
layers of the graph~\cite{swhipres2017}.

Most notably, the source contents require orders of magnitude more space
($\approx 300$ TiB as of May 2020) than the rest of the archive and are thus
stored on dedicated key-value object storages, keyed by their intrinsic hash.
In the in-house infrastructure, the object hashes allow for efficient
horizontal sharding, by assigning hash prefixes to specific servers. This
technique is used to trivially implement a reasonably performant and redundant
storage.
Key-value object storages are an almost universal primitive in cloud offerings,
which allows us to store an entire copy of the content layers in two different
clouds: the Azure Blob Storage and Amazon AWS S3.

On the other hand, the upper layers of the archive require significantly less
storage space ($\approx 6$ TiB) but have different functional requirements, as
they have more associated metadata (author names, revision messages, children
nodes, …) that need to be made searchable and joinable. This part of the graph
is stored in a \gls{RDBMS}, where nodes and edges have a few associated
relational tables that describe it. This allows to easily build indexes, e.g.,
on the hashes stored in the different fields, which can then be used to
randomly access specific nodes and edges, and to efficiently \emph{join} tables
together to combine data from multiple node types by following their links.

This approach can have scalability issues, as inserting data in traditional
\glspl{RDBMS} gets exponentially slower as the tables grow.  Other storage
systems for the graph are under study to circumvent this problem, notably the
use of key-value document storages like Cassandra or ScyllaDB.

One last relevant component is the \emph{journal}, which serves as a
synchronization pipeline across the infrastructure. In order to keep the
variety of storage backends (object storage, database, document stores…)
consistent together, the journal acts as a persistent log of the archive, where
all the ingested objects are pushed in order. All the backends and their
replicas can then subscribe to the journal and read its messages to keep
themselves up to date with the current state of the archive.
