\chapter{The Software Heritage Data Model}

% This chapter describes the abstract data model of the Software Heritage
% archive.

\section{Canonical Software Artifacts}

The Software Heritage archive is continuously ingesting software artifacts from
a wide array of sources, including different VCS and package managers that
each have their own internal data model. The main purpose of the Software
Heritage data model is to provide a generic structure in which the individual
object types specific to each system can be mapped to abstract concepts.  Git
``commits'', SVN ``revisions'' and Mercurial ``changesets'' all correspond to
the same idea of a frozen state of the source tree, and thus they are all
\emph{canonicalized} as a single type of artifact that we call ``revisions''.

By stripping the implementation peculiarities of the individual data sources,
the artifacts are boiled down to a purely abstract form. This unifies the
representation of all the artifacts stored in the archive, which is
particularly interesting for research: since all the artifacts are already
stored in canonical form, we can provide a uniform interface for researchers
to study artifacts coming from a variety of different sources. This isolates
the complexity of handling artifacts sourced from different \glspl{VCS} behind
an abstraction layer; researchers can then run analyses on the abstract
artifacts instead of having to deal with each specific system.

The following kinds of canonical software artifacts are supported in the data
model:

\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node[style=content,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Contents}} (or ``blobs'') are the raw content of the source
code files. They represent only the concrete binary data contained in the
files; file \emph{names} are external and directory-dependent, and are not
included in this type of artifact. Contents are identified by an intrinsic hash
of the full binary data they contain.

\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=directory,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Directories}} are the source code trees. They contain a
list of \emph{named} directory entries, each entry pointing to content objects
(``file entries''), other directories (``directory entries''), or revisions
(``revision entries''). Each entry is associated with a local binary name
(i.e., a relative path without any path separator) and permission metadata.
Revision entries are used to encode directories referencing a specific
revision, for instance Git submodules and Mercurial subrepositories.  The
permission integer can encode arbitrary permissions such as whether the file is
an executable, a symbolic link, who can read or write in it, etc.  They are
identified by an intrinsic hash of the textual manifest of all their entries
and their own identifiers.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=revision,scale=1.3] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Revisions}} (or ``commits'') are point-in-time captures of
the state of the entire source code tree of a project. Each revision points to
the ``root'' directory of the project source tree. In addition, revisions are
associated with the following commit metadata:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \emph{commit message}: a descriptive message generally detailing the
        changes made since the previous revision.
    \item \emph{author}: the name and e-mail of the person who authored the
        revision.
    \item \emph{committer}: the name and e-mail of the person who applied the
        revision to the project on behalf of the original author. Often this
        field contains the same information as the author field.
    \item \emph{date}: the date at which the revision was authored, including
        timezone information.
    \item \emph{committer date}: the date at which the revision was applied to
        the project, including timezone information. Often the same as the date
        field.
\end{itemize}

Finally, each revision points to an ordered list of all its parent revisions,
referenced using their own intrinsic identifier. The order does not have a
strict semantic meaning, it is mostly used as a guide by tools that show the
difference between two revisions. The first parent is generally considered to
be issued from the ``main'' branch that the revision is merged onto, and thus
diffing tools will show the impact of this revision on the main branch by
default.

Revisions are identified by an intrinsic hash of a textual manifest containing
all their metadata, the hash of their parents, and the hash of the root
directory of the source tree they reference.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=release,scale=1.3] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Releases}} (or ``tags'') denote marker objects that label
specific revisions as project milestones. These usually denote the revisions
where the software is distributed to its user base, as well as the various
steps of its release cycle. These releases are marked with a specific and
usually mnemonic short name (e.g., a version number). Aside from this name and
a reference to their target revision, releases additionally contain some
metadata:

\begin{itemize}
    \setlength\itemsep{0em}
    \item \emph{message}: an annotation generally describing the release, e.g.,
        by including its full changelog.
    \item \emph{author}: the name and e-mail of the person who authored the
        release.
    \item \emph{date}: the date at which the release was created, including
        timezone information.
\end{itemize}

The data model also supports revisions pointing to other kinds of artifacts
(contents and directories), which is supported by some \glspl{VCS} and can be
found in real-world repositories.

Releases are identified by an intrinsic hash of a manifest containing their
name, their metadata, and the hash of the target artifact they reference.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=snapshot,scale=1.5] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Snapshots}} are point-in-time captures of the \emph{full
state} of a project development repository. Unlike revisions, which capture the
state of a single development branch, snapshots capture the state of \emph{all}
the branches and releases in a repository. These artifacts are not typically
part of \glspl{VCS}, because they generally have no need to retain the
information of the past states of the full repository, as the important
historical information for developers is already tracked by revisions. In the
case of a software archive however, we need to track the successive state of
the repositories for each crawling \emph{visit}, which includes retaining old
information about which branches and tags were formerly present in the
repository, and what they pointed to.

More specifically, each snapshot contains a list of references to other
artifacts, which have an associated binary name (e.g., ``refs/heads/main'' or
``refs/tags/v0.1.2'') and the intrinsic hash of the artifact they reference.
Most references are either to revisions (in the case of branches) or release
objects, but on rare occasions they they can also point to directories and
contents, which is supported by some \glspl{VCS} and also sometimes present in
real-world repositories.

The data model also supports \emph{branch aliasing}: some branches stored in
snapshots do not point to a specific artifact, but rather reference another
branch name in the same snapshot. These are to be treated as symbolic links to
the target of the branch they reference.

Snapshots are also deduplicated and identified by an intrinsic hash, computed
from a manifest of each branch name and the intrinsic identifier of their
target. Therefore they also benefit from the advantageous space-complexity of
purely functional persistent structures as described in
Section~\ref{sec:purely-functional}: if two consecutive visits of the same
repository show that nothing changed in the interval, the visits can both share
the same deduplicated snapshot in the data model, instead of having to copy and
store the set of branches and tags twice.


\begin{wrapfigure}{r}{0.07\textwidth}\centering
\begin{tikzpicture}\node [style=origin,scale=1.9] (0) at (0, 0) {};\end{tikzpicture}
\end{wrapfigure}
\paragraph{\textbf{Origins}} are objects referencing the specific places from
which source code artifacts have been retrieved to be ingested into the
archive.  They are represented by a canonical URL (e.g., the address at which
one can \texttt{git clone} a repository or \texttt{wget} a source code
tarball.)

Origins only represent specific locations which host code, and are distinct
from the more abstract notion of \emph{projects}: a project is an entity that
can relate together different development resources, including websites, issue
trackers, mailing lists, and software origins. A software project can migrate
its development from one origin to another for various reasons, and even
migrate to a different \gls{VCS}. Projects are ontologically complex notions
that are not directly present in the archive; the data model is mostly
concerned about directly addressable and concrete artifacts, and \emph{origins}
are better suited for this purpose.

\section{Consolidating software artifacts in a unified archive}

So far, we have covered the different abstract artifacts stored in \glspl{VCS}
in a way that allows us to represent an entire repository using these building
blocks, notably by deduplicating them using their intrinsic identifiers.
In fact, it is possible to go even further, and deduplicate these artifacts
across the \emph{entire archive}.

A key point of consideration is that software products are generally built by
reusing components from other projects, rather than being mostly isolated and
independent pieces of work. This organic code reuse happens through different
means, either by simply copying source code files or modules between different
projects, or by ``forking'' an existing project (i.e., starting an independent
development on an existing project by building upon its development history).
Modern \glspl{VCS} also allow referencing external software projects to be
fetched as dependencies (e.g., Git submodules or Mercurial subrepositories),
which further entangles the relationships between different software projects.

\glspl{VCS} and the abstract software artifacts used by Software Heritage
already allow us to canonicalize and deduplicate objects inside a single
software project. This principle can be generalized to deduplicate the
artifacts found in several projects \emph{across the entire archive}.
This process of systematically gathering and storing all publicly available
software artifacts in a deduplicated and canonical fashion is inherently a way
to materialize an immense highly interconnected graph, linking together all
derivative works and shared codebases.

This process is illustrated in~\figref{fig:consolidating-archive}. The
intrinsic cryptographic hashes of the software artifacts are used to enforce
their full deduplication, consolidating them in a unified Merkle \gls{DAG} with
all the artifacts in the archive.

\begin{figure}
    \centering
    \input{../tikz/figures/consolidating-archive.tikz}
    \caption{Consolidation of two different repositories in the archive by
    deduplicating and sharing all their common artifacts}%
    \label{fig:consolidating-archive}
\end{figure}

In short, this means that whenever a directory, file or any artifact is
referenced by multiple projects, it will get deduplicated as a single node in
the graph, and will be directly referenced by all projects and artifacts
that contain it. As an example, there is only one artifact for the empty
content (a blob of length zero) in the entire archive, and millions of
directories reference it.

Merging all archived repositories into a single large collection of software
artifacts leverages all of the benefits of Merkle \gls{DAG} models for single
repositories to the entire archive: simple identification of unique artifacts,
built-in data integrity checks, very high rate of deduplication and reduced
storage costs.

In addition, storing canonical artifacts only once in the archive is also
semantically useful for research. Essentially, it becomes easy to visualize
and analyze code reuse: walking back the \gls{DAG} can generate a list of all
the projects that contain a specific piece of code. By looking at the commit
chains in the graph, it is possible to discover when any given file was copied,
and all changes that were subsequently made to it.
We can easily draw research applications for software evolution from this
capability: by having direct access to the history of changes of a specific
fragment of code in the entire corpus of public software commons, it could be
possible to make automatic linting suggestions based on likelihood that a given
piece of code will be modified in the future. Overall, materializing these
relationships between canonical software artifacts has the potential to widen
the frontier of software mining by providing access to a unique corpus of
provenance data and evolution history of the artifacts.
