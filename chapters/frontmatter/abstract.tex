\section*{Abstract}
\begin{SingleSpace}
    The Software Heritage project is a software archive containing the largest
    public collection of source code files along with their development history,
    in the form of an immense graph of hundreds of billions of edges. In this
    thesis, we present architectural techniques to make this graph available
    for research. We first propose some utilities to access the data at a
    micro-level in a way that is convenient for smaller-scale research.
    To run analyses on the entire archive, we extract a property graph in a
    relational format and evaluate the different ways this data can be
    exploited using in-house and cloud processing services.
    We find that while this approach is well suited to process large amounts of
    flat data in parallel, it has inherent limitations for the highly recursive
    graph structure of the archive. We propose the use of graph compression as
    a way to considerably reduce the memory usage of the graph, allowing us to
    load it entirely in physical memory. We develop a library to run arbitrary
    algorithms on the compressed graph of public development, using various
    mapping techniques to access properties at the node and edge levels.
    Then, we leverage this infrastructure to study the topology of the entire
    graph, looking at both its local properties and the way software projects
    are organized in forks. The in-depth understanding of this structure then
    allows us to discuss different approaches for distributed graph analysis.

\medskip

\textbf{Keywords:} empirical software engineering, source code, open source
software, version control system, digital preservation, graph topology, graph
compression

\end{SingleSpace}


\begin{otherlanguage}{french}
\section*{Résumé}
\begin{SingleSpace}

Software Heritage est une archive de logiciels contenant la plus
grande collection publique de fichiers de code source ainsi que l'historique de
leur développement, sous la forme d'un immense graphe de centaines de milliards
d'arêtes. Dans cette thèse, nous présentons des techniques architecturales pour
rendre ce graphe disponible à des fins de recherche. Nous proposons d'abord
quelques utilitaires pour accéder aux données à un niveau local d'une manière
adaptée à la recherche à petite échelle. Pour effectuer des analyses sur
l'ensemble de l'archive, nous extrayons un graphe de propriétés dans un format
relationnel et évaluons différents systèmes de traitement pour exploiter ces
données. Cette approche est adaptée au traitement de grandes quantités de
données horizontales, mais elle présente des limites inhérentes à la structure
fortement récursive du graphe. Nous proposons d'utiliser la compression de
graphe comme moyen de réduire considérablement la taille du graphe, ce qui nous
permet de le charger en mémoire vive. Nous développons une bibliothèque pour
exécuter des algorithmes arbitraires sur le graphe compressé, en utilisant
des techniques de mise en correspondance de ses propriétés au
niveau des nœuds et des arêtes. Nous utilisons ensuite cette infrastructure
pour étudier la topologie locale du graphe ainsi que son organisation en forks
de projets logiciels. Comprendre cette structure nous permet ensuite de
discuter de différentes approches pour l'analyse distribuée.

\medskip

\textbf{Mots-clés~:} génie logiciel empirique, code source, logiciel
open-source, système de gestion de version, conservation numérique, topologie
de graphe, compression de graphe

\end{SingleSpace}
\end{otherlanguage}

\clearpage

\begin{otherlanguage}{french}
\chapter*{Résumé détaillé}

Software Heritage est une initiative ambitieuse de préservation numérique qui
amasse des quantités sans précédent de code source de logiciels d'origines
diverses, en gardant la trace de toutes leurs évolutions telles que capturées
par les systèmes de contrôle de version (VCS). L'archive Software Heritage est
devenue le corpus le plus important et le plus complet d'artefacts logiciels
publics, englobant les principaux hébergeurs de code source (par exemple,
GitHub, GitLab, Bitbucket, Google Code), et supportant une variété de VCS (par
exemple, Git, Mercurial, SVN) et de gestionnaires de paquets (Debian, NixOS,
PyPI, NPM, \ldots).

Ces données sont stockées dans un format canonique et dédupliquées à travers
toute l'archive au niveau des artefacts logiciels, ce qui construit un immense
graphe partagé du développement logiciel, où les nœuds sont les fichiers
sources, les répertoires et les commits, et les arêtes contiennent les noms des
fichiers et les liens entre les commits et les répertoires.

La mise à disposition de cette base de connaissances universelle sur le
développement de logiciels offre des opportunités uniques pour ce que l'on
appelle communément la recherche sur le « Big Code » : interroger, analyser et
extraire des connaissances à partir du contenu des données et de la structure
du graphe de l'historique du développement logiciel. Rendre ces données
accessibles aux chercheurs pourrait débloquer de nouvelles capacités dans le
domaine de la fouille de logiciels. La possibilité d'effectuer des expériences
sur l'ensemble du graphe de développement des logiciels publics pourrait nous
aider à mieux comprendre l'évolution des logiciels au niveau macro et les
structures sociales sous-jacentes des projets logiciels. Cela réduirait
également les barrières à l'entrée des études empiriques en diminuant les coûts
de la collecte de données et en éliminant le besoin de récupérer et extraire
manuellement les données, et faciliterait la reproduction des études à grande
échelle en utilisant des échantillons de données complets.

L'exploitation d'une telle collection de codes sources sans précédent pose des
défis importants en termes de disponibilité et de représentation des données,
d'architecture système et de passage à l'échelle. Le graphe de l'archive
Software Heritage contient plus de 20 milliards de nœuds et 200 milliards
d'arêtes et croît de manière exponentielle au fil du temps. À cette échelle, il
est généralement difficile d'utiliser des outils standard pour l'analyse des
données, et de nouvelles techniques doivent être élaborées sur la base de
l'état de l'art du traitement des grands graphes.

\subsection*{Mise à disposition des artefacts logiciels à différentes échelles}

Pour mieux comprendre les types de données qui doivent être mises à
disposition, nous classons systématiquement les exigences fonctionnelles des
chercheurs dans les études d'extraction de logiciels. Nous classons les données
disponibles dans l'archive Software Heritage, y compris le contenu des fichiers
de code source, l'historique de développement et d'autres types de métadonnées,
mais aussi les données qui peuvent être dérivées du graphe : les différences de
commit, les graphes de dépendance, les graphes de communauté, etc.
Pour cela, nous faisons une revue de littérature de 54 articles scientifiques
sur la fouille de logiciels, et agrégeons leurs besoins en analysant les types
de données utilisées par chaques étude.

Notre première étape pour mettre à disposition une plateforme généraliste
d'analyse de données consiste à rendre les données disponibles dans un format
rudimentaire mais exploitable. Cela peut susciter un intérêt pour l'utilisation
de l'archive à des fins de recherche et nous permettre de mieux comprendre les
défis liés à son exploitation, en identifiant les principaux points sensibles
des chercheurs et les limites des différents formats que nous mettons à leur
disposition.

Nous fournissons quelques utilitaires pour récupérer les données à un niveau
micro : le Vault, un outil simple pour assembler la fermeture transitive d'un
artefact logiciel spécifique dans le graphe et le représenter dans un format
ouvert, et SwhFS, un système de fichiers virtuel pour exécuter des programmes
d'analyse ordinaires sur le code stocké dans l'archive sans avoir à le
télécharger au préalable.

Pour permettre des macro-analyses sur l'ensemble du graphe, nous extrayons de
l'archive un graphe de propriétés contenant toutes les métadonnées de
développement du logiciel au niveau du système de fichiers, de son historique
de développement et de l'endroit où il est hébergé. Nous représentons ce graphe
dans deux formats relationnels différents : un format en colonnes pour un
traitement à grande échelle sur des plateformes de cloud, et un format adapté à
l'importation dans des bases de données locales pour une analyse en interne.
Nous discutons de diverses considérations liées à ce jeu de données : les
différents formats, les performances, la dénormalisation des données et la
confidentialité des données. La mise en place de ces jeux de données sur des
plateformes cloud nous permet de comprendre les cas d'utilisation pour lesquels
ce format relationnel est adapté, ainsi que ses limites, notamment la
difficulté d'exécuter des expériences qui nécessitent des algorithmes de
parcours de graphe (par exemple, descendre une chaîne de commits).

\subsection*{Exploitation du graphe de développement logiciel en tant que
structure récursive}

Étant donné que le jeu de données contenant le graphe est une structure
intrinsèquement récursive, les outils d'analyse des données volumineuses qui
exploitent l'horizontalité des données pour mettre à l'échelle leur traitement
ne peuvent pas être utilisés de manière similaire sur des problèmes qui n'ont
pas une structure avec un parallélisme embarrassant. Une approche naïve
consistant à utiliser des hachages d'objets pour affecter aléatoirement les
artefacts à différentes shards n'est pas très efficace, car un algorithme de
parcours nécessitera de passer en permanence d'une shard à l'autre. Nous
examinons les solutions de passage à l'échelle existantes (GraphFrames, Neo4j,
AIGraph) et discutons de leurs limites pour l'analyse de notre graphe.

Une autre approche est celle de la compression de graphes : en utilisant des
techniques existantes pour la compression de très grands graphes, nous pouvons
faire tenir le graphe Software Heritage sans ses propriétés en mémoire sur une
seule machine. Cela nous permet d'exécuter des algorithmes de graphes standard
sans avoir à trouver un moyen de les paralléliser. Le graphe compressé peut
être utilisé à des fins de prototypage, mais aussi comme un service en
production qui peut répondre à des requêtes de parcours simples. Nous discutons
des différentes techniques de compression utilisées pour faire tenir la
topologie entière du graphe dans seulement $\sim150$ GiB de RAM, ainsi que de
certains arbitrages de stockage pour les métadonnées des nœuds et des bords du
graphe de propriétés.

Pour permettre aux chercheurs d'effectuer des expériences à plus petite
échelle, nous présentons un moyen d'extraire des sous-ensembles de données de
graphes représentatifs de n'importe quelle taille donnée à partir de l'archive.
Nous discutons de diverses implémentations des moyens d'obtenir une « vue »
d'une partie donnée du graphe, et nous mettons à disposition quelques
sous-ensembles de données « accrocheurs » en utilisant ces techniques.

\subsection*{Structure en graphe du développement logiciel}

La compréhension de la structure du graphe de développement est une étape
fondamentale vers l'analyse à grande échelle, car elle peut nous aider à
déterminer comment partitionner les données en groupes étroitement liés. De
plus, une meilleure compréhension de la topologie de ses différentes couches
ainsi que des différents cas limites et aberrants que l'on y trouve nous permet
de mieux organiser les données pour certains algorithmes spécifiques.

Dans un premier temps, nous examinons les propriétés topologiques de bas niveau
du graphe, en mesurant systématiquement quelques paramètres clés sur ses
différentes couches : distributions des degrés entrants et degrés sortants,
taille des composantes connexes, coefficients de clustering, longueur des plus
cours chemins. Nous comparons ces métriques avec d'autres graphes à grande
échelle générés par des activités humaines (réseaux sociaux, graphe du Web,
\ldots).

Nous observons une grande disparité topologique entre les différentes couches
qui constitutent le graphe, tant au niveau local que dans leur structure
globale. En décomposant le graphe en trois couches en fonction des types des
nœuds, nous constatons qu'elles ont des formes, des densités et des
connectivités radicalement différentes. En particulier, la couche du système de
fichiers domine largement les propriétés topologiques du graphe étant donné
qu'elle contient 90\% de ses nœuds et 97\% de ses arêtes. Les deux couches
principales du graphe ont des propriétés diamétralement opposées~: la couche du
système de fichiers a une forte connexité, une profondeur moyenne
caractéristique et un degré sortant moyen divergeant, tandis que la couche de
l'historique de développement a une faible connexité, une profondeur moyenne
divergeante et un degré sortant moyen caractéristique.

Une autre observation importante est que, étant donné que les nœuds de la
couche du système de fichier s'agrègent dans une composante géante, le graphe
ne peut pas être partitionné facilement grâce à une séparation en composantes
connexes strictes. Cependant, la plus grosse composante connexe de la couche de
l'historique de développement ne contient que 3\% des nœuds de cette dernière,
ce qui rend possible leur séparation en multiples partitions localement
connectées qui peuvent être traitées en parallèle de manière efficace.

Au niveau macro, nous cherchons à comprendre comment les artefacts logiciels
sont organisés et partagés entre différents dépôts en étudiant les « forks »
dans l'archive, en comparant les différentes définitions de ce qu'est un fork.
En exploitant le fait que les projets qui sont développés à partir d'autres
projets partagent une partie de leur histoire de développement, nous proposons
un algorithme pour organiser la liste de tous les dépôts dans l'archive en «
cliques de forks » (groupes de projets qui sont tous des forks les uns des
autres) et en « réseaux de forks » (groupes de projets qui sont transitivement
liés entre eux par des relations de forks). Nous calculons ces structures pour
l'ensemble du graphe de Software Heritage et mesurons leurs distributions de
taille.

\subsection*{Conclusion}

Enfin, nous résumons les principales contributions académiques de cette thèse,
et ses implications pour la recherche empirique en génie logiciel. Nous
discutons des futures directions de recherche et des sujets ouverts : le
support des requêtes arbitraires sur le graphe par le biais d'un langage de
requête expressif, la mise à disposition d'outils pour faciliter les analyses à
grande échelle, ainsi que la réduction du délai entre les jeux de données
publiques et les données de l'archive par le biais d'exportations incrémentales
du graphe.

\end{otherlanguage}


\chapter*{Extended abstract}

Software Heritage is an ambitious digital preservation initiative that is
amassing unprecedented quantities of software source code from a variety of
origins, keeping track of all their evolution histories as captured by version
control systems (VCS). The Software Heritage archive has grown to be the
largest and most comprehensive corpus of public software artifacts,
encompassing major source code hosts (e.g., GitHub, GitLab, Bitbucket,
Google Code), supporting a variety of VCS (e.g., Git, Mercurial, SVN) and
package managers (Debian, NixOS, PyPI, NPM, …).

This data is stored in a canonical format and deduplicated across the entire
archive at the level of software artifacts, which constructs an immense shared
graph of software development, where the nodes are the source files,
directories and commits, and the edges carry the names of the files and the
links between commits and directories.

The availability of this universal knowledge base on software development
provides unique opportunities for what is now known as “Big Code” research:
querying, analyzing and extracting knowledge both from the contents of the data
and from the structure of the development history graph.
Making this data accessible to researchers could unlock new capabilities in the
software mining field. Being able to run experiments on the entire graph of
public software development could help us gain a deeper understanding of the
evolution of software at a macro level, and the underlying social structures of
software projects. It would also reduce the barriers of entry to empirical
studies by lowering the costs of data collection and eliminating the need for
manual data scraping and retrieval, as well as facilitate replicating studies
at a large scale using comprehensive data samples.

The exploitation of such an unprecedented source code collection poses
significant challenges in terms of availability, data representation, system
architecture and scalability. The graph in the Software Heritage archive
contains more than 20 billion nodes and 200 billion edges and grows
exponentially over time. At this scale, it is generally difficult to use
off-the-shelf tools for general purpose data analysis, and new techniques must
be built on top of the state of the art of large graph processing.

\subsection*{Making software artifacts available at different scales}

To better understand the kinds of data that need to be made available, we
systematically categorize the functional requirements of researchers in
software mining studies.  We classify the data available in the Software
Heritage archive, including the content of the source code files, the
development history and other kinds of metadata, but also data that can be
derived from the graph: commit diffs, dependency graphs, community graphs, etc.

Our first step towards providing a general purpose platform for data analysis
is to make the data available in a crude but still exploitable format. As this
can gather interest towards using the archive for research purposes, it
allows us to better understand the challenges of exploiting it, by identifying
the main pain points of researchers and limitations of the different formats.

We provide a few utilities to fetch the data at a micro-level: the vault, a
simple tool to gather the transitive closure of a specific software artifact in
the graph and then representing it in an open format, and SwhFS, a virtual
filesystem to run common programs on the code stored in the archive without
having to pre-download it.

To make possible macro analyses on the entire graph, we extract from the
archive a property graph containing all the software development metadata at
the filesystem, history and hosting levels. We represent it in two different
relational formats: a columnar format for scalable cloud processing, and a
format suitable for import in local databases for in-house analysis. We discuss
various considerations related to the dataset: the different formats,
performance, data denormalization and data privacy.
Putting these datasets on cloud platforms allows us to understand the use cases
for which this relational format is suitable, as well as its limitations,
notably the difficulty of running experiments that require graph traversal
algorithms (e.g., walking down a commit chain).

\subsection*{Exploiting software development data as a recursive graph}

As the graph dataset is an inherently recursive structure, big data analysis
tools that exploit the ``flatness'' of data to scale-out their processing cannot
be used in a similar fashion on problems that are not embarrassingly parallel.
A naive scale-out approach of using object hashes to randomly assign the
artifacts to different shards is not very efficient, as a traversal algorithm
will require jumping between different shards all the time. We investigate
existing scale-out solutions (GraphFrames, Neo4j, AIGraph) and discuss their
limitations for scale-out graph analysis.

Another approach is that of graph compression: using existing techniques for
compressing very large graphs, we can fit the Software Heritage graph without
its properties in memory on a single machine. This allows us to run standard
graph algorithms without having to find a way to parallelize them. The
compressed graph can be used for prototyping purposes, but also as a production
service that can answer simple traversal queries. We discuss the different
compression techniques used to fit the entire graph topology in only
$\sim150$\,GiB of RAM, as well as some storage trade-offs for the node and
edge metadata on the property graph.

To let researchers run experiments on a smaller scale, we introduce a way to
extract representative graph subdatasets of any given size from the archive. We
discuss various implementations of ways to obtain a ``view'' of a given part of
the archive, and make some ``teaser'' datasets available using these techniques.

\subsection*{The graph structure of software development}

Understanding the structure of the development graph is a fundamental step
towards scale-out analysis, as it can help us determine how to partition the
data into tightly-knit clusters. Moreover, getting a better sense of the
topology of its different layers as well as the various edge cases and outliers
found in it will help us better organize the data for some domain-specific
algorithms.

As a first step, we look at the low-level topological properties of the graph,
by systematically measuring a few key metrics on its different layers:
in-degree and out-degree distributions, connected component sizes, clustering
coefficients, average path lengths. We compare these metrics with other
large-scale graphs generated by human activities (social networks, graph of the
Web, …).

On a macro-level, we seek to understand how the software artifacts are
organized and shared across different repositories by studying the ``forks'' in
the archive, comparing different definitions of what a fork is. By exploiting
the fact that projects that are built on top of others will share some of their
development history, we propose an algorithm to arrange the list of all the
repositories in the archive in ``fork cliques'' (clusters of projects that are
all forks of each other) and ``fork networks'' (clusters of projects that are
transitively linked together by forking relationships). We compute these
structures for the entire Software Heritage graph and measure their size
distributions.

\subsection*{Conclusion}

Finally, we summarize the main academic contributions of this thesis, and its
implications for empirical software engineering research. We discuss future
research directions and open subjects: supporting general-purpose graph queries
through an expressive query language, providing tooling to streamline scale-out
analyses, as well as reducing the lag between the public datasets and the live
data through incremental graph exports.
