\documentclass[aspectratio=169,xcolor=table]{beamer}
\input{header.tex}
\addbibresource{../thesis.bib}

\begin{document}
    \maketitle

    \begin{frame}{Outline}
        \tableofcontents
    \end{frame}

    \section{Introduction: Universal Mining in Software Heritage}
    % presentation
    % universal software mining

    \begin{frame}
        \frametitle{Sofware Mining}

        \begin{block}{Definition}
            \textbf{Software mining}: studying existing software repositories
            to help improve software development processes and practices.
            % all the byproducts of software development
            % improve the software of tomorrow
            % inform development best practices
        \end{block}

        \begin{block}{Applications}
            \begin{itemize}
                \item Software health, software evolution
                \item Automated bug detection
                \item Automated vulnerability repair
                \item Code autocompletion
                \item Clone detection
                \item License compliance
                \item …
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Universal Software Mining}

        \begin{block}{Current scale of software mining studies}
            \begin{itemize}
                \item Individual projects
                \item Up to thousands of popular repositories (e.g., ``top
                    1000 by stars'')
                \item Entire ecosystems (app stores, package managers, …)
            \end{itemize}
        \end{block}

        \begin{block}{Universal software mining}
            New possibilities offered by the rise of DVCS in the past 15 years.

            Next step: a framework to run empirical studies on
            \textbf{all the public software repositories}?

            \begin{itemize}
                \item Less repetitive, no need to crawl the data for each study
                \item Easier to replicate studies
                \item Reduce selection bias
                \item High-level view of social processes in software
                    development
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{In this thesis…}

        \begin{block}{}
            \Large
            I study how to organize the \textbf{graph of public software
            development}, a comprehensive dataset of software development data,
            to make it \textbf{accessible for software mining research}.
        \end{block}

        \begin{block}{}
            \emph{Research direction}: Working towards a research platform for
            Universal Software Analysis.

            \footnotesize
            \begin{thebibliography}{swhbenevol}
                \bibitem{swhbenevol2018} Antoine Pietri, Stefano Zacchiroli\newblock
                Towards Universal Software Evolution Analysis\newblock
                BENEVOL 2018\newblock
            \end{thebibliography}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{The Software Heritage Initiative}

        \begin{center}
            \includegraphics[width=.5\linewidth]{img/SWH-logo+motto.pdf}
        \end{center}

        \begin{block}{Collect, preserve and share \emph{all} software source
            code}
            \hfill Preserving our heritage, enabling better software and better
            science for all
            % \pause
        \end{block}

        \begin{columns}
            \begin{column}{.3\columnwidth}
                \begin{block}{Reference catalog}
                    \begin{center}
                        \includegraphics[width=.6\linewidth]{img/myriadsources}
                    \end{center}
                    \alert{find} and \alert{reference} all software source code
                    % \pause
                \end{block}
            \end{column}
            \begin{column}{.3\columnwidth}
                \begin{block}{Universal archive}
                    \begin{center}
                        \includegraphics[width=.6\linewidth]{img/fragilecloud}
                    \end{center}
                    \alert{preserve} all software source code
                    % \pause
                \end{block}
            \end{column}
            \begin{column}{.3\columnwidth}
                \begin{block}{Research infrastructure}
                    \begin{center}
                        \includegraphics[width=.7\linewidth]{img/atacama-telescope}
                    \end{center}
                    \alert{enable analysis} of all software source code
                \end{block}
            \end{column}
        \end{columns}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Data flow}
    %     \begin{center}
    %         \includegraphics[width=0.9\textwidth]{img/swh-dataflow.pdf}
    %     \end{center}
    % \end{frame}

    \begin{frame}
        \frametitle{Archive coverage --- archive.softwareheritage.org}
        % TODO: update numbers?
        % \vspace{-1mm}
        \begin{center}
            \includegraphics[trim=0 2cm 0 0, clip, width=0.7\linewidth]{img/archive-growth.png}
        \end{center}
        % \vspace{-2mm}
        \begin{center}
            \colorbox{white}{\includegraphics[width=0.8\linewidth]{img/archive-coverage.png}}
        \end{center}
        % \pause
        \vspace{-2mm}
        \begin{block}{}
            \begin{itemize}
                \item On disk: \textasciitilde{}750 TB (uncompressed)
                \item The largest public source code archive in the world (and
                    growing!)
            \end{itemize}
        \end{block}
    \end{frame}



    \section{Data Model}

    \begin{frame}
        \frametitle{A source code directory}

        \begin{columns}
            \column{.30\textwidth}
            \begin{figure}
                \begin{minipage}{\textwidth}
                \dirtree{%
                    .1 /.
                        .2 src.
                            .3 evalexpr.c.
                            .3 parser.
                                .4 ast.c.
                                .4 parser.c.
                                .4 lexer.c.
                        .2 tests.
                            .3 eval.c.
                            .3 operands.c.
                }
            \end{minipage}
            \end{figure}
            \column{.70\textwidth}
            \begin{figure}
                \centering
                \scalebox{0.9}{\input{../tikz/figures/dir-tree.tikz}}
            \end{figure}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Revisions}
        % Parallel history
        \begin{block}{}
            \begin{itemize}
                \item \textbf{Revisions} (or ``commits'') keep track of
                    successive states of a source directory.
            \end{itemize}
        \end{block}
        \vfill
        \begin{figure}
            \centering
            \scalebox{0.8}{\input{../tikz/figures/rev-chain-example.tikz}}
        \end{figure}
    \end{frame}

    \begin{frame}
        \frametitle{Branches}

        \begin{block}{}
            Developers can use ``branches'' to work on different features
            simultaneously.
        \end{block}
        \vfill
        % TODO: add a time arrow
        \begin{figure}
            \centering
            \scalebox{0.8}{\input{../tikz/figures/rev-branching-merging.tikz}}
        \end{figure}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Branches}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item \textbf{Branches} are dynamic pointers to revisions,
    %                 using mnemonic names to keep track of their purpose.
    %             \item Branch pointers move when more revisions are added to the
    %                 branch.
    %         \end{itemize}
    %     \end{block}
    %     % TODO: animate
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/rev-branches.tikz}}
    %     \end{figure}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Releases}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item \textbf{Releases} (or ``tags'') point to specific
    %                 milestones in the development history.
    %             \item Their names usually represent the software versions.
    %         \end{itemize}
    %     \end{block}
    %     % TODO: animate
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/rel-example.tikz}}
    %     \end{figure}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Deduplication}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item Lots of frozen states $\Rightarrow$ lots of copies of
    %                 objects
    %             \item Most objects stay identical from one revision to another
    %             \pause
    %             \item We can identify \& deduplicate them with
    %                 \textbf{cryptographic hash functions}.
    %         \end{itemize}
    %     \end{block}
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/cryptographic-hash-function.tikz}}
    %     \end{figure}
    %     \begin{block}{Cryptographic hash functions (SHA-1, SHA-256, BLAKE2, …)}
    %         \begin{itemize}
    %             \item Associates an arbitrary input with a
    %                 \emph{unique\footnote{Terms and conditions apply.}
    %                 identifier} called a \textbf{hash}
    %             \item Check if two objects are identical in O(1).
    %         \end{itemize}
    %     \end{block}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Deduplicating files}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item VCSs identify each file via their unique hash
    %             \item Identical files are \emph{deduplicated} (= shared) from
    %                 one revision to another.
    %         \end{itemize}
    %     \end{block}
    %     % TODO: animate
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/deduplicate-contents.tikz}}
    %     \end{figure}
    % \end{frame}

    \begin{frame}
        \frametitle{Deduplication}
        \begin{block}{}
            \begin{itemize}
                \item Instead of copying the nodes between each revision, we
                    can identify \& deduplicate them with \textbf{cryptographic
                    hash functions} (e.g., SHA-1)
                \item Each object is identified by a unique identifier
                    (``hash'') computed from its entire subtree
            \end{itemize}
        \end{block}
        % TODO: animate
        \vfill
        \begin{figure}
            \centering
            \scalebox{0.8}{\input{../tikz/figures/deduplicate-subtrees.tikz}}
        \end{figure}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Merkle DAG}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item Changing a single object only requires
    %                 $O(h)$ new nodes
    %         \end{itemize}
    %     \end{block}
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/okasaki-complexity.tikz}}
    %     \end{figure}
    % \end{frame}

    \begin{frame}
        \frametitle{Consolidation in a single archive}

        \begin{columns}
            \column{.50\textwidth}
            \begin{block}{}
                \begin{itemize}
                    \item In Software Heritage, \emph{all} the repositories are
                        consolidated into a single archive
                    \item Software artifacts are deduplicated \emph{across
                        different repositories}
                    \item The result is a single graph providing a
                        \textbf{global, unified view} on \textbf{all the
                        software development artifacts} relevant for software
                        mining research.
                    \item Helpful analogy: like a single Git repository but
                        with all the public code in the world.
                \end{itemize}
            \end{block}
            \column{.50\textwidth}
            \begin{figure}
                \centering
                \scalebox{0.4}{\input{../tikz/figures/consolidating-archive.tikz}}
            \end{figure}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Software Heritage Merkle DAG}
        \begin{block}{}
            The SWH graph \textbf{materializes the relationships} between
            all public software artifacts.

            \begin{itemize}
                \item Hash-based deduplication applied on every node in the
                    graph $\Rightarrow$ \textbf{Merkle DAG}
                \item Saves space, ensures data integrity
                \item Persistent structure: append only, great for archival
            \end{itemize}
        \end{block}
        \vfill
        \begin{columns}
            \column{0.2\columnwidth}
            \hfill
            \column{0.6\columnwidth}
            \begin{figure}
                \centering
                \scalebox{0.6}{\input{../tikz/figures/swh-model.tikz}}
            \end{figure}
            \column{0.2\columnwidth}
            \begin{block}{}
                \begin{itemize}
                    \item 20 B nodes
                    \item 220 B edges
                \end{itemize}
            \end{block}
        \end{columns}

    \end{frame}

    % \begin{frame}
    %     \frametitle{Software Heritage Merkle DAG: Detailed view}
    %     \begin{figure}
    %         \centering
    %         \includegraphics[height=7.5cm]{../img/swh-merkle-dag}
    %     \end{figure}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Graph statistics}
    %     \begin{columns}
    %         \column{.50\textwidth}
    %         \begin{block}{Graph topology}
    %             \begin{itemize}
    %                 \item $\approx$ 20 billion nodes
    %                 \item $\approx$ 220 billion edges
    %             \end{itemize}
    %         \end{block}

    %         \column{.50\textwidth}
    %         \begin{block}{Software artifacts}
    %             \begin{itemize}
    %                 \item $\approx$ 150 million software projects
    %                 \item $\approx$ 2 billion commits
    %                 \item $\approx$ 10 billion source code files
    %             \end{itemize}
    %         \end{block}
    %     \end{columns}
    % \end{frame}

    % vcs
    % archive

    \section{Making Software Data Available for Mining}

    \begin{frame}
        \frametitle{Requirement analysis for Empirical Software Engineering}

        \begin{block}{Identifying researchers need}
            Literature review of \textbf{54 papers} from the Mining Software
            Repositories conference (MSR~2019).
        \end{block}

        \begin{block}{Categories of requested data}
            \begin{itemize}
                \item Blobs
                \item Filesystem hierarchy (\emph{file names, directories})
                \item History graph (\emph{revisions})
                \item Content search (\emph{full-text search index})
                \item Provenance (\emph{backwards index})
                \item Commit diffs
                \item Community graph (\emph{revision authors})
                \item Dependency data
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Data volume challenges}

        \begin{block}{Local analysis}
            Handling data at that scale is a hard practical problem for
            researchers:
            \begin{itemize}
                \item Data does not fit on a single machine
                \item High deduplication: entangled structure, hard to
                    parallelize
                \item Downloading this volume of data can take months
            \end{itemize}
        \end{block}

        \begin{block}{Approaches addressed in this thesis}
            \begin{itemize}
                \item Sampling: access restricted amounts of data
                \item Scale-out: platform for distributed computing
                \item Scale-up: compression
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{The Vault}

        \begin{block}{}
            The \textbf{Vault}: Download single directories or entire
            repositories.
        \end{block}

        \begin{center}
            \includegraphics[width=\linewidth]{img/vault.png}
        \end{center}

        \begin{block}{}
            \begin{itemize}
                \item Retrieves the transitive closure of a
                    given object and bundles it in a tarball.
                \item Serves as a cache for downloadable tarballs.
                \item Scales up to tens of thousands of repositories
            \end{itemize}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item \textbf{Pro}: does not require any special analysis tool
                \item \textbf{Con}: no deduplication (larger size, loss of
                    information)
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{SwhFS}

        \begin{block}{}
            The \textbf{Software Heritage Filesystem}: a virtual FUSE
            filesystem to mount the archive as a local directory.

            \begin{itemize}
                \item Useful for prototyping
                \item Easy to exploit with common CLI tools \\
                    → Local file hierarchy maps well with archived repositories
                \item Suited for small-scale experiments
            \end{itemize}

            \footnotesize
            \begin{thebibliography}{swhfs}
                \bibitem{swhfs2020} Thibault Allançon, Antoine Pietri, Stefano Zacchiroli\newblock
                The Software Heritage Filesystem (SwhFS): Integrating Source Code Archival with Development\newblock
                ICSE 2021, IEEE\newblock
            \end{thebibliography}
        \end{block}

        \begin{block}{}
            \begin{minted}{console}
$ cd archive/swh:1:dir:1fee702c7e6d14395bbf5ac3598e73bcbf97b030
$ grep -i antenna THE_LUNAR_LANDING.s | cut -f 5
# IS THE LR ANTENNA IN POSITION 1 YET
# BRANCH IF ANTENNA ALREADY IN POSITION 1
            \end{minted}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{The Software Heritage Graph Dataset}

        \begin{block}{}
            The \textbf{Software Heritage Graph Dataset}: a snapshot of the
            entire graph of software development (without the file contents).

            \footnotesize
            \begin{thebibliography}{msr2019}
                \bibitem{swhgraph2019} Antoine Pietri, Diomidis Spinellis, Stefano Zacchiroli\newblock
                The Software Heritage graph dataset: public software development under one roof\newblock
                Mining Software Repositories 2019\newblock
            \end{thebibliography}
        \end{block}

        \begin{block}{Formats}
            \begin{itemize}
                \item A set of \emph{relational tables} in columnar format for
                    scale-out processing
                \item A \emph{graph edges} format for use in graph databases
                    and graph analysis platforms
            \end{itemize}
        \end{block}

        \begin{block}{Availability}
            \begin{itemize}
                \item Downloadable for local use
                \item Cloud processing platforms: Amazon Athena, Azure
                    Databricks
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}[fragile]{Example queries}
        \begin{block}{Most frequent first commit words}
            \begin{minted}[fontsize=\small]{sql}
SELECT COUNT(*) AS c, word FROM (
  SELECT LOWER(REGEXP_EXTRACT(FROM_UTF8(
  message), 'ˆ\w+')) AS word FROM revision)
WHERE word != ''
GROUP BY word ORDER BY COUNT(*) DESC LIMIT 5;
            \end{minted}

            \begin{center}
                \begin{tabular}{rl}
                    Count & Word\\
                    \hline
                    \num{71338310} & update\\
                    \num{64980346} & merge\\
                    \num{56854372} & add\\
                    \num{44971954} & added\\
                    \num{33222056} & fix\\
                \end{tabular}
            \end{center}
        \end{block}

        \begin{block}{}
            Analyzes 1.1 billion revision messages in 30 seconds.
        \end{block}
    \end{frame}

    \begin{frame}[fragile]{Example queries}
        \begin{columns}
            \column{0.5\textwidth}
            \begin{block}{Weekend work}
                \inputminted[fontsize=\tiny, firstline=3]{sql}{../codesamples/graph-dataset/weekend-work.sql}
            \end{block}
            \column{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\linewidth]{../img/graph-dataset/weekend-work}
            \end{center}
        \end{columns}

        \begin{block}{}
            Analyzes 1.1 billion revision timestamps in 7 seconds.
        \end{block}
    \end{frame}

    % \begin{frame}[fragile]{Example queries}
    %     \begin{block}{Spark: Connected components size distribution}
    %         \inputminted[fontsize=\small]{sql}{../codesamples/graph-dataset/spark-cc.py}
    %     \end{block}

    %     \begin{block}{}
    %         \textbf{Warning}: distributed graph algorithms on Spark are very
    %         expensive (\textasciitilde{}5000 USD for the entire graph with
    %         Azure Databricks).
    %     \end{block}
    % \end{frame}

    \section{Graph Compression and Exploitation}

    \begin{frame}
        \frametitle{Compression approach}

        \begin{block}{}
            \textbf{Objective}: Analyzing the \emph{entire graph of public
            software development} on a single machine.

            \footnotesize
            \begin{thebibliography}{swhgraphcomp}
                \bibitem{Boldi2020} Paolo Boldi, Antoine Pietri, Sebastiano Vigna, Stefano Zacchiroli
                \newblock Ultra-Large-Scale Repository Analysis via Graph Compression
                \newblock SANER 2020, 27th Intl. Conf. on Software Analysis, Evolution and Reengineering. IEEE
            \end{thebibliography}
        \end{block}

        \begin{block}{Advantages}
            \begin{itemize}
                \item Simpler for prototyping, no need to write distributed
                    algorithms
                \item Cheaper than scale-out processing
                \item No need to resort to sampling
                \item Allows us to run topological analyses quickly
            \end{itemize}
        \end{block}

        \begin{block}{Compression techniques}
            We apply existing compression techniques used to compress the
            \textbf{graph of the Web}.
        \end{block}
    \end{frame}

    \begin{frame}{Compression pipeline}
        \begin{block}{Web graph → Software development graph: (re)establishing locality}
            Key for good compression is a \alert{node ordering} that ensures
            neighbor \textbf{locality} and adjacency \textbf{similarity}.
            \begin{itemize}
                \item Lexicographically-ordered URLs in the Graph of the Web
                    have these properties.
                \item It is \emph{not} the case with cryptographic Merkle
                    IDs\ldots{}
                \item \ldots{}but is the case \emph{again} after a
                    breadth-first traversal
            \end{itemize}
        \end{block}
        \vspace{-0.7cm}
        \begin{center}
            \includegraphics[width=1\linewidth]{../img/compression/compression_steps-nofiles}
        \end{center}
        \vspace{-1cm}
        \begin{itemize}
            \item \alert{MPH:} minimal perfect hash, mapping Merkle IDs to 0..N-1 integers
            \item \alert{BV compress:} Boldi-Vigna compression (based on MPH order)
            \item \alert{BFS:} breadth-first visit to renumber
            \item \alert{Permute:} update BV compression according to BFS order
        \end{itemize}
    \end{frame}

    \begin{frame}{Compression results}

        \begin{block}{}
            We ran the compression pipeline on the input corpus using the WebGraph
            framework
            \begin{thebibliography}{}
                \footnotesize
                \bibitem{BoVWFI} Paolo Boldi and Sebastiano Vigna.
                \newblock The WebGraph framework I: Compression techniques
                \newblock WWW 2004: 13th Intl. World Wide Web Conference. ACM
            \end{thebibliography}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item Server equipped with 24 CPUs and 750 GB of RAM
                \item \textbf{Compression time}: 138 hours (6 days)
                \item \textbf{Compression efficiency}: 6 TiB edge file → 91 GiB
                    forward, 83 GiB transposed
            \end{itemize}
        \end{block}

        \begin{block}{Benchmarks}
            \begin{itemize}
                \item Full traversal: 1h48m (1.81 M nodes/s)
                    \begin{itemize}
                        \item For comparison, on Spark: 4 hours, 80 nodes(!),
                            5000\,USD
                    \end{itemize}
                \item Edge access: 12.0 M edges/s
            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{LLP compression}
        \begin{block}{Layered Label Propagation}
            \begin{thebibliography}{Foo Bar, 1969}
                \small \vspace{-2mm}
                \bibitem{Boldi2010} Paolo Boldi, Marco Rosa, Massimo Santini, Sebastiano Vigna
                \newblock Layered Label Propagation: A MultiResolution Coordinate-Free Ordering for Compressing Social Networks
                \newblock WWW 2010: 20th Intl. World Wide Web Conference. ACM
            \end{thebibliography}
        \end{block}
        \begin{block}{}
            \begin{itemize}
                \item Algorithm to uncover locality information
                \item Propagates labels on random nodes to discover neighborhoods
                \item Compression requires more runtime memory (33 bytes per node)
                \item Even more impressive compression ratio (91 GiB → 60 GiB,
                    reduced by $\textasciitilde{}35\%$)
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{Graph Attributes}
        \begin{block}{Node attributes}
            \begin{itemize}
                \item The compressed in-memory graph structure has \alert{no attributes}
                \item Usual data design is to exploit the 0..N-1 integer ranges to \alert{memory map
                    \emph{node} attributes} from secondary storage (node ID →
                    node attribute)
                    \begin{itemize}
                        \item We do this for node types (mapping: 4 GiB),
                            timestamps (mapping: 149 GiB), etc.
                        \item Data structures: integer/byte arrays, front-coded
                            string lists, etc.
                    \end{itemize}
            \end{itemize}
        \end{block}
        \begin{block}{Edge attributes}
            \begin{itemize}
                \item Built-in WebGraph support for attributes on the \alert{edges} (generally integers)
                \item For file \emph{names}, we use another minimal perfect hash to map file names to integers
            \end{itemize}
        \end{block}
        \begin{block}{Disk/memory consideration}
            \begin{itemize}
                \item Labels and mappings can be either in RAM or
                    \texttt{mmap()}-ed from disk
                \item Time/memory tradeoff, depends on access patterns, intensive
                    workloads etc.
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}[fragile]{Graph Querying}
        \begin{block}{}
            \textbf{Option 1}: Write a traversal algorithm using low-level Java
            primitives
        \end{block}
        \begin{minted}[fontsize=\scriptsize,highlightlines={8}]{java}
HashSet<Long> visited = new HashSet<>();
Stack<Long> stack = new Stack<>();
stack.push(srcNodeId);
visited.add(srcNodeId);

while (!stack.isEmpty()) {
    long currentNodeId = stack.pop();
    LazyLongIterator it = graph.successors(currentNodeId);
    for (long neighborNodeId; (neighborNodeId = it.nextLong()) != -1; ) {
        if (!visited.contains(neighborNodeId)) {
            stack.push(neighborNodeId);
            visited.add(neighborNodeId);
        }
    }
}
        \end{minted}
        \begin{block}{}
            \begin{itemize}
                \item Very efficient but burdensome, requires local access to
                    the graph server.
                \item Most traversals are simple traversals ⇒ need for a
                    generic traversal query interface.
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{Graph Querying}

        \begin{block}{}
            \textbf{Option 2}: HTTP API for simple graph traversals

            \begin{itemize}
                \item Generic remote API for graph traversals, Java/Python/aiohttp backend
                \item Limited to simple DFS from a single node (forward or
                    backward graph)
                \item Traversal types: neighbors, leaves, all nodes, all edges
                \item Supports edge type filtering
            \end{itemize}
        \end{block}

        \begin{minted}[fontsize=\scriptsize]{text}
> GET /leaves/swh:1:rev:f39d[...]2a35?direction=backward
swh:1:ori:634a2b699d442aa9abd5008f379847816f54ab85
swh:1:ori:571a86b198c6c66ef33025249f7e455b529aae65
swh:1:ori:c15194d6cb59a6d32777ca3b287ea6664d540df3
...

> GET /visit/nodes/swh:1:rev:c6df[...]fc28?edges=rel:rev,rev:rev
swh:1:rel:c6df0a7ef73ca90825f1472b8a3c5f7a2ce3fc28
swh:1:rev:c8448ff2f9234332f0bc25dc3a13031f8ab3c73c
swh:1:rev:4b63dbd4e782e74bdc050c4579381d29b4bd41c0
...
        \end{minted}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Graph Subdatasets}

    %     \begin{block}{Generating representative subgraphs}
    %         \begin{itemize}
    %             \item Useful for smaller-scale experimentation, prototyping
    %             \item Focusing analysis on a relevant subset
    %             \item Representative samples → transitive closure of a subset
    %                 of origins
    %             \item Use a fitted log model to estimate the size of the
    %                 resulting subgraph
    %         \end{itemize}
    %     \end{block}

    %     \begin{center}
    %         \includegraphics[width=.5\linewidth]{../img/graph-exploitation/subdataset_size_function_fit.pdf}
    %     \end{center}
    % \end{frame}

    \section{Graph Topology of Software Development}

    \begin{frame}
        \frametitle{Graph Topology: Research Questions}

        \begin{block}{}
            The Software Heritage Graph Dataset materializes a \emph{network of
            relationships between software artifacts} which has not yet been
            empirically studied as a whole.
        \end{block}

        \begin{block}{Research questions}

            \begin{itemize}
                \item What is the low-level topology of the graph of software
                    development?

                    Network topology metrics: Degree distributions, connected
                    components, distance between roots and leaves, clustering
                    coefficient.

                \item What do these metrics tell us about this graph and its
                    layers?
                    \begin{itemize}
                \item Best approaches for large scale analysis?
                \item Methodological implications for software mining?
                    \end{itemize}
            \end{itemize}

            The \textbf{compressed graph framework} allows us to answer these
            questions experimentally.

            \footnotesize
            \begin{thebibliography}{swhforks}
                \bibitem{swhforks} Antoine Pietri, Guillaume Rousseau, Stefano Zacchiroli\newblock
                Determining the intrinsic structure of public software development history\newblock
                Mining Software Repositories 2020\newblock
            \end{thebibliography}
        \end{block}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Graph layers}

    %     \begin{block}{}
    %         We study the topology of the graph as a whole, but also of its
    %         different semantic layers:
    %     \end{block}

    %     \begin{center}
    %         \scalebox{0.6}{\input{../tikz/figures/swh-layers.tikz}}
    %     \end{center}
    % \end{frame}

    \begin{frame}
        \frametitle{Average degree}

        \begin{table}
            \centering
            \begin{tabular}[t]{l S[table-format=3.3]}
                \textbf{Dataset} & \textbf{Average degree} \\
                \hline
                \textbf{swh-2020-history}    & 1.021 \\
                \textbf{swh-2020-commit}     & 1.022 \\
                \textbf{swh-2020-hosting}    & 3.39 \\
                bitcoin-2013 & 6.4 \\
                dblp-2011 (Co-authorship)       & 6.8 \\
                \textbf{swh-2020}            & 11.0 \\
                \textbf{swh-2020-filesystem} & 12.1 \\
                twitter-2010      & 35.2 \\
                clueweb12                    & 43.1 \\
                uk-2014 (Web)               & 60.4 \\
                fb-2011 (Facebook)          & 169.0 \\
            \end{tabular}
        \end{table}
    \end{frame}

    \begin{frame}
        \frametitle{Out-degree distributions: filesystem layer}

        \begin{block}{}
            Distribution of the number of entries of each directory in the
            graph
        \end{block}

        \begin{figure}
            \centering
            \includegraphics[width=0.5\linewidth]{../img/topology/inout/dir+cnt_out}
        \end{figure}

        \begin{block}{}
            ⇒ Scale invariance, no characteristic degree (diverging average).
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Out-degree distributions: commit layer}
        \begin{block}{}
            Distribution of the number of parents of each commit in the graph
        \end{block}

        \begin{figure}
            \centering
            \includegraphics[width=0.5\linewidth]{../img/topology/inout/rev_out}
        \end{figure}

        \begin{block}{}
            Characteristic degrees (<3) due to development patterns.
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Distance between roots and leaves}
        \begin{figure}
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=\linewidth]{../img/topology/shortestpath/dir+cnt}
                \caption{Filesystem layer}
            \end{subfigure}\hfill
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=\linewidth]{../img/topology/shortestpath/rev}
                \caption{Commit layer}
            \end{subfigure}
        \end{figure}
    \end{frame}

    \begin{frame}
        \frametitle{Connected components}
        \begin{figure}
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=0.9\linewidth]{../img/topology/connectedcomponents/dir+cnt}
                \caption{Filesystem layer}
            \end{subfigure}\hfill
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=0.9\linewidth]{../img/topology/connectedcomponents/rev}
                \caption{Commit layer}
            \end{subfigure}
        \end{figure}

        \begin{center}
            \begin{tabular}[t]{l r r r}
                \textbf{Layer} & \textbf{\# of WCC}
                               & \textbf{Size of largest WCC}
                               & \textbf{\% of nodes in largest}
                               \\
                               \hline
                Full graph       & \num{33104255}  & \num{18902683142} & 97.79\% \\
                Filesystem layer & \num{46286502}  & \num{16565521611} & 97.16\% \\
                Commit layer     & \num{88031649}  & \num{51543944}    & 2.61\% \\
            \end{tabular}
        \end{center}
    \end{frame}

    \begin{frame}
        \frametitle{Takeaways: filesystem / commit layer duality}

        \begin{block}{}
            The filesystem and commit layers have almost opposite topological
            properties.
        \end{block}

        \begin{columns}
            \column{0.5\columnwidth}
            \begin{block}{Filesystem layer}
                \begin{itemize}
                    \item Dense, non-partitionable (giant WCC)
                    \item Characteristic depth
                    \item Arbitrary outdegree
                \end{itemize}
                \begin{center}
                    \scalebox{0.7}{\input{../tikz/figures/topology-summary-filesystem.tikz}}
                \end{center}
            \end{block}

            \column{0.5\columnwidth}
            \begin{block}{Commit layer}
                \begin{itemize}
                    \item Sparse, partitionable (max WCC = 3\%)
                    \item Arbitrary depth
                    \item Characteristic outdegree (degenerate)
                \end{itemize}
                \begin{center}
                    \scalebox{0.5}{\input{../tikz/figures/topology-summary-revision.tikz}}
                \end{center}
            \end{block}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Implications for software mining research}

        \begin{block}{Layers}
            \begin{itemize}
                \item Large disparity in the low-level structure of layers
                \item Important to study layers separately to understand
                    the graph structure
            \end{itemize}
        \end{block}

        \begin{block}{Methodology}
            \begin{itemize}
                \item High kurtosis / propensity to produce outliers
                \item No obvious rule to ``filter'' outliers in many
                    distributions
                \item Highlights the importance of exhaustive approaches
            \end{itemize}
        \end{block}

        \begin{block}{Distributed analysis}
            \begin{itemize}
                \item No natural partitioning in small connected components
                \item Need for more subtle approaches (modular decomposition?)
            \end{itemize}
        \end{block}
    \end{frame}

    \section{Identification of Software Forks}

    \begin{frame}
        \frametitle{Studying software forks}

        \begin{block}{}

            \begin{itemize}
                \item Topology analysis of the graph of software development
                    gives us a low-level understanding of relationships between
                    \emph{deduplicated software artifacts}
                \item At a higher level, this graph is structured by
                    \textbf{code reuse patterns}
                \item One way to understand these is to study \textbf{software
                    forks}: projects derived from existing software and being
                    developed independently.
            \end{itemize}

            \footnotesize
            \begin{thebibliography}{swhforks}
                \bibitem{swhforks} Antoine Pietri, Guillaume Rousseau, Stefano Zacchiroli\newblock
                Forking Without Clicking: on How to Identify Software Repository Forks\newblock
                Mining Software Repositories 2020\newblock
            \end{thebibliography}
        \end{block}

        \begin{block}{Applications in software evolution \& software health}
            \begin{itemize}
                \item Finding active and maintained projects
                \item Understanding ``Hard'' and ``development'' forks
                \item Identifying criteria for successful forks
            \end{itemize}
        \end{block}
    \end{frame}

                % \item The compressed graph allows us to run exhaustive
                %     quantitative studies on software forks
                % \item Understanding forks is a key research direction in
                %     software health and software evolution

    \begin{frame}
        \frametitle{Previous approaches for studying forks}

        \begin{block}{}
            \begin{itemize}
                \item Historically, studies have relied on forge-specific
                    metadata: relationships created by pressing the "fork"
                    button
                    \begin{center}
                        \includegraphics[width=2cm]{img/forkbutton}
                    \end{center}
                \item Github provides a graph of forks to parent repositories
                    \begin{center}
                        \includegraphics[width=7cm]{img/forknetwork}
                    \end{center}
            \end{itemize}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item Restricted to a single forge, relies on external metadata
                \item We can use the \textbf{compressed graph} for a more
                    exhaustive approach
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Intrinsic forks}
        \begin{block}{Concept}
            \begin{center}
                \begin{tikzpicture}[every node/.append style={cloud,draw,very thick,align=center}]
                    \node[cloud puffs=50, aspect=7] (cloud){%
                        \textbf{Intrinsic forks} are software projects\\
                        with \textbf{shared development history}.
                    };
                \end{tikzpicture}
            \end{center}

            \begin{itemize}
                \item Identified by intrinsic DVCS information
                    \begin{itemize}
                        \item Shared commits
                        \item Shared root directories
                    \end{itemize}
                \item Includes projects for which no forge-level metadata is present
            \end{itemize}
        \end{block}

        \begin{block}{Research question}
            How do forge forks compare to forks defined by shared development
            history?
        \end{block}
    \end{frame}

    \begin{frame}{Methodology}
        \begin{block}{Experimental design}
            We \textbf{identify} intrinsic forks in the graph of development
            history, then quantitatively \textbf{compare} them to forks
            identified by forge metadata.
        \end{block}

        \begin{block}{New concepts}
            \begin{itemize}
                \item \textbf{Fork networks}: repositories connected by
                    forking relationships
                \item \textbf{Fork cliques}: repositories that are all forks of
                    each other
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Distribution comparison (fork cliques)}
        \begin{center}
            \includegraphics[width=0.5\linewidth]{../img/forks/fork-clique-partition-freq-distribution.pdf}
        \end{center}
        \begin{block}{}
            \begin{itemize}
                \item \textbf{+8\% new forks discovered} (shared-commit forks)
                \item Positive difference ⇒ more exhaustive identification of
                    forks
                \item Distribution similarity ⇒ Matches developers'
                    expectations of what a fork is
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Implications for software mining research}

        \begin{block}{}
            \begin{itemize}
                \item Relying on forge metadata ⇒ selection bias, not
                    exhaustive
                \item Using shared development history uncovers more forks
                \item Robust approach across forges and across VCSs
            \end{itemize}
        \end{block}

    \end{frame}

    \section{Conclusion}

    \begin{frame}
        \frametitle{Academic contributions}

        \begin{block}{Contextualization}
            \begin{itemize}
                \item Literature review: identify research needs
                \item Roadmap to universal software mining
            \end{itemize}
        \end{block}

        \begin{block}{Making software artifacts data available}
            \begin{itemize}
                \item Small scale: Vault, SwhFS
                \item Scale-out: Graph dataset
                \item Scale-up: Graph compression \& exploitation
            \end{itemize}
        \end{block}

        \begin{block}{Empirical studies on the graph structure}
            \begin{itemize}
                \item Low-level: topological properties
                \item High-level: structure of forks
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Empirical findings \& impact on software mining}
        \begin{block}{Topology}
            \begin{itemize}
                \item Disparity between graph layers
                \item Graph cannot be partitioned naturally
                \item No sound way to filter outliers
            \end{itemize}
        \end{block}
        \begin{block}{Forks}
            \begin{itemize}
                \item Forks can be identified more exhaustively with shared
                    development history
                \item They qualitatively fit established notions of what constitutes a ``fork''
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Future work}

        \begin{block}{}
            \begin{itemize}
                \item Incremental graph compression
                \item Expressive remote graph querying
                \item Graph partitioning techniques for sharding (modular
                    decomposition?)
                \item Derived graphs (community graphs, diff graphs, …)
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Thanks!}

        \begin{block}{}
            \emph{All} this work is open \{source, data, access, …\}.

            \url{https://forge.softwareheritage.org}
            \hfill
            \url{https://github.com/seirl/thesis}
        \end{block}

        \begin{block}{}
            \tiny
            \begin{itemize}
                \item \fullcite{swh-benevol2018-universal-analysis}
                \item \fullcite{swh-msr2019-dataset}
                \item \fullcite{msr-2020-challenge}
                \item \fullcite{saner-2020-swh-graph}
                \item \fullcite{swh-msr2020-forking}
                \item \fullcite{msr-2020-topology}
                \item \fullcite{swh-2021-swhfs}
            \end{itemize}
        \end{block}
    \end{frame}
\end{document}
