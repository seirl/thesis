\documentclass[aspectratio=169,xcolor=table]{beamer}
\input{header.tex}
\addbibresource{../thesis.bib}

\begin{document}
    \pdfpcnote{%
        Welcome to my defense, I'm going to present what I've been working
        on during my PhD, under the supervision of my advisor, Stefano
        Zacchiroli.  Before discussing my subject, I first need to give a
        bit of context.
    }
    \maketitle

    \section*{Context}
    % presentation
    % universal software mining

    \begin{frame}
        \frametitle{Sofware Mining}

        \pdfpcnote{%
            I worked in a field of research called "software mining". The goal
            is to study existing software repositories in a systematic way to
            try to **uncover patterns** in software development and in the
            **social processes that govern it**, in the goal of improving these
            development practices.
            \\\\
            This field has a lot of applications, like automated bug fixing,
            code completion or finding unmaintained dependencies. Because of
            this potential, it has drawn a lot of interest from computer
            science researchers.
        }

        \begin{block}{Definition}
            \textbf{Software mining}: studying existing software repositories
            to help improve software development processes and practices.
            % all the byproducts of software development
            % improve the software of tomorrow
            % inform development best practices
        \end{block}

        \begin{block}{Applications}
            \begin{itemize}
                \item Software health, software evolution
                \item Automated bug detection
                \item Automated vulnerability repair
                \item Code autocompletion
                \item Clone detection
                \item License compliance
                \item …
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Universal Software Mining}

        \pdfpcnote{%
            Initially, software mining studies were done at a relatively
            small scale, only individual projects or a dozen related projects.
            Then it started growing in scale, with researchers analyzing
            thousands of repositories at once or even sometimes up to entire
            ecosystems. This allowed us to have a more and more exhaustive
            understanding of software development processes.
            \\\\
            The main question that initiated the work I'm presenting here is:
            could we go even further? Could we provide a way to run empirical
            studies on all the body of public software development?
            \\\\
            This would be really impactful for the software mining field. It
            would **reduce the barriers of entry** for researchers, who
            wouldn't have to do all the manual data crawling themselves. It
            would make it really easy to replicate studies, and give us a more
            complete/exhaustive view on development practices and social
            processes.
        }

        \begin{block}{Current scale of software mining studies}
            \begin{itemize}
                \item Individual projects
                \item Up to thousands of popular repositories (e.g., ``top
                    1000 by stars'')
                \item Entire ecosystems (app stores, package managers, …)
            \end{itemize}
        \end{block}

        \begin{block}{Universal software mining}
            Next step: a framework to run empirical studies on
            \textbf{all the public software repositories}?

            \begin{itemize}
                \item Less repetitive, no need to crawl the data for each study
                \item Easier to replicate studies
                \item High-level view of social processes in software
                    development
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{In this thesis…}

        \pdfpcnote{%
            In this thesis, I study...
            \\\\
            The goal I'm working towards here is trying to build a research
            platform, a place where researchers could come with their empirical
            studies and run them on this exhaustive dataset. And all that
            without having to do the boring work/manual crawling.
        }

        \begin{block}{}
            \Large
            I study how to organize the \textbf{graph of public software
            development}, a comprehensive dataset of software development data,
            to make it \textbf{accessible for software mining research}.
        \end{block}

        \begin{block}{}
            \emph{Research direction}: Working towards a research platform for
            Universal Software Analysis.

            \footnotesize
            \begin{thebibliography}{swhbenevol}
                \bibitem{swhbenevol2018} Antoine Pietri, Stefano Zacchiroli\newblock
                Towards Universal Software Evolution Analysis\newblock
                BENEVOL 2018\newblock
            \end{thebibliography}
        \end{block}
    \end{frame}


    % \begin{frame}
    %     \frametitle{The Software Heritage Initiative}

    %     \begin{center}
    %         \includegraphics[width=.5\linewidth]{img/SWH-logo+motto.pdf}
    %     \end{center}

    %     \begin{block}{Collect, preserve and share \emph{all} software source
    %         code}
    %         \hfill Preserving our heritage, enabling better software and better
    %         science for all
    %         % \pause
    %     \end{block}

    %     \begin{columns}
    %         \begin{column}{.3\columnwidth}
    %             \begin{block}{Reference catalog}
    %                 \begin{center}
    %                     \includegraphics[width=.6\linewidth]{img/myriadsources}
    %                 \end{center}
    %                 \alert{find} and \alert{reference} all software source code
    %                 % \pause
    %             \end{block}
    %         \end{column}
    %         \begin{column}{.3\columnwidth}
    %             \begin{block}{Universal archive}
    %                 \begin{center}
    %                     \includegraphics[width=.6\linewidth]{img/fragilecloud}
    %                 \end{center}
    %                 \alert{preserve} all software source code
    %                 % \pause
    %             \end{block}
    %         \end{column}
    %         \begin{column}{.3\columnwidth}
    %             \begin{block}{Research infrastructure}
    %                 \begin{center}
    %                     \includegraphics[width=.7\linewidth]{img/atacama-telescope}
    %                 \end{center}
    %                 \alert{enable analysis} of all software source code
    %             \end{block}
    %         \end{column}
    %     \end{columns}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Data flow}
    %     \begin{center}
    %         \includegraphics[width=0.9\textwidth]{img/swh-dataflow.pdf}
    %     \end{center}
    % \end{frame}

    \begin{frame}
        \frametitle{Corpus: The Software Heritage Archive ---
        archive.softwareheritage.org}

        \pdfpcnote{%
            As an approximation for this body of software development data, we
            use the Software Heritage archive, which is to my knowledge the
            largest and most exhaustive dataset of software development data
            available publicly.
            \\\\
            It contains more than 10 billion source files from more than 150
            million projects, which sums up to more than 900 TB on disk.
            \\\\
            It archives data from a variety of different places, collaborative
            development platforms like GitHub, Gitlab or Bitbucket, language
            package managers like NPM or PyPI or distribution packages like
            NixOS or Debian. This variety of sources is really interesting
            because it gives us a complete picture of software development in
            general, and not specific to a single platform.
        }

        \begin{block}{}
            \begin{itemize}
                \item We use the \textbf{Software Heritage archive} as our best
                    approximation of the entire corpus of public software
                    development.
                \item Largest public source code archive in the world (more
                    than 900 TB, growing daily).
            \end{itemize}
        \end{block}
        % TODO: update numbers?
        % \vspace{-1mm}
        \begin{center}
            \includegraphics[trim=0 2cm 0 0, clip, width=0.7\linewidth]{img/archive-growth.png}
        \end{center}
        % \vspace{-2mm}
        \begin{center}
            \colorbox{white}{\includegraphics[width=0.7\linewidth]{img/archive-coverage.png}}
        \end{center}
        % \pause
        \vspace{-2mm}
        \begin{block}{}
            \begin{itemize}
                \item On disk: \textasciitilde{}750 TB (uncompressed)
                \item The largest public archive of source code in the world
                    (and growing!)
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{Outline}
        \tableofcontents

        \pdfpcnote{%
            Here is the outline of this presentation.
            \\\\
            First I'm going to present a generic data model of the software
            development data that researchers might want to exploit.
            \\\\
            Then I'll discuss different ways we can make this data accessible
            for software mining at different scales.
            \\\\
            We will see that these approaches have some limitations. To address
            them, I'll introduce graph compression as a way to efficiently run
            exhaustive studies on our corpus.
            \\\\
            Finally I describe two empirical studies I performed, a generic
            network study to understand the topology of the graph of software
            development, and another one to look at how this graph is
            structured by code reuse patterns and software forks.
        }
    \end{frame}

    \section{Data Model}

    \begin{frame}
        \frametitle{A source code directory}

        \pdfpcnote{%
            At a fundamental level, the basic building block of software
            development is source code directories. Here is a very simple
            example of a program written in C.
            \\\\
            There are two types of objects here, source code files at the
            bottom, and the directories that organize them in a tree hierarchy.
            \\\\
            This code generally doesn't remain static, it evolves over time.
            It's really useful to keep track of this evolution.
        }

        \begin{columns}
            \column{.30\textwidth}
            \begin{figure}
                \begin{minipage}{\textwidth}
                \dirtree{%
                    .1 /.
                        .2 src.
                            .3 evalexpr.c.
                            .3 parser.
                                .4 ast.c.
                                .4 parser.c.
                                .4 lexer.c.
                        .2 tests.
                            .3 eval.c.
                            .3 operands.c.
                }
            \end{minipage}
            \end{figure}
            \column{.70\textwidth}
            \begin{figure}
                \centering
                \scalebox{0.9}{\input{../tikz/figures/dir-tree.tikz}}
            \end{figure}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Revisions}
        \pdfpcnote{%
            For that, we introduce another type of object called revisions or
            commits. They form a chain that retains frozen past states of the
            source directory.
            \\\\
            Keeping track of how the codebase evolved is really useful to
            developers, because it allows them to quickly find where a bug
            was introduced and go back to a working state. It also allows
            them to retain institutional knowledge by keeping a memory
            of all the changes that were introduced over time.
        }

        % Parallel history
        \begin{block}{}
            \begin{itemize}
                \item \textbf{Revisions} (or ``commits'') keep track of
                    successive states of a source directory.
            \end{itemize}
        \end{block}
        \vfill
        \begin{figure}
            \centering
            \scalebox{0.8}{\input{tikz/rev-chain-example.tikz}}
        \end{figure}
    \end{frame}

    \begin{frame}
        \frametitle{Branches}
        \pdfpcnote{%
            An advantage of this design is that it allows developers to work in
            parallel, on separate features. For that, they can work on branches
            that they ``merge'' back when they want to integrate their work to
            a mainline.
        }

        \begin{block}{}
            Developers can use ``branches'' to work on different features
            simultaneously.
        \end{block}
        \vfill
        % TODO: add a time arrow
        \begin{figure}
            \centering
            \scalebox{0.8}{\input{../tikz/figures/rev-branching-merging.tikz}}
        \end{figure}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Branches}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item \textbf{Branches} are dynamic pointers to revisions,
    %                 using mnemonic names to keep track of their purpose.
    %             \item Branch pointers move when more revisions are added to the
    %                 branch.
    %         \end{itemize}
    %     \end{block}
    %     % TODO: animate
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/rev-branches.tikz}}
    %     \end{figure}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Releases}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item \textbf{Releases} (or ``tags'') point to specific
    %                 milestones in the development history.
    %             \item Their names usually represent the software versions.
    %         \end{itemize}
    %     \end{block}
    %     % TODO: animate
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/rel-example.tikz}}
    %     \end{figure}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Deduplication}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item Lots of frozen states $\Rightarrow$ lots of copies of
    %                 objects
    %             \item Most objects stay identical from one revision to another
    %             \pause
    %             \item We can identify \& deduplicate them with
    %                 \textbf{cryptographic hash functions}.
    %         \end{itemize}
    %     \end{block}
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/cryptographic-hash-function.tikz}}
    %     \end{figure}
    %     \begin{block}{Cryptographic hash functions (SHA-1, SHA-256, BLAKE2, …)}
    %         \begin{itemize}
    %             \item Associates an arbitrary input with a
    %                 \emph{unique\footnote{Terms and conditions apply.}
    %                 identifier} called a \textbf{hash}
    %             \item Check if two objects are identical in O(1).
    %         \end{itemize}
    %     \end{block}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Deduplicating files}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item VCSs identify each file via their unique hash
    %             \item Identical files are \emph{deduplicated} (= shared) from
    %                 one revision to another.
    %         \end{itemize}
    %     \end{block}
    %     % TODO: animate
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/deduplicate-contents.tikz}}
    %     \end{figure}
    % \end{frame}

    \begin{frame}
        \frametitle{Deduplication}
        \pdfpcnote{%
            Copying these states from one revision to another will duplicate a
            lot of identical objects, like files and directories that weren't
            modified in a given revision. Instead of just storing copies, we
            can identify these duplicate nodes using cryptographic hash
            functions.
            \\\\
            We can attribute a unique fixed-length identifier to any given
            object in the graph by recursively computing these hash functions
            on their entire subtrees. We then store them as single nodes in the
            graph, so a unique directory will only be present once in this
            graph even if a lot of revisions or other directories point towards
            it.
        }
        \begin{block}{}
            \begin{itemize}
                \item Instead of copying the nodes between each revision, we
                    can identify \& deduplicate them with \textbf{cryptographic
                    hash functions} (e.g., SHA-1)
                \item Each object is identified by a unique identifier
                    (``hash'') computed from its entire subtree
            \end{itemize}
        \end{block}
        % TODO: animate
        \vfill
        \begin{figure}
            \centering
            \scalebox{0.8}{\input{../tikz/figures/deduplicate-subtrees.tikz}}
        \end{figure}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Merkle DAG}
    %     \begin{block}{}
    %         \begin{itemize}
    %             \item Changing a single object only requires
    %                 $O(h)$ new nodes
    %         \end{itemize}
    %     \end{block}
    %     \vfill
    %     \begin{figure}
    %         \centering
    %         \scalebox{0.8}{\input{../tikz/figures/okasaki-complexity.tikz}}
    %     \end{figure}
    % \end{frame}

    \begin{frame}
        \frametitle{Consolidation in a single archive}
        \pdfpcnote{%
            The Software Heritage data models extends this idea of
            deduplication of identical objects, but it applies it to the entire
            corpus of software data.
            Basically, all the repositories are consolidated into a single
            archive, and then the objects contained in these repositories are
            deduplicated, but across all the repositories at once. If a file or
            a directory was copied across multiple repositories, they will
            still only be stored once in the archive.
            \\\\
            This is a really interesting data model, because in effect it
            materializes a giant graph which gives us a global, unified view of
            all the software development artifacts that software mining
            researchers might want to analyze. It's like a map of the stars of
            all the software data; we can directly see how code is reused
            across repos through the relationships between objects.
            \\\\
            One way to imagine it is to visualize a single giant Git repository
            with all the code in the world.
        }

        \begin{columns}
            \column{.50\textwidth}
            \begin{block}{}
                \begin{itemize}
                    \item In Software Heritage, \emph{all} the repositories are
                        consolidated into a single archive
                    \item Software artifacts are deduplicated \emph{across
                        different repositories}
                    \item The result is a single graph providing a
                        \textbf{global, unified view} on \textbf{all the
                        software development artifacts} relevant for software
                        mining research.
                    \item Helpful analogy: like a single Git repository but
                        with all the public code in the world.
                \end{itemize}
            \end{block}
            \column{.50\textwidth}
            \begin{figure}
                \centering
                \scalebox{0.4}{\input{../tikz/figures/consolidating-archive.tikz}}
            \end{figure}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Software Heritage Merkle DAG}
        \pdfpcnote{%
            The data model we end up with is called a Merkle DAG because of
            this hash deduplication on all the nodes in the graph. It has very
            useful properties for archival, notably the fact that it's an
            append-only structure where nodes are never modified.
            \\\\
            At the bottom there is the filesystem layer with files and
            directories, then the history layer which contains all the
            commits, and then it goes up to the repository URLs where the data
            was crawled in the first place.
            \\\\
            The graph contains more than 20 billion nodes and 220 billion
            edges, which actually makes it the largest publicly available graph
            dataset.
        }
        \begin{block}{}

            \begin{itemize}
                \item Hash-based deduplication applied on every node in the
                    graph $\Rightarrow$ \textbf{Merkle DAG}
                \item Persistent structure: append only, great for archival
            \end{itemize}
        \end{block}
        \vfill
        \begin{columns}
            \column{0.2\columnwidth}
            \hfill
            \column{0.6\columnwidth}
            \begin{figure}
                \centering
                \scalebox{0.6}{\input{../tikz/figures/swh-model.tikz}}
            \end{figure}
            \column{0.2\columnwidth}
            \begin{block}{}
                \begin{itemize}
                    \item 20 B nodes
                    \item 220 B edges
                \end{itemize}
            \end{block}
        \end{columns}

    \end{frame}

    % \begin{frame}
    %     \frametitle{Software Heritage Merkle DAG: Detailed view}
    %     \begin{figure}
    %         \centering
    %         \includegraphics[height=7.5cm]{../img/swh-merkle-dag}
    %     \end{figure}
    % \end{frame}

    % \begin{frame}
    %     \frametitle{Graph statistics}
    %     \begin{columns}
    %         \column{.50\textwidth}
    %         \begin{block}{Graph topology}
    %             \begin{itemize}
    %                 \item $\approx$ 20 billion nodes
    %                 \item $\approx$ 220 billion edges
    %             \end{itemize}
    %         \end{block}

    %         \column{.50\textwidth}
    %         \begin{block}{Software artifacts}
    %             \begin{itemize}
    %                 \item $\approx$ 150 million software projects
    %                 \item $\approx$ 2 billion commits
    %                 \item $\approx$ 10 billion source code files
    %             \end{itemize}
    %         \end{block}
    %     \end{columns}
    % \end{frame}

    % vcs
    % archive

    \section{Making Software Data Available for Mining}

    \begin{frame}
        \frametitle{Requirement analysis for Empirical Software Engineering}

        \pdfpcnote{%
            The first thing we need is to understand the kinds of studies that
            researchers might want to perform on this data. To get a better
            understanding of that, I reviewed 54 papers from the MSR conference
            and categorized the different types of data that researchers
            usually make use of in their studies.
            \\\\
            A lot of these use cases require being able to efficiently query
            the graph of software development...
        }

        \begin{block}{Identifying researchers need}
            Literature review of \textbf{54 papers} from the Mining Software
            Repositories conference (MSR~2019).
        \end{block}

        \begin{block}{Categories of requested data}
            \begin{itemize}
                \item Blobs
                \item Filesystem hierarchy (\emph{file names, directories})
                \item History graph (\emph{revisions})
                \item Content search (\emph{full-text search index})
                \item Provenance (\emph{backwards index})
                \item Commit diffs
                \item Community graph (\emph{revision authors})
                \item Dependency data
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Data volume challenges}
        \pdfpcnote{%
            But the thing is that this volume of data is completely
            unmanageable for most researchers. It doesn't fit on a single
            machine and so it often requires distributed approaches. Just
            downloading it takes entire months.\\
            Also, its structure makes it hard to analyze because of the high
            deduplication, which makes it a very entangled structure where
            parallel approaches are not necessarily straightforward.
            \\\\
            So in my thesis in present three different approaches to reduce the
            amount of data that researchers need to have **locally**.
            \\\\
            The first one is sampling, giving ways to access a small restricted
            set of data.
            The second one is to help them query the graph remotely by putting
            the graph on cloud-based distributed computing platforms.
            The last one is to compress the data so that it can actually fit on
            a single machine to be analyzed locally.
        }

        \begin{block}{Local analysis}
            Handling data at that scale is a hard practical problem for
            researchers:
            \begin{itemize}
                \item Data does not fit on a single machine
                \item Downloading this volume of data can take months
                \item High deduplication: entangled structure, hard to
                    parallelize
            \end{itemize}
        \end{block}

        \begin{block}{Approaches addressed in this thesis}
            \begin{itemize}
                \item Sampling: access restricted amounts of data
                \item Scale-out: platform for distributed computing
                \item Scale-up: compression
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{The Vault}

        \pdfpcnote{%
            The first thing you could do to analyze many repositories at once
            is to just download them one by one. In the archive there's no
            longer a notion of "repository" because everything is deduplicated.
            So I propose a new tool called the Vault to reconstitute these
            repositories by computing the transitive closure of a given
            artifact.
            \\\\
            We were able to use this for a few mining experiments that required
            up to tens of thousands of repositories at once. It's really
            convienient in that it doesn't require any kind of special analysis
            tool, because in effect you're just downloading the repository
            itself and you can study it like you study any repository. However,
            you lose all the deduplication, so you no longer have this map of
            relationships between software objects.
        }

        \begin{block}{}
            The \textbf{Vault}: Download single directories or entire
            repositories.
        \end{block}

        \begin{center}
            \includegraphics[width=\linewidth]{img/vault.png}
        \end{center}

        \begin{block}{}
            \begin{itemize}
                \item Retrieves the transitive closure of a
                    given object and bundles it in a tarball.
                \item Serves as a cache for downloadable tarballs.
                \item Scales up to tens of thousands of repositories
            \end{itemize}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item \textbf{Pro}: does not require any special analysis tool
                \item \textbf{Con}: no deduplication (larger size, loss of
                    information)
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{SwhFS}

        \pdfpcnote{%
            Another option we propose for sampling is the Software Heritage
            Filesystem, a way to expose the contents of the archive as a
            virtual filesystem. It works by lazily fetching the files and
            directories from the archive when the user accesses them.
            \\\\
            Like for the Vault, this is really useful for small-scale
            experiments because it doesn't require any specialized tools, but
            it's not suitable for exhaustive analyses of the graph.
            As an example, here is a simple way to count the number of lines of
            Javascript code in a given revision.
            \\\\
            So the Vault and this filesystem are the two ways we make small
            sets of data easily accessible for software mining. But our main
            goal is universal software mining, we want to be able to run more
            exhaustive experiments...
        }

        \begin{block}{}
            The \textbf{Software Heritage Filesystem}: a virtual FUSE
            filesystem to mount the archive as a local directory.

            \begin{itemize}
                \item Useful for prototyping
                \item Easy to exploit with common CLI tools \\
                    → Local file hierarchy maps well with archived repositories
                \item Suited for small-scale experiments
            \end{itemize}

            \footnotesize
            \begin{thebibliography}{swhfs}
                \bibitem{swhfs2020} Thibault Allançon, Antoine Pietri, Stefano Zacchiroli\newblock
                The Software Heritage Filesystem (SwhFS): Integrating Source Code Archival with Development\newblock
                ICSE 2021, IEEE\newblock
            \end{thebibliography}
        \end{block}

        \begin{block}{}
            \begin{minted}{console}
$ cd archive/swh:1:rev:9d76c0b163675505d1a901e5fe5249a2c55609bc
$ find root/src/ -type f -name ’*.js’ | xargs cat | wc -l
10136
            \end{minted}
%             \begin{minted}{console}
% $ cd archive/swh:1:dir:1fee702c7e6d14395bbf5ac3598e73bcbf97b030
% $ grep -i antenna THE_LUNAR_LANDING.s | cut -f 5
% # IS THE LR ANTENNA IN POSITION 1 YET
% # BRANCH IF ANTENNA ALREADY IN POSITION 1
%             \end{minted}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{The Software Heritage Graph Dataset}

        \pdfpcnote{%
            So we present the Software Heritage Graph dataset, which is an
            export of the entire graph of software development, without the
            file contents -- just the graph.
            \\\\
            We provide it in two different formats, a set of relational tables
            in columnar format that can be used in classic distributed
            computing platforms like Spark or Hadoop, and one that is more
            graph oriented, a set of edges that you can import in graph
            processing platforms like Neo4J or Amazon Neptune.
            \\\\
            Of course you can just download this dataset and process them on
            your own infrastructure, but the main idea here is to reduce
            frictions and costs, so we have these available as public datasets
            on Amazon Athena and Azure Databricks, that anyone can just query
            without having to pay for the storage.
            \\\\
            Let's see a few examples of experiments that this allows us to
            perform.
        }

        \begin{block}{}
            The \textbf{Software Heritage Graph Dataset}: a snapshot of the
            entire graph of software development (without the file contents).

            \footnotesize
            \begin{thebibliography}{msr2019}
                \bibitem{swhgraph2019} Antoine Pietri, Diomidis Spinellis, Stefano Zacchiroli\newblock
                The Software Heritage graph dataset: public software development under one roof\newblock
                Mining Software Repositories 2019\newblock
            \end{thebibliography}
        \end{block}

        \begin{block}{Formats}
            \begin{itemize}
                \item A set of \emph{relational tables} in columnar format for
                    scale-out processing
                \item A \emph{graph edges} format for use in graph databases
                    and graph analysis platforms
            \end{itemize}
        \end{block}

        \begin{block}{Availability}
            \begin{itemize}
                \item Downloadable for local use
                \item Cloud processing platforms: Amazon Athena, Azure
                    Databricks
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}[fragile]{Example queries}
        \pdfpcnote{%
            This query is to find the most common "actions" that developers
            perform in their commits, which are generally the first words of
            the commit message. So using Amazon Athena, in just a few lines of
            SQL we are able to extract this information from 1.1 billion
            revisions, and in just 30 seconds we get this result, with "update"
            being the most common action followed by "merge".
        }

        \begin{block}{Most frequent first commit words}
            \begin{minted}[fontsize=\small]{sql}
SELECT COUNT(*) AS c, word FROM (
  SELECT LOWER(REGEXP_EXTRACT(FROM_UTF8(
  message), 'ˆ\w+')) AS word FROM revision)
WHERE word != ''
GROUP BY word ORDER BY COUNT(*) DESC LIMIT 5;
            \end{minted}

            \begin{center}
                \begin{tabular}{rl}
                    Count & Word\\
                    \hline
                    \num{71338310} & update\\
                    \num{64980346} & merge\\
                    \num{56854372} & add\\
                    \num{44971954} & added\\
                    \num{33222056} & fix\\
                \end{tabular}
            \end{center}
        \end{block}

        \begin{block}{}
            Analyzes 1.1 billion revision messages in 30 seconds.
        \end{block}
    \end{frame}

    \begin{frame}[fragile]{Example queries}
        \pdfpcnote{%
            For a more complex query, here we're trying to answer the question
            of whether there's been a change in the proportion of commits
            made during the week-end. Maybe our hypothesis is that people are
            changing their work/life balance, something like that. And again we
            can analyze all the commits in the dataset in just a few seconds,
            and we get these results. There's no clear trend, the best fit line
            is flat, so we can't really conclude anything from the commit data
            at least.
        }

        \begin{columns}
            \column{0.5\textwidth}
            \begin{block}{Weekend work}
                \inputminted[fontsize=\tiny, firstline=3]{sql}{../codesamples/graph-dataset/weekend-work.sql}
            \end{block}
            \column{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\linewidth]{../img/graph-dataset/weekend-work}
            \end{center}
        \end{columns}

        \begin{block}{}
            Analyzes 1.1 billion revision timestamps in 7 seconds.
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Recursive queries}
        \pdfpcnote{%
            So this scale-out approach of massively distributing computations
            on extremly large volumes of data works really well for
            embarassingly parallel queries, that can be just trivially
            distributed on clusters of machine.
            \\\\
            But for a lot of research use cases, we need to exploit the
            recursive structure of the graph, run actual graph algorithms on
            this dataset like getting transitive closures or computing
            connected components. And doing this on scale-out solutions is
            completely cost prohibitive, we tried that on Spark using the
            GraphFrames library and it required spawning a cluster of 80 nodes
            for 4 hours, which costs approximately 5000 dollars.
            \\\\
            So now the question is, could we make it accessible to researchers
            to run complex graph algorithms on the dataset of software
            development?
        }
        \begin{block}{}
            \begin{itemize}
                \item This approach works really well for \textbf{embarassingly
                    parallel} queries
                \item Scale-out solutions are less efficient for
                    \textbf{recursive queries} that exploit the
                    hierarchical/structured nature of the graph
                \item BFS Traversal of the graph on Spark: 4 hours, 80
                    nodes(!), 5000 USD
            \end{itemize}
        \end{block}

        \begin{block}{Research question}
            Can recursive graph algorithms be performed in an
            accessible and cost-efficient way?
        \end{block}
    \end{frame}

    % \begin{frame}[fragile]{Example queries}
    %     \begin{block}{Spark: Connected components size distribution}
    %         \inputminted[fontsize=\small]{sql}{../codesamples/graph-dataset/spark-cc.py}
    %     \end{block}

    %     \begin{block}{}
    %         \textbf{Warning}: distributed graph algorithms on Spark are very
    %         expensive (\textasciitilde{}5000 USD for the entire graph with
    %         Azure Databricks).
    %     \end{block}
    % \end{frame}

    \section{Graph Compression}

    \begin{frame}
        \frametitle{Compression approach}

        \pdfpcnote{%
            The idea here is to compress the graph of software development in a
            way that makes it possible to analyze it on a single machine.
            One major advantage of this approach is that wouldn't need to write
            distributed versions of the graph algorithms, which can often
            require a lot of research in itself, and makes it less
            accessible.\\
            If done efficiently, we expect it to be a lot cheaper
            than distributed approaches, so that should allow us to run
            exhaustive analyses very quickly and gain a better understanding of
            the graph.
            \\\\
            We're not proposing new graph compression techniques here, but
            importing and evaluating existing ones for our specific domain. In
            particular we're going to look at techniques used to compress the
            graph of the Web, and see if they can be adapted for the graph of
            software development.
        }

        \begin{block}{}
            \textbf{Objective}: Analyzing the \emph{entire graph of public
            software development} on a single machine.

            \footnotesize
            \begin{thebibliography}{swhgraphcomp}
                \bibitem{Boldi2020} Paolo Boldi, Antoine Pietri, Sebastiano Vigna, Stefano Zacchiroli
                \newblock Ultra-Large-Scale Repository Analysis via Graph Compression
                \newblock SANER 2020, 27th Intl. Conf. on Software Analysis, Evolution and Reengineering. IEEE
            \end{thebibliography}
        \end{block}

        \begin{block}{Advantages}
            \begin{itemize}
                \item Simpler for prototyping, no need to write distributed
                    algorithms
                \item Cheaper than scale-out processing
                \item Allows us to run exhaustive analyses quickly
            \end{itemize}
        \end{block}

        \begin{block}{Compression techniques}
            We apply existing compression techniques used to compress the
            \textbf{graph of the Web}.
        \end{block}
    \end{frame}

    \begin{frame}{Compression pipeline}
        \pdfpcnote{%
            The way the compression works in the graph of the web is that it
            exploits one of its properties, which is that URLs that are
            lexicographically similar will generally be in the same
            neighborhood in the graph. And having this node locality is a key
            property to be able to compress adjacency lists efficiently.
            \\\\
            For the graph of software development we don't have that property,
            because the node identifiers are just randomly distributed
            cryptographic hashes. However we can reconstitute a node order that
            has this locality information by doing a breadth-first traversal.
            \\\\
            The idea of this pipeline is that we will first compress the graph
            using a random node ordering, which will give us a really bad
            compression ratio. But starting from that graph, we can run a
            BFS and use that new node ordering to recompress the graph and
            achieve a better compression ratio.
        }

        \begin{block}{Web graph → Software development graph: (re)establishing locality}
            Key for good compression of adjacency lists is a \alert{node
            ordering} that ensures \textbf{neighbor locality}.
            \begin{itemize}
                \item Lexicographically-ordered URLs in the Graph of the Web
                    have this property.
                \item It is \emph{not} the case with cryptographic Merkle
                    IDs\ldots{}
                \item \ldots{}but is the case \emph{again} after a
                    breadth-first traversal
            \end{itemize}
        \end{block}
        \vspace{-0.7cm}
        \begin{center}
            \includegraphics[width=1\linewidth]{../img/compression/compression_steps-nofiles}
        \end{center}
        \vspace{-1cm}
        \begin{itemize}
            \item \alert{MPH:} minimal perfect hash, mapping Merkle IDs to 0..N-1 integers
            \item \alert{BV compress:} Boldi-Vigna compression (based on MPH order)
            \item \alert{BFS:} breadth-first visit to renumber
            \item \alert{Permute:} update BV compression according to BFS order
        \end{itemize}
    \end{frame}

    \begin{frame}{Compression results}
        \pdfpcnote{%
            We ran this compression pipeline on the graph dataset using the
            WebGraph framework, on a server with 24 CPUs and 750 GB of RAM.
            The compression only takes 6 days, and from the 6 TB input we
            obtain a compressed graph of only 91 GB. We can also produce a
            transposed graph for backward traversals, and this one is 83 GB.
            Those figures only include the graph structure of nodes and edges,
            but not the data associated with them (like file contents or commit
            messages...)
            \\\\
            Now it's definitely possible to put these graphs in RAM, at least
            on a high-end machine. We ran a benchmark to see how fast it was to
            run algorithms on the entire graph, and a full traversal takes less
            than 2 hours.
        }
        \begin{block}{}
            We ran the compression pipeline on the input corpus using the WebGraph
            framework
            \begin{thebibliography}{}
                \footnotesize
                \bibitem{BoVWFI} Paolo Boldi and Sebastiano Vigna.
                \newblock The WebGraph framework I: Compression techniques
                \newblock WWW 2004: 13th Intl. World Wide Web Conference. ACM
            \end{thebibliography}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item Server equipped with 24 CPUs and 750 GB of RAM
                \item \textbf{Compression time}: 138 hours (6 days)
                \item \textbf{Compression efficiency}: 6 TiB edge file → 91 GiB
                    forward, 83 GiB transposed
            \end{itemize}
        \end{block}

        \begin{block}{Benchmark}
            Full traversal: 1h48m (1.81 M nodes/s)
        \end{block}

    \end{frame}

    \begin{frame}{LLP compression}
        \pdfpcnote{%
            It's possible to push the compression even further with Layered
            Label Propagation, which is another algorithm that can discover
            locality information between nodes.
            The way it works is by assigning random labels to node and
            propagating them to they neighbors, which generates clusters of
            nodes all located in the same neighborhood.
            \\\\
            This step goes after the BFS in the pipeline, so we get better and
            better compression at every step.
            It has a big tradeoff in that it requires a lot more runtime
            memory, which is not always achievable depending on your hardware
            configuration, but the compression ratio we obtain is really
            impressive, it reduces the size of the graph 35\% further.
        }
        \begin{block}{Layered Label Propagation}
            \begin{thebibliography}{Foo Bar, 1969}
                \small \vspace{-2mm}
                \bibitem{Boldi2010} Paolo Boldi, Marco Rosa, Massimo Santini, Sebastiano Vigna
                \newblock Layered Label Propagation: A MultiResolution Coordinate-Free Ordering for Compressing Social Networks
                \newblock WWW 2010: 20th Intl. World Wide Web Conference. ACM
            \end{thebibliography}
        \end{block}
        \begin{block}{}
            \begin{itemize}
                \item Algorithm to uncover locality information
                \item Propagates labels on random nodes to discover neighborhoods
                \item Compression requires more runtime memory (33 bytes per node)
                \item Even more impressive compression ratio (91 GiB → 60 GiB,
                    reduced by \textasciitilde{}35\%)
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{Graph Attributes}
        \pdfpcnote{%
            Up until now we have only compressed the structure of the graph but
            not the attributes stored on the nodes and the edges, like commit
            messages, timestamps, etc. But this data is needed for a lot of use
            cases, like if you want to find the earliest commit in which a
            unique file has ever been found.
            \\\\
            In my thesis, I propose various designs to actually store this data
            so that it can be accessed during graph traversals. These
            techniques exploit the fact that node IDs are contiguous to store
            them as binary arrays on disk. We can mmap these arrays at runtime
            depending on which kinds of data we need access to.
            \\\\
            Storing data on edges is a bit more involved, but there is built in
            support in WebGraph to achieve that. I use other techniques to
            compress the attributes on the edges, like a MPH on the file names
            to only have to store them once.
        }
        \begin{block}{Node attributes}
            \begin{itemize}
                \item The compressed in-memory graph structure has \alert{no attributes}
                \item Usual data design is to exploit the 0..N-1 integer ranges to \alert{memory map
                    \emph{node} attributes} from secondary storage (node ID →
                    node attribute)
                    \begin{itemize}
                        \item We do this for node types (mapping: 4 GiB),
                            timestamps (mapping: 149 GiB), etc.
                        \item Data structures: integer/byte arrays, front-coded
                            string lists, etc.
                    \end{itemize}
            \end{itemize}
        \end{block}
        \begin{block}{Edge attributes}
            \begin{itemize}
                \item Built-in WebGraph support for attributes on the \alert{edges} (generally integers)
                \item For file \emph{names}, we use another minimal perfect hash to map file names to integers
            \end{itemize}
        \end{block}
        % \begin{block}{Disk/memory consideration}
        %     \begin{itemize}
        %         \item Labels and mappings can be either in RAM or
        %             \texttt{mmap()}-ed from disk
        %         \item Time/memory tradeoff, depends on access patterns, intensive
        %             workloads etc.
        %     \end{itemize}
        % \end{block}
    \end{frame}

    \begin{frame}[fragile]{Graph Querying}
        \pdfpcnote{%
            Once we have this compressed graph, we need to make it actually
            accessible for researchers who want to run experiments on it. We
            have a very basic API to write graph algorithms, but it only
            provides very low level primitives like the successors() function
            to get the adjacency list of a node. A big issue here is that this
            requires local access to the server, so you can't really set it up
            as a service that researchers can query.
            \\
            If we want to do better we need to build a traversal language on
            top of this.
        }
        \begin{block}{}
            \textbf{Option 1}: Write a traversal algorithm using Java
            graph primitives
        \end{block}
        \begin{minted}[fontsize=\scriptsize,highlightlines={8}]{java}
HashSet<Long> visited = new HashSet<>();
Stack<Long> stack = new Stack<>();
stack.push(srcNodeId);
visited.add(srcNodeId);

while (!stack.isEmpty()) {
    long currentNodeId = stack.pop();
    LazyLongIterator it = graph.successors(currentNodeId);
    for (long neighborNodeId; (neighborNodeId = it.nextLong()) != -1; ) {
        if (!visited.contains(neighborNodeId)) {
            stack.push(neighborNodeId);
            visited.add(neighborNodeId);
        }
    }
}
        \end{minted}
        \begin{block}{}
            \begin{itemize}
                \item Efficient but low-level \& requires local access to the
                    graph server.
                \item Simpler/remote querying ⇒ need to build traversal query
                    language
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{Graph Querying}
        \pdfpcnote{%
            So we built a simple HTTP API which can be used to run simple
            traversals remotely. It has a few base primitives, like getting the
            neighbors or the leaves of a node, or return its entire transitive
            closure. It can't do more complex queries like traversals starting
            from multiple nodes at the same time, but it does have a few
            options like a way to add constraints on the types of edges
            traversed by the algorithm.
            \\\\
            Now that we have built this framework to cheaply run exhaustive
            experiments on the compressed graph, we can use it to try to
            understand the structure of this giant graph of software
            development.
        }

        \begin{block}{}
            \textbf{Option 2}: HTTP API for simple graph traversals

            \begin{itemize}
                \item Generic remote API for graph traversals, Java/Python/aiohttp backend
                \item Limited to simple DFS from a single node (forward or
                    backward graph)
                \item Traversal types: neighbors, leaves, all nodes, all edges
                \item Supports edge type filtering
            \end{itemize}
        \end{block}

        \begin{minted}[fontsize=\scriptsize]{text}
> GET /leaves/swh:1:rev:f39d[...]2a35?direction=backward
swh:1:ori:634a2b699d442aa9abd5008f379847816f54ab85
swh:1:ori:571a86b198c6c66ef33025249f7e455b529aae65
swh:1:ori:c15194d6cb59a6d32777ca3b287ea6664d540df3
...

> GET /visit/nodes/swh:1:rev:c6df[...]fc28?edges=rel:rev,rev:rev
swh:1:rel:c6df0a7ef73ca90825f1472b8a3c5f7a2ce3fc28
swh:1:rev:c8448ff2f9234332f0bc25dc3a13031f8ab3c73c
swh:1:rev:4b63dbd4e782e74bdc050c4579381d29b4bd41c0
...
        \end{minted}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Graph Subdatasets}

    %     \begin{block}{Generating representative subgraphs}
    %         \begin{itemize}
    %             \item Useful for smaller-scale experimentation, prototyping
    %             \item Focusing analysis on a relevant subset
    %             \item Representative samples → transitive closure of a subset
    %                 of origins
    %             \item Use a fitted log model to estimate the size of the
    %                 resulting subgraph
    %         \end{itemize}
    %     \end{block}

    %     \begin{center}
    %         \includegraphics[width=.5\linewidth]{../img/graph-exploitation/subdataset_size_function_fit.pdf}
    %     \end{center}
    % \end{frame}

    \section{Graph Topology of Software Development}

    \begin{frame}
        \frametitle{Graph Topology: Research Questions}
        \pdfpcnote{%
            As we said before, the graph dataset is a very meaningful object
            for software mining researchers, because it materializes all the
            relationships between all the software objects available publicly.
            This creates a very complex structure that has never been
            empirically studied before, but the now the compressed graph
            framework can be used to run exhaustive studies on it.
            \\\\
            So the first thing we want to do is to compute the key metrics that
            characterize complex networks in general, like degree
            distributions, connected components, distances between roots and
            leaves and clustering coefficient. I computed all these metrics on
            the entire graph but also on its different layers, so now I can
            show you what they actually tell us about the graph. And from there
            we can deduce potential implications for software mining, like ways
            to distribute computations on the graph efficiently, and
            methodological implications for empirical studies.
        }

        \begin{block}{}
            The Software Heritage Graph Dataset materializes a \emph{network of
            relationships between software artifacts} which has not yet been
            empirically studied as a whole.
        \end{block}

        \begin{block}{Research questions}

            \begin{itemize}
                \item What is the network topology of the graph of software
                    development?

                    Network topology metrics: Degree distributions, connected
                    components, distance between roots and leaves, clustering
                    coefficient.

                \item What do these metrics tell us about this graph and its
                    layers?
                    \begin{itemize}
                \item Best approaches for large scale analysis?
                \item Methodological implications for software mining?
                    \end{itemize}
            \end{itemize}

            The \textbf{compressed graph framework} allows us to answer these
            questions experimentally.

            \footnotesize
            \begin{thebibliography}{swhforks}
                \bibitem{swhforks} Antoine Pietri, Guillaume Rousseau, Stefano Zacchiroli\newblock
                Determining the intrinsic structure of public software development history\newblock
                Mining Software Repositories 2020\newblock
            \end{thebibliography}
        \end{block}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Graph layers}

    %     \begin{block}{}
    %         We study the topology of the graph as a whole, but also of its
    %         different semantic layers:
    %     \end{block}

    %     \begin{center}
    %         \scalebox{0.6}{\input{../tikz/figures/swh-layers.tikz}}
    %     \end{center}
    % \end{frame}

    \begin{frame}
        \frametitle{Average degree}
        \pdfpcnote{%
            The first metric we can look at is the average degree of the graph,
            which is the average number of neighbors of all the nodes in the
            graph. We can compare this degree with other complex networks that
            have been studied in the literature, like social or collaboration
            networks.
            \\\\
            We can see that the graph is relatively sparse, with a degree of
            around 11. For comparison the social graph of Facebook has an
            average degree of 169. Another important thing is that this degree
            of 11 is largely dominated by the filesystem layer, which contains
            around 90\% of all the nodes in the graph. The commit layer is even
            more sparse, with an average degree of around one, which likely
            means it's mostly constituted of long chains.
        }
        \begin{block}{}
            Average number of neighbors of all the nodes in the graph

            % (e.g., social networks → number of friends/followers)
        \end{block}

        \begin{table}
            \centering
            \begin{tabular}[t]{l S[table-format=3.3]}
                \textbf{Dataset} & \textbf{Average degree} \\
                \hline
                % \textbf{swh-2020-history}    & 1.021 \\
                \textbf{swh-2020-commit}     & 1.022 \\
                % \textbf{swh-2020-hosting}    & 3.39 \\
                bitcoin-2013 & 6.4 \\
                dblp-2011 (Co-authorship)       & 6.8 \\
                \textbf{swh-2020}            & 11.0 \\
                \textbf{swh-2020-filesystem} & 12.1 \\
                twitter-2010      & 35.2 \\
                clueweb12                    & 43.1 \\
                uk-2014 (Web)               & 60.4 \\
                fb-2011 (Facebook)          & 169.0 \\
            \end{tabular}
        \end{table}
    \end{frame}

    \begin{frame}
        \frametitle{Out-degree distributions: filesystem layer}
        \pdfpcnote{%
            Here is the plot of the outdegree distribution of the filesystem
            layer, which corresponds to the distribution of the number of
            entries of each directory in the graph.
            \\\\
            This is how you read this frequency distribution: this shows that
            there are around 10^8 nodes with **exactly** 20 children. There's
            also a cumulative frequency distribution, in which you can see that
            there are around 10^9 nodes with **more** than 20 children.
            \\\\
            What we can see here is that there is not really a threshold
            effect after which the degrees are really unlikely, it decreases in
            something that looks like a power law. So there is not really a
            characteristic number of entries in a directory.
        }

        \begin{block}{}
            Distribution of the number of entries of each directory in the
            graph
        \end{block}

        \begin{figure}
            \centering
            \only<1,4>{%
                \includegraphics[width=0.5\linewidth]{img/topology/inout/dir+cnt_out}
            }
            \only<2>{%
                \includegraphics[width=0.5\linewidth]{img/topology/inout/dir+cnt_out_annotated}
            }
            \only<3>{%
                \includegraphics[width=0.5\linewidth]{img/topology/inout/dir+cnt_out_annotated_cum}
            }
        \end{figure}

        \begin{block}{}
            ⇒ No characteristic number of entries in a directory.
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Out-degree distributions: commit layer}
        \pdfpcnote{%
            It's very different in the commit layer. This shows the
            distribution of the number of parents of the commits in the graph.
            And here we can see a huge threshold effect, basically there are a
            lot of commits with only one parents, many commits with two
            parents, but after that it's mostly outliers. Keep in mind that
            this is a logarithmic scale, so there are three orders of magnitude
            between these two points.
            \\\\
            This happens because of development
            patterns, most commits will have only one parent, except for merge
            commits which generally have two. It's possible to make octopus
            merges with more than 3 parents, but it's exceedingly rare.
        }
        \begin{block}{}
            Distribution of the number of parents of each commit in the graph
        \end{block}

        \begin{figure}
            \centering
            \includegraphics[width=0.5\linewidth]{img/topology/inout/rev_out}
        \end{figure}

        \begin{block}{}
            ⇒ Characteristic number of parents due to development patterns.
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Distance between roots and leaves}
        \pdfpcnote{%
            We can see a similar disparity in the distance between the roots
            and the leaves. For the filesystem layer, this corresponds to the
            depth of files in directory trees. Here there is a notable
            threshold effect: most of the files are less than 10-20 levels
            deep, and files deeper than that are very rare.
            \\\\
            Commit chains don't have characteristic lengths, they seem to
            decrease in what again looks like a power law, meaning their length
            is arbitrary.
        }

        \begin{figure}
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=\linewidth]{img/topology/shortestpath/dir+cnt}
                \caption{Depth of files in directory trees}
            \end{subfigure}\hfill
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=\linewidth]{img/topology/shortestpath/rev}
                \caption{Length of commit chains}
            \end{subfigure}
        \end{figure}
    \end{frame}

    \begin{frame}
        \pdfpcnote{%
            An interesting metric is the size distribution of connected
            components. We use connected components to identify clusters of
            connected nodes but completely isolated from the
            rest of the graph. This is very useful, because if the graph is
            constituted a lot of small isolated island of software, it's really
            easy to shard the entire graph in distributed clusters. You can
            then just analyze each component separately without needing
            to synchronize the processes.
            \\\\
            Well, it turns out that this is not the case at all for the
            filesystem layer. more than 97\% of all the nodes are
            interconnected together in the giant component you see here. For
            the commit layer, this is a lot more doable, since the largest
            component only contains 3\% of the nodes.
        }
        \frametitle{Connected components}
        \begin{figure}
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=0.9\linewidth]{img/topology/connectedcomponents/dir+cnt}
                \caption{Filesystem layer}
            \end{subfigure}\hfill
            \begin{subfigure}{.49\textwidth}
                \centering
                \includegraphics[width=0.9\linewidth]{img/topology/connectedcomponents/rev}
                \caption{Commit layer}
            \end{subfigure}
        \end{figure}

        \begin{center}
            \begin{tabular}[t]{l r r r}
                \textbf{Layer} & \textbf{\# of WCC}
                               & \textbf{Size of largest WCC}
                               & \textbf{\% of nodes in largest}
                               \\
                               \hline
                Full graph       & \num{33104255}  & \num{18902683142} & 97.79\% \\
                Filesystem layer & \num{46286502}  & \num{16565521611} & 97.16\% \\
                Commit layer     & \num{88031649}  & \num{51543944}    & 2.61\% \\
            \end{tabular}
        \end{center}
    \end{frame}

    \begin{frame}
        \frametitle{Takeaways: filesystem / commit layer duality}
        \pdfpcnote{%
            To summarize, what's really interesting is the duality between the
            filesystem and the commit layers, they have almost opposite
            topological properties.
            \\\\
            The filesystem layer is very dense and cannot be partitioned easily
            because it has a giant connected component. The files have a
            characteristic depth of less than 10-20 levels, but there's no
            characteristic number of entries in the directories.
            \\\\
            The commit layer is the opposite in almost every way. It's a lot
            sparser, its largest component is rather small which makes it
            possible to partition it. The commit chains are arbitrarily long,
            but commits have a characteristic outdegree of about one, which
            makes them look like long degenerate chains.
        }

        \begin{block}{}
            The filesystem and commit layers have almost opposite topological
            properties.
        \end{block}

        \begin{columns}
            \column{0.5\columnwidth}
            \begin{block}{Filesystem layer}
                \begin{itemize}
                    \item Dense, non-partitionable (giant WCC)
                    \item Characteristic depth
                    \item Arbitrary outdegree
                \end{itemize}
                \begin{center}
                    \scalebox{0.7}{\input{../tikz/figures/topology-summary-filesystem.tikz}}
                \end{center}
            \end{block}

            \column{0.5\columnwidth}
            \begin{block}{Commit layer}
                \begin{itemize}
                    \item Sparse, partitionable (max WCC = 3\%)
                    \item Arbitrary depth
                    \item Characteristic outdegree (degenerate)
                \end{itemize}
                \begin{center}
                    \scalebox{0.5}{\input{../tikz/figures/topology-summary-revision.tikz}}
                \end{center}
            \end{block}
        \end{columns}
    \end{frame}

    \begin{frame}
        \frametitle{Implications for software mining research}
        \pdfpcnote{%
            This has a few implications for software mining research.
            The first one is that there's a wild disparity between the
            different layers that constitute the graph, so they need to be
            studied separately, otherwise the behavior you will observe will be
            dominated by the filesystem layer and you will miss the subtlety of
            the topology of the different layers.
            \\\\
            The second thing to note is that a lot of distributions seem to
            have a high propensity to produce outliers, so we should be careful
            to systematically justify how we filter outliers in empirical
            studies. It also shows that exhaustive studies can be very useful
            to check that results generalize properly.
            \\\\
            Finally, my experiments show that there is no natural way to
            partition the graph in small isolated clusters, which would be
            useful to distribute computations. We probably need more subtle
            approaches to achieve that, like modular decomposition.
        }

        \begin{block}{Layers}
            \begin{itemize}
                \item Large disparity in the topological structure of layers
                \item Important to study layers separately to understand
                    the graph structure
            \end{itemize}
        \end{block}

        \begin{block}{Methodology}
            \begin{itemize}
                \item High kurtosis / propensity to produce outliers
                \item No obvious rule to ``filter'' outliers in many
                    distributions
                \item Highlights the importance of exhaustive approaches
            \end{itemize}
        \end{block}

        \begin{block}{Distributed analysis}
            \begin{itemize}
                \item No natural partitioning in small connected components
                \item Need for more subtle approaches (modular decomposition?)
            \end{itemize}
        \end{block}
    \end{frame}

    \section{Identification of Software Forks}

    \begin{frame}
        \frametitle{Studying software forks}
        \pdfpcnote{%
            The topology analysis is a generic way to analyze any complex
            network, it's domain agnostic. It's very useful to understand the
            fundamental structure of the graph. But we also know that because
            this particular graph shows relationships between deduplicated
            software artifacts, it is semantically structured by code reuse
            patterns, people copying code from one version or one project to
            another. And one way to try to understand this domain-specific
            structure is to study software forks, which are projects built on
            top of existing projects, starting from their codebase but
            continuing the development in parallel.
            \\\\
            In general, studying software forks has many useful applications in
            empirical software engineering, because it helps us understands how
            communities grow and evolve and how that leads to more or less
            successful or healthy projects.
        }

        \begin{block}{}

            \begin{itemize}
                \item Topology analysis of the graph of software development
                    gives us a domain-agnostic understanding of relationships between
                    \emph{deduplicated software artifacts}
                \item This particular graph is structured by
                    \textbf{code reuse patterns}
                \item One way to understand these is to study \textbf{software
                    forks}: projects derived from existing software and being
                    developed independently.
            \end{itemize}

            \footnotesize
            \begin{thebibliography}{swhforks}
                \bibitem{swhforks} Antoine Pietri, Guillaume Rousseau, Stefano Zacchiroli\newblock
                Forking Without Clicking: on How to Identify Software Repository Forks\newblock
                Mining Software Repositories 2020\newblock
            \end{thebibliography}
        \end{block}

        \begin{block}{Applications in software evolution \& software health}
            \begin{itemize}
                \item Finding active and maintained projects
                \item Understanding ``Hard'' and ``development'' forks
                \item Identifying criteria for successful forks
            \end{itemize}
        \end{block}
    \end{frame}

                % \item The compressed graph allows us to run exhaustive
                %     quantitative studies on software forks
                % \item Understanding forks is a key research direction in
                %     software health and software evolution

    \begin{frame}
        \frametitle{Previous approaches for studying forks}
        \pdfpcnote{%
            Most studies studying software forks have only looked at platform
            metadata, like what happens when you press the "fork" button on
            collaborative development platforms like GitHub. You can extract
            and study a graph of these relationships from GitHub, but this
            approach is restricted to a single platform. You can't see the code
            reuse patterns that happen between different platforms or even
            different version control systems.\\
            Using the compressed graph, we can try to have a more exhaustive
            approach to identify these forks.
        }

        \begin{block}{}
            \begin{itemize}
                \item Most studies have relied on metadata from collaborative
                    development platforms to identify forks: relationships
                    created by pressing the ``fork'' button
                    \begin{center}
                        \includegraphics[width=2cm]{img/forkbutton}
                    \end{center}
                \item Github provides a graph of forks to parent repositories
                    \begin{center}
                        \includegraphics[width=7cm]{img/forknetwork}
                    \end{center}
            \end{itemize}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item Restricted to a single platform, relies on external metadata
                \item We can use the \textbf{compressed graph} for a more
                    exhaustive approach
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Intrinsic forks}
        \pdfpcnote{%
            The general idea is that instead of relying on platform metadata,
            we can instead try to find projects that share some development
            history, that we call intrinsic forks. If two different
            repositories have at least one commit in common, we can consider
            that they are forks of each other, without relying on external
            information.
            \\\\
            So we're trying to quantitatively evaluate whether this definition
            is good, how does it compare to the previous approach of using
            platform metadata?
            \\\\
            So I designed an experiment to do this comparison in an exhaustive
            way. I use the compressed graph to identify all the intrinsic forks
            in the graph of software development, then I compare them to the
            ones you get by using the metadata on GitHub.
        }
        \begin{block}{}
            Intuition: \textbf{Intrinsic forks} are software projects
            with \textbf{shared development history}.
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item Identified by intrinsic DVCS information (= at least one
                    shared commit)
                \item Includes projects for which no forking metadata is present
            \end{itemize}
        \end{block}

        \begin{block}{Research question}
            How do forks from collaborative development platforms compare to
            forks defined by shared development history?
        \end{block}

        \begin{block}{Experimental design}
            We \textbf{identify} intrinsic forks in the graph of development
            history, then quantitatively \textbf{compare} them to forks
            identified by GitHub metadata.
        \end{block}
    \end{frame}

    % \begin{frame}{Methodology}
    %     \pdfpcnote{%
    %     }

    %     \begin{block}{New concepts}
    %         \begin{itemize}
    %             \item \textbf{Fork networks}: repositories connected by
    %                 forking relationships
    %             \item \textbf{Fork cliques}: repositories that are all forks of
    %                 each other
    %         \end{itemize}
    %     \end{block}
    % \end{frame}

    \begin{frame}
        \frametitle{Distribution comparison}
        \pdfpcnote{%
            There are a few considerations on how to cluster similar forks
            together that I detail in the thesis, but basically these are the
            results we get. We see that by using platform specific metadata, we
            miss around 8\% of all the forks. And that's only on two comparable
            datasets, so it doesn't even include the cross-platform forks that
            we could discover using this technique.
            \\\\
            The difference between the two distributions stay mostly positive,
            which indicates that this method is more exhaustive in general. But
            the similarities between the two distributions are also visually
            striking, which indicates that our new definition really matches
            well with the intuitive notion that people have of what a fork
            corresponds to.
        }
        \begin{center}
            \includegraphics[width=0.5\linewidth]{../img/forks/fork-clique-partition-freq-distribution.pdf}
        \end{center}
        \begin{block}{}
            \begin{itemize}
                \item \textbf{8\% of forks missed} when using platform-level
                    metadata
                \item Positive difference ⇒ more exhaustive identification of
                    forks
                \item Distribution similarity ⇒ Matches developers'
                    expectations of what a fork is
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Implications for software mining research}
        \pdfpcnote{%
            This study has a few other implications for software mining
            research. If we only rely on platform metadata to identify
            software forks we will miss a lot of forks, and it might introduce
            an important selection bias because we will only ever consider the
            forks inside a single platform. The technique we propose to
            identify intrinsic forks is more robust, and can uncover forks
            across different platforms and even different VCS.
        }

        \begin{block}{}
            \begin{itemize}
                \item Relying on forge metadata ⇒ selection bias,
                    non-exhaustive
                \item Using shared development history uncovers more forks
                \item Robust approach across forges and across VCSs
            \end{itemize}
        \end{block}

    \end{frame}

    \section{Conclusion}

    \begin{frame}
        \frametitle{Academic contributions}
        \pdfpcnote{%
            I'll go over a quick summary of my main academic contributions. The
            first thing I've done is to frame the problem by doing a literature
            review of researcher needs, and establishing a roadmap towards
            universal software mining
            \\\\
            I proposed ways to make software development data accessible to
            researchers at three different scales: restricted sets of data with
            the Vault and the SwhFS, scale-out processing with the graph
            dataset on various clouds, and a compression approach to analyze
            the graph exhaustively on a single machine.
            \\\\
            Using these platforms, I realized two empirical studies to try to
            understand the structure of the graph, both in a domain-agnostic
            way by computing metrics that characterize its topological
            properties, but also in a domain-specific way, to see how the graph
            is structured by code reuse patterns and software forks. I detailed
            the methodological implications of these studies for software
            mining.
        }

        \begin{block}{Contextualization}
            \begin{itemize}
                \item Literature review: identify research needs
                \item Roadmap to universal software mining (BENEVOL 2018)
            \end{itemize}
        \end{block}

        \begin{block}{Making software artifacts data available}
            \begin{itemize}
                \item Small scale: Vault, SwhFS (ICSE 2021)
                \item Scale-out: Graph dataset (MSR 2019)
                \item Scale-up: Graph compression (SANER 2020)
            \end{itemize}
        \end{block}

        \begin{block}{Empirical studies on the graph structure}
            \begin{itemize}
                \item Domain-agnostic: topological properties (MSR 2020)
                \item Domain-specific: structure of forks (MSR 2020)
            \end{itemize}
        \end{block}
    \end{frame}

    % \begin{frame}
    %     \frametitle{Empirical findings \& impact on software mining}
    %     \begin{block}{Topology}
    %         \begin{itemize}
    %             \item Disparity between graph layers
    %             \item Graph cannot be partitioned naturally
    %             \item No sound way to filter outliers
    %         \end{itemize}
    %     \end{block}
    %     \begin{block}{Forks}
    %         \begin{itemize}
    %             \item Forks can be identified more exhaustively with shared
    %                 development history
    %             \item They qualitatively fit established notions of what constitutes a ``fork''
    %         \end{itemize}
    %     \end{block}
    % \end{frame}

    \begin{frame}
        \frametitle{Future work}
        \pdfpcnote{%
            The work I presented opens various perspectives for future
            research. The most immediate follow-up would be to make this more
            production-ready so that it can be reused easily.
            \\\\
            Right now the compressed graph works on a static export of the
            dataset, which means there is always a lag with the live data. We
            could try to make this compression incremental to reduce this lag.
            We could also integrate more expressive query languages like GQL or
            Gremlin to make it possible to run more complex queries without
            having to use low-level algorithms.
            \\\\
            We could also make improvements to scale-out processing. The
            general idea here is to try to find hybrid approaches between the
            compressed graph and distributed computing. For instance we could
            try to shard the graph efficiently using modular decomposition or
            locality preserving orders like the BFS or LLP. It should also be
            possible to do more integration between the two, so that you can
            transparently perform parts of a computation on the compressed
            graph, and offload the rest on the scale-out platforms to maximize
            efficiency.
            \\\\
            Finally this platform could be used to generate derived graphs that
            are also interesting to software mining researchers, like community
            graphs, commit diffs or dependency graphs. We could compress them
            and make them accessible in the same way.
        }

        \begin{block}{Compressed graph improvements}
            \begin{itemize}
                \item Incremental graph compression
                \item Expressive remote graph querying
            \end{itemize}
        \end{block}
        \begin{block}{Scale-out processing}
            \begin{itemize}
                \item Sharding the graph with modular decomposition,
                    locality-preserving orders…
                \item Integrating the compressed graph with scale-out
                    processing
            \end{itemize}
        \end{block}
        \begin{block}{Generating derived data}
            \begin{itemize}
                \item Community graphs
                \item Commit diffs
                \item Dependency graphs
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Thanks!}

        \begin{block}{}
            \emph{All} this work is open \{source, data, access, …\}.

            \url{https://forge.softwareheritage.org}
            \hfill
            \url{https://github.com/seirl/thesis}
        \end{block}

        \begin{block}{}
            \tiny
            \begin{itemize}
                \item \fullcite{swh-benevol2018-universal-analysis}
                \item \fullcite{swh-msr2019-dataset}
                \item \fullcite{msr-2020-challenge}
                \item \fullcite{saner-2020-swh-graph}
                \item \fullcite{swh-msr2020-forking}
                \item \fullcite{msr-2020-topology}
                \item \fullcite{swh-2021-swhfs}
            \end{itemize}
        \end{block}
    \end{frame}
\end{document}
